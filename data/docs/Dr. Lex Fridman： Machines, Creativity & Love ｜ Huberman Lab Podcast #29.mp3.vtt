WEBVTT

00:00.000 --> 00:02.260
 Welcome to the Huberman Lab Podcast,

00:02.260 --> 00:04.900
 where we discuss science and science-based tools

00:04.900 --> 00:05.900
 for everyday life.

00:09.300 --> 00:12.040
 I'm Andrew Huberman, and I'm a professor of neurobiology

00:12.040 --> 00:15.020
 and ophthalmology at Stanford School of Medicine.

00:15.020 --> 00:17.900
 Today, I have the pleasure of introducing Dr. Lex Friedman

00:17.900 --> 00:21.100
 as our guest on the Huberman Lab Podcast.

00:21.100 --> 00:23.180
 Dr. Friedman is a researcher at MIT

00:23.180 --> 00:26.540
 specializing in machine learning, artificial intelligence,

00:26.540 --> 00:29.340
 and human-robot interactions.

00:29.340 --> 00:31.820
 I must say that the conversation with Lex

00:31.820 --> 00:34.980
 was without question one of the most fascinating

00:34.980 --> 00:36.600
 conversations that I've ever had,

00:36.600 --> 00:39.440
 not just in my career, but in my lifetime.

00:39.440 --> 00:41.500
 I knew that Lex worked on these topics,

00:41.500 --> 00:43.940
 and I think many of you are probably familiar with Lex

00:43.940 --> 00:45.180
 and his interest in these topics

00:45.180 --> 00:48.260
 from his incredible podcast, the Lex Friedman Podcast.

00:48.260 --> 00:50.240
 If you're not already watching that podcast,

00:50.240 --> 00:53.740
 please subscribe to it, it is absolutely fantastic.

00:53.740 --> 00:56.040
 But in holding this conversation with Lex,

00:56.040 --> 00:58.500
 I realized something far more important.

00:58.500 --> 01:00.900
 He revealed to us a bit of his dream,

01:00.900 --> 01:03.360
 his dream about humans and robots,

01:03.360 --> 01:04.980
 about humans and machines,

01:04.980 --> 01:07.520
 and about how those interactions can change the way

01:07.520 --> 01:08.900
 that we perceive ourselves

01:08.900 --> 01:10.740
 and that we interact with the world.

01:10.740 --> 01:13.060
 We discuss relationships of all kinds,

01:13.060 --> 01:16.420
 relationships with animals, relationships with friends,

01:16.420 --> 01:20.420
 relationships with family, and romantic relationships.

01:20.420 --> 01:23.200
 And we discuss relationships with machines,

01:23.200 --> 01:26.500
 machines that move and machines that don't move,

01:26.500 --> 01:28.980
 and machines that come to understand us in ways

01:28.980 --> 01:31.700
 that we could never understand for ourselves,

01:31.700 --> 01:35.720
 and how those machines can educate us about ourselves.

01:35.720 --> 01:37.420
 Before this conversation,

01:37.420 --> 01:40.060
 I had no concept of the ways in which machines

01:40.060 --> 01:43.700
 could inform me or anyone about themselves.

01:43.700 --> 01:46.780
 By the end, I was absolutely taken with the idea,

01:46.780 --> 01:48.540
 and I'm still taken with the idea

01:48.540 --> 01:51.920
 that interactions with machines of a very particular kind,

01:51.920 --> 01:55.180
 a kind that Lex understands and wants to bring to the world,

01:55.180 --> 01:57.360
 can not only transform the self,

01:57.360 --> 02:00.100
 but may very well transform humanity.

02:00.100 --> 02:01.340
 So whether or not you're familiar

02:01.340 --> 02:03.320
 with Dr. Lex Friedman or not,

02:03.320 --> 02:05.340
 I'm certain you're going to learn a tremendous amount

02:05.340 --> 02:07.500
 from him during the course of our discussion,

02:07.500 --> 02:08.940
 and that it will transform the way

02:08.940 --> 02:12.060
 that you think about yourself and about the world.

02:12.060 --> 02:13.860
 Before we begin, I want to mention

02:13.860 --> 02:15.100
 that this podcast is separate

02:15.100 --> 02:17.560
 from my teaching and research roles at Stanford.

02:17.560 --> 02:19.960
 It is, however, part of my desire and effort

02:19.960 --> 02:22.740
 to bring zero cost to consumer information about science

02:22.740 --> 02:25.560
 and science-related tools to the general public.

02:25.560 --> 02:26.720
 In keeping with that theme,

02:26.720 --> 02:29.580
 I'd like to thank the sponsors of today's podcast.

02:29.580 --> 02:31.420
 Our first sponsor is Roca.

02:31.420 --> 02:33.340
 Roca makes sunglasses and eyeglasses

02:33.340 --> 02:35.620
 that are of absolutely phenomenal quality.

02:35.620 --> 02:36.500
 The company was founded

02:36.500 --> 02:38.500
 by two all-American swimmers from Stanford,

02:38.500 --> 02:40.760
 and everything about the sunglasses and eyeglasses

02:40.760 --> 02:43.900
 they've designed had performance in mind.

02:43.900 --> 02:46.220
 Now, I've spent a career working on the visual system,

02:46.220 --> 02:47.700
 and one of the fundamental issues

02:47.700 --> 02:49.700
 that your visual system has to deal with

02:49.700 --> 02:52.780
 is how to adjust what you see when it gets darker

02:52.780 --> 02:54.600
 or brighter in your environment.

02:54.600 --> 02:56.820
 With Roca sunglasses and eyeglasses,

02:56.820 --> 02:58.920
 whether or not it's dim in the room or outside,

02:58.920 --> 03:00.100
 whether or not there's cloud cover,

03:00.100 --> 03:01.540
 or whether or not you walk into a shadow,

03:01.540 --> 03:04.300
 you can always see the world with absolute clarity.

03:04.300 --> 03:06.220
 And that just tells me that they really understand

03:06.220 --> 03:07.620
 the way that the visual system works,

03:07.620 --> 03:10.140
 processes like habituation and attenuation.

03:10.140 --> 03:12.520
 All these things that work at a real mechanistic level

03:12.520 --> 03:14.540
 have been built into these glasses.

03:14.540 --> 03:16.740
 In addition, the glasses are very lightweight.

03:16.740 --> 03:19.060
 You don't even notice really that they're on your face.

03:19.060 --> 03:21.780
 And the quality of the lenses is terrific.

03:21.780 --> 03:23.500
 Now, the glasses were also designed

03:23.500 --> 03:25.460
 so that you could use them not just while working

03:25.460 --> 03:28.580
 or at dinner, et cetera, but while exercising.

03:28.580 --> 03:30.620
 They don't fall off your face or slip off your face

03:30.620 --> 03:31.800
 if you're sweating.

03:31.800 --> 03:33.460
 And as I mentioned, they're extremely lightweight,

03:33.460 --> 03:34.860
 so you can use them while running,

03:34.860 --> 03:37.060
 you can use them while cycling, and so forth.

03:37.060 --> 03:39.680
 Also, the aesthetic of Roca glasses is terrific.

03:39.680 --> 03:41.680
 Unlike a lot of performance glasses out there,

03:41.680 --> 03:44.580
 which frankly make people look like cyborgs,

03:44.580 --> 03:46.020
 these glasses look great.

03:46.020 --> 03:47.200
 You can wear them out to dinner.

03:47.200 --> 03:50.460
 You can wear them for essentially any occasion.

03:50.460 --> 03:51.980
 If you'd like to try Roca glasses,

03:51.980 --> 03:55.460
 you can go to roca.com, that's R-O-K-A.com,

03:55.460 --> 03:56.860
 and enter the code Huberman

03:56.860 --> 03:59.140
 to save 20% off your first order.

03:59.140 --> 04:01.060
 That's Roca, R-O-K-A.com,

04:01.060 --> 04:03.500
 and enter the code Huberman at checkout.

04:03.500 --> 04:06.900
 Today's episode is also brought to us by InsideTracker.

04:06.900 --> 04:09.400
 InsideTracker is a personalized nutrition platform

04:09.400 --> 04:11.980
 that analyzes data from your blood and DNA

04:11.980 --> 04:13.560
 to help you better understand your body

04:13.560 --> 04:15.700
 and help you reach your health goals.

04:15.700 --> 04:18.220
 I am a big believer in getting regular blood work done

04:18.220 --> 04:20.700
 for the simple reason that many of the factors

04:20.700 --> 04:23.180
 that impact our immediate and long-term health

04:23.180 --> 04:25.820
 can only be assessed from a quality blood test.

04:25.820 --> 04:28.460
 And now with the advent of quality DNA tests,

04:28.460 --> 04:30.900
 we can also get insight into some of our genetic

04:30.900 --> 04:34.300
 underpinnings of our current and long-term health.

04:34.300 --> 04:36.700
 The problem with a lot of blood and DNA tests out there,

04:36.700 --> 04:38.500
 however, is you get the data back

04:38.500 --> 04:40.480
 and you don't know what to do with those data.

04:40.480 --> 04:41.760
 You see that certain things are high

04:41.760 --> 04:42.980
 or certain things are low,

04:42.980 --> 04:45.200
 but you really don't know what the actionable items are,

04:45.200 --> 04:47.300
 what to do with all that information.

04:47.300 --> 04:49.740
 With InsideTracker, they make it very easy

04:49.740 --> 04:52.580
 to act in the appropriate ways on the information

04:52.580 --> 04:55.060
 that you get back from those blood and DNA tests.

04:55.060 --> 04:57.580
 And that's through the use of their online platform.

04:57.580 --> 04:59.860
 They have a really easy to use dashboard

04:59.860 --> 05:02.860
 that tells you what sorts of things can bring the numbers

05:02.860 --> 05:05.740
 for your metabolic factors, endocrine factors, et cetera,

05:05.740 --> 05:08.040
 into the ranges that you want and need

05:08.040 --> 05:10.140
 for immediate and long-term health.

05:10.140 --> 05:13.100
 In fact, I know one individual just by way of example

05:13.100 --> 05:15.280
 that was feeling good but decided to go

05:15.280 --> 05:16.940
 with an InsideTracker test and discovered

05:16.940 --> 05:18.340
 that they had high levels of what's called

05:18.340 --> 05:19.820
 C-reactive protein.

05:19.820 --> 05:21.820
 They would have never detected that otherwise.

05:21.820 --> 05:23.340
 C-reactive protein is associated

05:23.340 --> 05:25.900
 with a number of deleterious health conditions,

05:25.900 --> 05:28.180
 some heart issues, eye issues, et cetera.

05:28.180 --> 05:30.180
 And so they were able to take immediate action

05:30.180 --> 05:33.380
 to try and resolve those CRP levels.

05:33.380 --> 05:35.980
 And so with InsideTracker, you get that sort of insight.

05:35.980 --> 05:38.420
 And as I mentioned before, without a blood or DNA test,

05:38.420 --> 05:40.380
 there's no way you're going to get that sort of insight

05:40.380 --> 05:42.820
 until symptoms start to show up.

05:42.820 --> 05:44.300
 If you'd like to try InsideTracker,

05:44.300 --> 05:47.360
 you can go to insidetracker.com slash Huberman

05:47.360 --> 05:50.260
 to get 25% off any of InsideTracker's plans.

05:50.260 --> 05:52.680
 You just use the code Huberman at checkout.

05:52.680 --> 05:55.420
 That's insidetracker.com slash Huberman

05:55.420 --> 05:58.700
 to get 25% off any of InsideTracker's plans.

05:58.700 --> 06:01.540
 Today's podcast is brought to us by Athletic Greens.

06:01.540 --> 06:03.180
 Athletic Greens is an all-in-one

06:03.180 --> 06:05.660
 vitamin mineral probiotic drink.

06:05.660 --> 06:09.020
 I started taking Athletic Greens way back in 2012.

06:09.020 --> 06:11.860
 And so I'm delighted that they're sponsoring the podcast.

06:11.860 --> 06:13.740
 The reason I started taking Athletic Greens

06:13.740 --> 06:15.820
 and the reason I still take Athletic Greens

06:15.820 --> 06:19.440
 is that it covers all of my vitamin mineral probiotic bases.

06:19.440 --> 06:22.020
 In fact, when people ask me, what should I take?

06:22.020 --> 06:24.460
 I always suggest that the first supplement people take

06:24.460 --> 06:26.940
 is Athletic Greens for the simple reason

06:26.940 --> 06:29.480
 is that the things it contains covers your bases

06:29.480 --> 06:31.900
 for metabolic health, endocrine health,

06:31.900 --> 06:33.980
 and all sorts of other systems in the body.

06:33.980 --> 06:36.500
 And the inclusion of probiotics are essential

06:36.500 --> 06:38.900
 for a healthy gut microbiome.

06:38.900 --> 06:40.540
 There are now tons of data showing

06:40.540 --> 06:42.820
 that we have neurons in our gut.

06:42.820 --> 06:44.240
 And keeping those neurons healthy

06:44.240 --> 06:45.620
 requires that they are exposed

06:45.620 --> 06:47.940
 to what are called the correct microbiota,

06:47.940 --> 06:49.900
 little microorganisms that live in our gut

06:49.900 --> 06:51.180
 and keep us healthy.

06:51.180 --> 06:53.860
 And those neurons in turn help keep our brain healthy.

06:53.860 --> 06:56.500
 They influence things like mood, our ability to focus,

06:56.500 --> 06:59.900
 and many, many other factors related to health.

06:59.900 --> 07:01.380
 With Athletic Greens, it's terrific

07:01.380 --> 07:03.160
 because it also tastes really good.

07:03.160 --> 07:04.700
 I drink it once or twice a day.

07:04.700 --> 07:07.060
 I mix mine with water and I add a little lemon juice

07:07.060 --> 07:09.380
 or sometimes a little bit of lime juice.

07:09.380 --> 07:11.080
 If you want to try Athletic Greens,

07:11.080 --> 07:14.100
 you can go to athleticgreens.com slash Huberman.

07:14.100 --> 07:16.660
 And if you do that, you can claim their special offer.

07:16.660 --> 07:18.540
 They're giving away five free travel packs,

07:18.540 --> 07:20.420
 the little packs that make it easy to mix up

07:20.420 --> 07:22.660
 Athletic Greens while you're on the road.

07:22.660 --> 07:26.260
 And they'll give you a year supply of vitamin D3 and K2.

07:26.260 --> 07:28.940
 Again, go to athleticgreens.com slash Huberman

07:28.940 --> 07:30.900
 to claim that special offer.

07:30.900 --> 07:34.620
 And now my conversation with Dr. Lex Friedman.

07:34.620 --> 07:35.540
 We meet again.

07:35.540 --> 07:36.500
 We meet again.

07:36.500 --> 07:39.600
 Thanks so much for sitting down with me.

07:39.600 --> 07:43.460
 I have a question that I think is on a lot of people's minds

07:43.460 --> 07:46.380
 or ought to be on a lot of people's minds

07:46.380 --> 07:50.020
 because we hear these terms a lot these days,

07:50.020 --> 07:53.100
 but I think most people, including most scientists

07:53.100 --> 07:56.900
 and including me, don't know really

07:56.900 --> 08:01.900
 what is artificial intelligence and how is it different

08:01.900 --> 08:05.240
 from things like machine learning and robotics?

08:05.240 --> 08:08.900
 So if you would be so kind as to explain to us

08:08.900 --> 08:13.900
 what is artificial intelligence and what is machine learning?

08:14.020 --> 08:17.100
 Well, I think that question is as complicated

08:17.100 --> 08:21.780
 and as fascinating as the question of what is intelligence.

08:21.780 --> 08:26.480
 So I think of artificial intelligence first

08:26.480 --> 08:28.740
 as a big philosophical thing.

08:28.740 --> 08:35.740
 Pamela McCordick said AI was the ancient wish

08:37.180 --> 08:40.620
 to forge the gods or was born as an ancient wish

08:40.620 --> 08:41.740
 to forge the gods.

08:41.740 --> 08:44.260
 So I think at the big philosophical level,

08:44.260 --> 08:48.300
 it's our longing to create other intelligent systems,

08:48.300 --> 08:50.400
 perhaps systems more powerful than us.

08:51.740 --> 08:56.740
 At the more narrow level, I think it's also a set of tools

08:56.740 --> 08:59.260
 that are computational mathematical tools

08:59.260 --> 09:01.100
 to automate different tasks.

09:01.100 --> 09:05.620
 And then also it's our attempt to understand our own mind.

09:05.620 --> 09:09.940
 So build systems that exhibit some intelligent behavior

09:09.940 --> 09:12.900
 in order to understand what is intelligence

09:12.900 --> 09:14.580
 in our own selves.

09:14.580 --> 09:16.160
 So all those things are true.

09:16.160 --> 09:19.340
 Of course, what AI really means as a community,

09:19.340 --> 09:21.500
 as a set of researchers and engineers,

09:21.500 --> 09:25.300
 it's a set of tools, a set of computational techniques

09:25.300 --> 09:27.680
 that allow you to solve various problems.

09:29.580 --> 09:33.020
 There's a long history that approaches the problem

09:33.020 --> 09:34.220
 from different perspectives.

09:34.220 --> 09:37.660
 What's always been throughout one of the threads,

09:37.660 --> 09:40.260
 one of the communities goes under the flag

09:40.260 --> 09:43.780
 of machine learning, which is emphasizing

09:43.780 --> 09:48.100
 in the AI space, the task of learning.

09:48.100 --> 09:50.520
 How do you make a machine that knows very little

09:50.520 --> 09:53.800
 in the beginning, follow some kind of process

09:53.800 --> 09:56.260
 and learns to become better and better

09:56.260 --> 09:58.260
 at a particular task.

09:58.260 --> 10:03.260
 What's been most very effective in the recent about 15 years

10:03.660 --> 10:05.700
 is a set of techniques that fall under the flag

10:05.700 --> 10:08.660
 of deep learning that utilize neural networks.

10:08.660 --> 10:12.120
 What neural networks are, are these fascinating things

10:12.120 --> 10:17.120
 inspired by the structure of the human brain very loosely,

10:17.240 --> 10:20.540
 but they have, it's a network of these little basic

10:20.540 --> 10:24.400
 computational units called neurons, artificial neurons.

10:24.400 --> 10:27.580
 And they have, these architectures have an input

10:27.580 --> 10:30.140
 and output, they know nothing in the beginning

10:30.140 --> 10:33.500
 and they're tasked with learning something interesting.

10:33.500 --> 10:35.220
 What that something interesting is,

10:35.220 --> 10:38.220
 usually involves a particular task.

10:38.220 --> 10:41.160
 The, there's a lot of ways to talk about this

10:41.160 --> 10:42.000
 and break this down.

10:42.000 --> 10:45.880
 Like one of them is how much human supervision

10:45.880 --> 10:48.340
 is required to teach this thing.

10:48.340 --> 10:51.860
 So supervised learning, this broad category,

10:51.860 --> 10:56.180
 is the neural network knows nothing in the beginning

10:56.180 --> 10:59.500
 and then it's given a bunch of examples of,

10:59.500 --> 11:02.020
 in computer vision, that will be examples of cats,

11:02.020 --> 11:06.080
 dogs, cars, traffic signs, and then you're given the image

11:06.080 --> 11:09.540
 and you're given the ground truth of what's in that image.

11:09.540 --> 11:13.040
 And when you get a large database of such image examples

11:13.040 --> 11:16.660
 where you know the truth, the neural network

11:16.660 --> 11:18.620
 is able to learn by example,

11:18.620 --> 11:20.460
 that's called supervised learning.

11:20.460 --> 11:22.760
 The question, there's a lot of fascinating questions

11:22.760 --> 11:26.040
 within that, which is how do you provide the truth?

11:26.040 --> 11:28.780
 When you've given an image of a cat,

11:30.180 --> 11:32.920
 how do you provide to the computer

11:32.920 --> 11:34.920
 that this image contains a cat?

11:34.920 --> 11:37.980
 Do you just say the entire image is a picture of a cat?

11:37.980 --> 11:40.340
 Do you do what's very commonly been done,

11:40.340 --> 11:41.540
 which is a bounding box?

11:41.540 --> 11:45.060
 You have a very crude box around the cat's face

11:45.060 --> 11:46.520
 saying this is a cat.

11:46.520 --> 11:48.720
 Do you do semantic segmentation?

11:48.720 --> 11:51.000
 Mind you, this is a 2D image of a cat.

11:51.000 --> 11:53.940
 So it's not a, the computer knows nothing

11:53.940 --> 11:55.660
 about our three-dimensional world.

11:55.660 --> 11:57.340
 It's just looking at a set of pixels.

11:57.340 --> 12:01.140
 So semantic segmentation is drawing a nice,

12:01.140 --> 12:04.920
 very crisp outline around the cat and saying that's a cat.

12:04.920 --> 12:07.060
 That's really difficult to provide that truth.

12:07.060 --> 12:09.500
 And one of the fundamental open questions

12:09.500 --> 12:10.800
 in computer vision is,

12:10.800 --> 12:13.800
 is that even a good representation of the truth?

12:13.800 --> 12:18.540
 Now, there's another contrasting set of ideas.

12:18.540 --> 12:21.120
 Their attention, their overlapping

12:21.120 --> 12:24.340
 is what's used to be called unsupervised learning,

12:24.340 --> 12:27.140
 what's commonly now called self-supervised learning,

12:27.140 --> 12:29.420
 which is trying to get less and less

12:29.420 --> 12:33.980
 and less human supervision into the task.

12:33.980 --> 12:38.820
 So self-supervised learning is more,

12:38.820 --> 12:42.020
 it's been very successful in the domain of a language model,

12:42.020 --> 12:43.180
 natural language processing,

12:43.180 --> 12:44.960
 and now more and more is being successful

12:44.960 --> 12:46.460
 in computer vision tasks.

12:46.460 --> 12:48.860
 And what's the idea there is,

12:48.860 --> 12:53.160
 let the machine without any ground truth annotation,

12:53.160 --> 12:55.740
 just look at pictures on the internet

12:55.740 --> 12:57.580
 or look at texts on the internet

12:57.580 --> 13:02.300
 and try to learn something generalizable

13:02.300 --> 13:05.800
 about the ideas that are at the core of language

13:05.800 --> 13:07.180
 or at the core of vision.

13:07.180 --> 13:08.580
 And based on that,

13:09.580 --> 13:12.900
 we humans at its best like to call that common sense.

13:12.900 --> 13:15.980
 So we have this giant base of knowledge

13:15.980 --> 13:18.700
 on top of which we build more sophisticated knowledge,

13:18.700 --> 13:21.380
 but we have this kind of common sense knowledge.

13:21.380 --> 13:23.380
 And so the idea with self-supervised learning

13:23.380 --> 13:25.980
 is to build this common sense knowledge

13:25.980 --> 13:30.420
 about what are the fundamental visual ideas

13:30.420 --> 13:33.260
 that make up a cat and a dog and all those kinds of things

13:33.260 --> 13:35.780
 without ever having human supervision.

13:35.780 --> 13:40.780
 The dream there is you just let an AI system

13:40.780 --> 13:44.840
 that's self-supervised run around the internet for a while,

13:44.840 --> 13:47.480
 watch YouTube videos for millions and millions of hours,

13:47.480 --> 13:52.080
 and without any supervision be primed and ready

13:52.080 --> 13:54.640
 to actually learn with very few examples

13:54.640 --> 13:56.680
 once the human is able to show up.

13:56.680 --> 14:00.160
 We think of children in this way, human children,

14:00.160 --> 14:03.040
 is your parents only give one or two examples

14:03.040 --> 14:04.600
 to teach a concept.

14:04.600 --> 14:07.040
 The dream with self-supervised learning

14:07.040 --> 14:10.080
 is that would be the same with machines

14:10.080 --> 14:13.960
 that they would watch millions of hours of YouTube videos

14:13.960 --> 14:16.760
 and then come to a human and be able to understand

14:16.760 --> 14:19.360
 when the human shows them this is a cat.

14:19.360 --> 14:20.800
 Like, remember this is a cat.

14:20.800 --> 14:23.540
 They will understand that a cat is not just a thing

14:23.540 --> 14:27.520
 with pointy ears or a cat is a thing that's orange

14:27.520 --> 14:30.800
 or is furry, they'll see something more fundamental

14:30.800 --> 14:32.720
 that we humans might not actually be able

14:32.720 --> 14:34.400
 to introspect and understand.

14:34.400 --> 14:36.800
 Like if I asked you what makes a cat versus a dog,

14:36.800 --> 14:39.400
 you wouldn't probably not be able to answer that.

14:39.400 --> 14:42.760
 But if I showed you, brought to you a cat and a dog,

14:42.760 --> 14:44.400
 you'll be able to tell the difference.

14:44.400 --> 14:47.040
 What are the ideas that your brain uses

14:47.040 --> 14:48.800
 to make that difference?

14:48.800 --> 14:51.280
 That's the whole dream of self-supervised learning

14:51.280 --> 14:53.880
 is it would be able to learn that on its own,

14:53.880 --> 14:56.080
 that set of common sense knowledge

14:56.080 --> 14:57.880
 that's able to tell the difference.

14:57.880 --> 15:01.600
 And then there's like a lot of incredible uses

15:01.600 --> 15:03.040
 of self-supervised learning,

15:04.000 --> 15:07.280
 very weirdly called self-play mechanism.

15:07.280 --> 15:11.720
 That's the mechanism behind the reinforcement learning

15:11.720 --> 15:15.720
 successes of the systems that want to go

15:15.720 --> 15:18.840
 at alpha zero, that want to chess.

15:18.840 --> 15:20.800
 Oh, I see, that play games.

15:20.800 --> 15:21.880
 That play games. Got it.

15:21.880 --> 15:26.000
 So the idea of self-play is probably applies

15:26.000 --> 15:27.880
 to other domains than just games,

15:27.880 --> 15:30.840
 is a system that just plays against itself.

15:30.840 --> 15:33.580
 And this is fascinating in all kinds of domains,

15:33.580 --> 15:36.600
 but it knows nothing in the beginning.

15:36.600 --> 15:39.540
 And the whole idea is it creates a bunch of mutations

15:39.540 --> 15:44.540
 of itself and plays against those versions of itself.

15:46.620 --> 15:50.220
 And the fascinating thing is when you play against systems

15:50.220 --> 15:51.820
 that are a little bit better than you,

15:51.820 --> 15:53.720
 you start to get better yourself.

15:53.720 --> 15:56.120
 Like learning, that's how learning happens.

15:56.120 --> 15:57.280
 That's true for martial arts,

15:57.280 --> 15:59.240
 that's true in a lot of cases,

15:59.240 --> 16:02.040
 where you want to be interacting with systems

16:02.040 --> 16:03.920
 that are just a little better than you.

16:03.920 --> 16:06.320
 And then through this process of interacting

16:06.320 --> 16:08.120
 with systems just a little better than you,

16:08.120 --> 16:10.480
 you start following this process where everybody

16:10.480 --> 16:12.640
 starts getting better and better and better and better

16:12.640 --> 16:15.480
 until you are several orders of magnitude better

16:15.480 --> 16:18.080
 than the world champion in chess, for example.

16:18.080 --> 16:21.040
 And it's fascinating because it's like a runaway system.

16:21.040 --> 16:23.560
 One of the most terrifying and exciting things

16:23.560 --> 16:27.200
 that David Silver, the creator of AlphaGo and AlphaZero,

16:27.200 --> 16:31.960
 one of the leaders of the team said to me is

16:31.960 --> 16:36.640
 they haven't found the ceiling for AlphaZero,

16:36.640 --> 16:39.360
 meaning it could just arbitrarily keep improving.

16:39.360 --> 16:41.840
 Now in the realm of chess, that doesn't matter to us,

16:41.840 --> 16:44.980
 that it's like, it just ran away with the game of chess.

16:44.980 --> 16:48.360
 Like it's like just so much better than humans.

16:48.360 --> 16:52.120
 But the question is if you can create that in the realm

16:52.120 --> 16:56.060
 that does have a bigger, deeper effect on human beings,

16:56.060 --> 16:59.940
 on societies, that can be a terrifying process.

16:59.940 --> 17:01.780
 To me, it's an exciting process

17:01.780 --> 17:03.720
 if you supervise it correctly,

17:03.720 --> 17:08.720
 if you inject what's called value alignment,

17:10.960 --> 17:13.500
 you make sure that the goals that the AI is optimizing

17:13.500 --> 17:17.000
 is aligned with human beings and human societies.

17:17.000 --> 17:19.200
 There's a lot of fascinating things to talk about

17:19.200 --> 17:23.200
 within the specifics of neural networks

17:23.200 --> 17:25.600
 and all the problems that people are working on.

17:25.600 --> 17:28.280
 But I would say the really big exciting one

17:28.280 --> 17:29.840
 is self-supervised learning,

17:29.840 --> 17:33.020
 where trying to get less and less human supervision,

17:35.520 --> 17:38.740
 less and less human supervision of neural networks.

17:38.740 --> 17:41.980
 And also just to comment and I'll shut up.

17:41.980 --> 17:43.100
 No, please keep going.

17:43.100 --> 17:44.120
 I'm learning.

17:44.120 --> 17:45.480
 I have questions, but I'm learning.

17:45.480 --> 17:46.480
 So please keep going.

17:46.480 --> 17:49.420
 So to me, what's exciting is not the theory,

17:49.420 --> 17:51.240
 it's always the application.

17:51.240 --> 17:52.920
 One of the most exciting applications

17:52.920 --> 17:55.120
 of artificial intelligence,

17:55.120 --> 17:57.480
 specifically neural networks and machine learning

17:57.480 --> 17:59.140
 is Tesla Autopilot.

17:59.140 --> 18:01.600
 So these are systems that are working in the real world.

18:01.600 --> 18:03.600
 This isn't an academic exercise.

18:03.600 --> 18:05.200
 This is human lives at stake.

18:05.200 --> 18:07.200
 This is safety critical.

18:07.200 --> 18:09.480
 These are automated vehicles, autonomous vehicles.

18:09.480 --> 18:11.280
 Semi-autonomous, we want to be.

18:11.280 --> 18:12.320
 Okay.

18:12.320 --> 18:15.000
 We've gone through wars on these topics.

18:15.000 --> 18:16.280
 Semi-autonomous.

18:16.280 --> 18:17.120
 Semi-autonomous.

18:17.120 --> 18:22.120
 So even though it's called FSD, full self-driving,

18:22.560 --> 18:24.960
 it is currently not fully autonomous,

18:24.960 --> 18:27.740
 meaning human supervision is required.

18:27.740 --> 18:30.920
 So human is tasked with overseeing the systems.

18:30.920 --> 18:35.220
 In fact, liability wise, the human is always responsible.

18:35.220 --> 18:37.940
 This is a human factor psychology question,

18:37.940 --> 18:39.360
 which is fascinating.

18:39.360 --> 18:43.000
 I'm fascinated by the whole space,

18:43.000 --> 18:46.160
 which is a whole nother space of human robot interaction.

18:46.160 --> 18:48.760
 When AI systems and humans work together

18:48.760 --> 18:49.960
 to accomplish tasks.

18:49.960 --> 18:54.960
 That dance to me is one of the smaller communities,

18:54.960 --> 18:58.280
 but I think it will be one of the most important

18:58.280 --> 19:00.340
 open problems once they're solved,

19:00.340 --> 19:03.160
 is how do humans and robots dance together?

19:04.040 --> 19:07.860
 To me, semi-autonomous driving is one of those spaces.

19:07.860 --> 19:11.680
 So for Elon, for example, he doesn't see it that way.

19:11.680 --> 19:16.520
 He sees semi-autonomous driving as a stepping stone

19:16.520 --> 19:18.620
 towards fully autonomous driving.

19:18.620 --> 19:22.720
 Like humans and robots can't dance well together.

19:22.720 --> 19:25.320
 Like humans and humans dance and robots and robots dance.

19:25.320 --> 19:28.060
 Like we need to, this is an engineering problem.

19:28.060 --> 19:31.680
 We need to design a perfect robot that solves this problem.

19:31.680 --> 19:34.120
 To me forever, maybe this is not the case with driving,

19:34.120 --> 19:37.140
 but the world is going to be full of problems

19:37.140 --> 19:40.400
 where it's always humans and robots have to interact

19:40.400 --> 19:43.520
 because I think robots will always be flawed,

19:43.520 --> 19:47.460
 just like humans are going to be flawed, are flawed.

19:47.460 --> 19:51.000
 And that's what makes life beautiful, that they're flawed.

19:51.000 --> 19:53.800
 That's where learning happens at the edge

19:53.800 --> 19:55.860
 of your capabilities.

19:55.860 --> 20:00.320
 So you always have to figure out how can flawed robots

20:00.320 --> 20:03.800
 and flawed humans interact together,

20:03.800 --> 20:08.360
 such that they, like the sum is bigger than the whole,

20:08.360 --> 20:11.260
 as opposed to focusing on just building the perfect robot.

20:12.540 --> 20:15.360
 So that's one of the most exciting applications,

20:15.360 --> 20:17.800
 I would say, of artificial intelligence to me,

20:17.800 --> 20:20.640
 is autonomous driving and semi-autonomous driving.

20:20.640 --> 20:23.240
 And that's a really good example of machine learning

20:23.240 --> 20:25.540
 because those systems are constantly learning.

20:26.780 --> 20:30.620
 And there's a process there that maybe I can comment on.

20:31.820 --> 20:34.360
 Andre Karpathy, who's the head of Autopilot,

20:34.360 --> 20:36.360
 calls it the data engine.

20:36.360 --> 20:38.900
 And this process applies for a lot of machine learning,

20:38.900 --> 20:40.340
 which is you build a system

20:40.340 --> 20:42.200
 that's pretty good at doing stuff.

20:42.200 --> 20:45.320
 You send it out into the real world.

20:45.320 --> 20:47.560
 It starts doing the stuff and then it runs

20:47.560 --> 20:50.640
 into what are called edge cases, like failure cases,

20:50.640 --> 20:51.700
 where it screws up.

20:52.640 --> 20:55.480
 You know, we do this as kids that, you know, you have-

20:55.480 --> 20:56.320
 We do this as adults.

20:56.320 --> 20:58.560
 We do this as adults, exactly.

20:58.560 --> 21:00.120
 But we learn really quickly.

21:00.120 --> 21:01.320
 But the whole point,

21:01.320 --> 21:03.640
 and this is the fascinating thing about driving,

21:03.640 --> 21:06.120
 is you realize there's millions of edge cases.

21:06.960 --> 21:10.800
 There's just like weird situations that you did not expect.

21:10.800 --> 21:13.480
 And so the data engine process is

21:13.480 --> 21:15.080
 you collect those edge cases,

21:15.080 --> 21:17.080
 and then you go back to the drawing board

21:17.080 --> 21:18.440
 and learn from them.

21:18.440 --> 21:21.000
 And so you have to create this data pipeline

21:21.000 --> 21:22.680
 where all these cars,

21:22.680 --> 21:25.320
 hundreds of thousands of cars as you're driving around,

21:25.320 --> 21:27.040
 and something weird happens.

21:27.040 --> 21:30.880
 And so whenever this weird detector fires,

21:30.880 --> 21:32.460
 it's another important concept,

21:33.360 --> 21:37.400
 that piece of data goes back to the mothership

21:37.400 --> 21:40.880
 for the training, for the retraining of the system.

21:40.880 --> 21:42.600
 And through this data engine process,

21:42.600 --> 21:44.920
 it keeps improving and getting better and better

21:44.920 --> 21:45.880
 and better and better.

21:45.880 --> 21:49.320
 So basically, you send out a pretty clever AI systems

21:49.320 --> 21:54.320
 out into the world and let it find the edge cases.

21:54.420 --> 21:56.600
 Let it screw up just enough

21:56.600 --> 21:58.560
 to figure out where the edge cases are,

21:58.560 --> 22:00.800
 and then go back and learn from them,

22:00.800 --> 22:02.640
 and then send out that new version

22:02.640 --> 22:04.240
 and keep updating that version.

22:04.240 --> 22:06.320
 Is the updating done by humans?

22:06.320 --> 22:08.540
 The annotation is done by humans.

22:09.460 --> 22:11.040
 So you have to,

22:11.040 --> 22:14.800
 the weird examples come back, the edge cases,

22:14.800 --> 22:17.680
 and you have to label what actually happened in there.

22:17.680 --> 22:22.680
 There's also some mechanisms for automatically labeling,

22:23.140 --> 22:25.840
 but mostly I think you always have to rely on humans

22:25.840 --> 22:28.080
 to improve, to understand what's happening

22:28.080 --> 22:30.000
 in the weird cases.

22:30.000 --> 22:31.760
 And then there's a lot of debate.

22:31.760 --> 22:32.640
 And that's the other thing,

22:32.640 --> 22:34.440
 what is artificial intelligence?

22:34.440 --> 22:36.800
 Which is a bunch of smart people

22:36.800 --> 22:39.740
 having very different opinions about what is intelligence.

22:39.740 --> 22:41.880
 So AI is basically a community of people

22:41.880 --> 22:43.960
 who don't agree on anything.

22:43.960 --> 22:45.800
 It seems to be the case.

22:45.800 --> 22:48.660
 And first of all, this is a beautiful description of terms

22:48.660 --> 22:51.900
 that I've heard many times among my colleagues at Stanford,

22:51.900 --> 22:53.760
 at meetings, in the outside world.

22:53.760 --> 22:55.940
 And there's so many fascinating things.

22:55.940 --> 22:56.960
 I have so many questions,

22:56.960 --> 23:00.200
 but I do want to ask one question about the culture of AI,

23:00.200 --> 23:02.640
 because it does seem to be a community where,

23:02.640 --> 23:03.960
 at least as an outsider,

23:03.960 --> 23:06.160
 where it seems like there's very little consensus

23:06.160 --> 23:07.280
 about what the terms

23:07.280 --> 23:09.620
 and the operational definitions even mean.

23:09.620 --> 23:12.340
 And there seems to be a lot of splitting happening now

23:12.340 --> 23:14.880
 of not just supervised and unsupervised learning,

23:14.880 --> 23:18.080
 but these sort of intermediate conditions

23:18.080 --> 23:20.780
 where machines are autonomous,

23:20.780 --> 23:22.120
 but then go back for more instruction,

23:22.120 --> 23:24.000
 like kids go home from college during the summer

23:24.000 --> 23:26.240
 and get a little, mom still feeds them,

23:26.240 --> 23:28.680
 then eventually they leave the nest kind of thing.

23:29.760 --> 23:32.640
 Is there something in particular about engineers

23:32.640 --> 23:35.820
 or about people in this realm of engineering

23:35.820 --> 23:39.080
 that you think lends itself to disagreement?

23:39.080 --> 23:43.440
 Yeah, I think, so first of all, the more specific you get,

23:43.440 --> 23:44.640
 the less disagreement there is.

23:44.640 --> 23:45.900
 So there's a lot of disagreement

23:45.900 --> 23:47.880
 about what is artificial intelligence,

23:47.880 --> 23:50.640
 but there's less disagreement about what is machine learning

23:50.640 --> 23:52.680
 and even less when you talk about active learning

23:52.680 --> 23:56.600
 or machine teaching or self-supervised learning.

23:56.600 --> 23:59.780
 And then when you get into NLP language models

23:59.780 --> 24:00.620
 or transformers,

24:00.620 --> 24:03.780
 when you get into specific neural network architectures,

24:03.780 --> 24:05.620
 there's less and less and less disagreement

24:05.620 --> 24:06.660
 about those terms.

24:06.660 --> 24:08.060
 So you might be hearing the disagreement

24:08.060 --> 24:09.340
 from the high level terms,

24:09.340 --> 24:12.060
 and that has to do with the fact that engineering,

24:12.060 --> 24:15.580
 especially when you're talking about intelligent systems

24:15.580 --> 24:20.580
 is a little bit of an art and a science.

24:20.840 --> 24:25.300
 So the art part is the thing that creates disagreements

24:25.300 --> 24:28.620
 because then you start having disagreements

24:28.620 --> 24:33.620
 about how easy or difficult a particular problem is.

24:33.860 --> 24:37.580
 For example, a lot of people disagree with Elon,

24:37.580 --> 24:41.100
 how difficult the problem of autonomous driving is.

24:41.100 --> 24:43.120
 And so, but nobody knows.

24:43.120 --> 24:44.340
 So there's a lot of disagreement

24:44.340 --> 24:47.160
 about what are the limits of these techniques.

24:47.160 --> 24:50.560
 And through that, the terminology also contains within it,

24:50.560 --> 24:53.940
 the disagreements.

24:53.940 --> 24:56.760
 But overall, I think it's also a young science

24:56.760 --> 24:58.820
 that also has to do with that.

24:58.820 --> 25:01.180
 So like, it's not just engineering,

25:01.180 --> 25:03.960
 it's that artificial intelligence truly

25:03.960 --> 25:06.800
 as a large scale discipline where it's thousands,

25:06.800 --> 25:09.420
 tens of thousands, hundreds of thousands of people

25:09.420 --> 25:11.660
 working on it, huge amounts of money being made.

25:11.660 --> 25:13.820
 That's a very recent thing.

25:13.820 --> 25:16.460
 So we're trying to figure out those terms.

25:16.460 --> 25:18.860
 And of course there's egos and personalities

25:18.860 --> 25:20.940
 and a lot of fame to be made,

25:22.620 --> 25:25.740
 like the term deep learning, for example.

25:25.740 --> 25:28.020
 Neural networks have been around for many, many decades,

25:28.020 --> 25:30.860
 since the sixties, you can argue since the forties.

25:30.860 --> 25:33.440
 So there was a rebranding of neural networks

25:33.440 --> 25:36.880
 into the word deep learning, the term deep learning

25:36.880 --> 25:40.900
 that was part of the reinvigoration of the field.

25:40.900 --> 25:42.680
 But it's really the same exact thing.

25:42.680 --> 25:43.640
 I didn't know that.

25:43.640 --> 25:46.020
 I mean, I grew up in the age of neuroscience

25:46.020 --> 25:49.060
 when neural networks were discussed.

25:49.060 --> 25:51.380
 Computational neuroscience and theoretical neuroscience,

25:51.380 --> 25:53.060
 they had their own journals.

25:53.060 --> 25:55.020
 It wasn't actually taken terribly seriously

25:55.020 --> 25:57.220
 by experimentalists until a few years ago.

25:57.220 --> 26:00.680
 I would say about five to seven years ago,

26:00.680 --> 26:03.500
 excellent theoretical neuroscientists like Larry Abbott

26:03.500 --> 26:07.460
 and other colleagues, certainly at Stanford as well,

26:07.460 --> 26:08.780
 that people started paying attention

26:08.780 --> 26:10.340
 to computational methods.

26:10.340 --> 26:13.200
 But these terms, neural networks, computational methods,

26:13.200 --> 26:15.140
 I actually didn't know that neural network works

26:15.140 --> 26:18.540
 in deep learning where those have now become

26:18.540 --> 26:19.380
 kind of synonymous.

26:19.380 --> 26:22.700
 No, they were always, no, they're always the same thing.

26:22.700 --> 26:24.160
 Interesting.

26:24.160 --> 26:25.740
 I'm a neuroscientist and I didn't know that.

26:25.740 --> 26:28.300
 So, well, because neural networks probably mean

26:28.300 --> 26:30.180
 something else in neuroscience, not something else,

26:30.180 --> 26:32.620
 but a little different flavor depending on the field.

26:32.620 --> 26:36.700
 And that's fascinating too, because neuroscience and AI,

26:36.700 --> 26:38.980
 people have started working together

26:38.980 --> 26:41.560
 and dancing a lot more in the recent,

26:41.560 --> 26:43.020
 I would say probably decade.

26:43.020 --> 26:44.900
 Oh, machines are going into the brain.

26:46.140 --> 26:47.600
 I have a couple of questions,

26:47.600 --> 26:49.800
 but one thing that I'm sort of fixated on

26:49.800 --> 26:54.400
 that I find incredibly interesting is this example you gave

26:54.400 --> 26:58.180
 of playing a game with a mutated version of yourself

26:58.180 --> 26:59.420
 as a competitor.

26:59.420 --> 27:02.340
 Yeah, I find that incredibly interesting

27:02.340 --> 27:05.860
 as a kind of a parallel or a mirror for what happens

27:05.860 --> 27:07.600
 when we try and learn as humans,

27:07.600 --> 27:10.540
 which is we generate repetitions of whatever it is

27:10.540 --> 27:13.260
 we're trying to learn and we make errors.

27:13.260 --> 27:15.020
 Occasionally we succeed.

27:15.020 --> 27:16.580
 In a simple example, for instance,

27:16.580 --> 27:18.980
 of trying to throw bull's eyes on a dartboard.

27:18.980 --> 27:20.460
 I'm going to have errors, errors, errors.

27:20.460 --> 27:21.680
 I'll probably miss the dartboard

27:21.680 --> 27:23.380
 and maybe occasionally hit a bull's eye.

27:23.380 --> 27:26.220
 And I don't know exactly what I just did, right?

27:26.220 --> 27:28.720
 But then let's say I was playing darts

27:28.720 --> 27:30.340
 against a version of myself

27:30.340 --> 27:32.460
 where I was wearing a visual prism,

27:32.460 --> 27:34.840
 like my visual, I had a visual defect.

27:36.700 --> 27:38.940
 You learn certain things in that mode as well.

27:38.940 --> 27:42.900
 You're saying that a machine can sort of mutate itself.

27:42.900 --> 27:45.100
 Does the mutation always cause a deficiency

27:45.100 --> 27:46.540
 that it needs to overcome?

27:46.540 --> 27:47.980
 Because mutations in biology

27:47.980 --> 27:49.580
 sometimes give us superpowers, right?

27:49.580 --> 27:51.060
 Occasionally you'll get somebody

27:51.060 --> 27:52.700
 who has better than 20, 20 vision

27:52.700 --> 27:56.420
 and they can see better than 99.9% of people out there.

27:56.420 --> 27:59.140
 So when you talk about a machine playing a game

27:59.140 --> 28:01.320
 against a mutated version of itself,

28:01.320 --> 28:04.700
 is the mutation always what we call a negative mutation

28:04.700 --> 28:07.620
 or an adaptive or a maladaptive mutation?

28:07.620 --> 28:11.860
 No, you don't know until you mutate first

28:11.860 --> 28:14.460
 and then figure out and they compete against each other.

28:14.460 --> 28:15.780
 So you're evolving,

28:15.780 --> 28:18.380
 the machine gets to evolve itself in real time.

28:18.380 --> 28:21.820
 Yeah, and I think of it, which would be exciting,

28:21.820 --> 28:23.580
 if you could actually do with humans,

28:23.580 --> 28:28.580
 it's not just, so usually you freeze a version

28:29.460 --> 28:30.300
 of the system.

28:30.300 --> 28:33.900
 So really you take an Andrew of yesterday

28:33.900 --> 28:35.620
 and you make 10 clones of them

28:36.560 --> 28:38.940
 and then maybe you mutate, maybe not.

28:38.940 --> 28:41.060
 And then you do a bunch of competitions

28:41.060 --> 28:42.440
 of the Andrew of today.

28:42.440 --> 28:45.620
 Like you fight to the death and who wins last.

28:45.620 --> 28:47.140
 So I love that idea of like creating

28:47.140 --> 28:50.980
 a bunch of clones of myself from each of the day

28:50.980 --> 28:54.500
 for the past year and just seeing who's going to be better

28:54.500 --> 28:58.820
 at like podcasting or science or picking up chicks at a bar

28:58.820 --> 29:01.980
 or I don't know, or competing in jujitsu.

29:01.980 --> 29:03.080
 That's one way to do it.

29:03.080 --> 29:06.320
 I mean, a lot of Lexes would have to die for that process,

29:06.320 --> 29:07.860
 but that's essentially what happens

29:07.860 --> 29:09.460
 is in reinforcement learning

29:09.460 --> 29:11.420
 through the self-play mechanisms,

29:11.420 --> 29:14.700
 it's a graveyard of systems that didn't do that well.

29:14.700 --> 29:19.700
 And the surviving, the good ones survive.

29:19.700 --> 29:22.660
 Do you think that, I mean, Darwin's theory of evolution

29:22.660 --> 29:26.300
 might have worked in some sense in this way,

29:26.300 --> 29:27.740
 but at the population level.

29:27.740 --> 29:29.560
 I mean, you get a bunch of birds with different shaped beaks

29:29.560 --> 29:30.980
 and some birds have the shape beak

29:30.980 --> 29:32.260
 that allows them to get the seeds.

29:32.260 --> 29:34.860
 I mean, it's a trivially simple example

29:34.860 --> 29:39.260
 of Darwinian evolution, but I think it's correct

29:39.260 --> 29:40.780
 even though it's not exhaustive.

29:40.780 --> 29:42.140
 Is that what you're referring to?

29:42.140 --> 29:44.100
 You essentially that normally this is done

29:44.100 --> 29:45.580
 between members of a different species.

29:45.580 --> 29:47.860
 Lots of different members of species have different traits

29:47.860 --> 29:49.420
 and some get selected for,

29:49.420 --> 29:52.580
 but you could actually create multiple versions of yourself

29:52.580 --> 29:53.880
 with different traits.

29:53.880 --> 29:56.460
 So with, I should probably have said this,

29:56.460 --> 29:59.220
 but perhaps it's implied,

29:59.220 --> 30:01.220
 but the machine learning or the reinforcement learning

30:01.220 --> 30:02.480
 through these processes,

30:02.480 --> 30:04.180
 one of the big requirements

30:04.180 --> 30:06.500
 is to have an objective function, a loss function,

30:06.500 --> 30:07.800
 a utility function.

30:07.800 --> 30:10.220
 Those are all different terms for the same thing.

30:10.220 --> 30:15.080
 Is there's like an equation that says what's good.

30:15.080 --> 30:17.500
 And then you're trying to optimize that equation.

30:17.500 --> 30:20.980
 So there's a clear goal for these systems.

30:20.980 --> 30:23.940
 Because it's a game, like with chess, there's a goal.

30:23.940 --> 30:26.820
 But for anything, anything you want machine learning

30:26.820 --> 30:29.940
 to solve, there needs to be an objective function.

30:29.940 --> 30:32.860
 In machine learning, it's usually called loss function

30:32.860 --> 30:34.340
 that you're optimizing.

30:34.340 --> 30:36.460
 The interesting thing about evolution,

30:37.780 --> 30:38.700
 complicated of course,

30:38.700 --> 30:41.700
 but the goal also seems to be evolving.

30:41.700 --> 30:44.020
 Like it's a, I guess adaptation to the environment

30:44.020 --> 30:46.460
 is the goal, but it's unclear.

30:46.460 --> 30:48.580
 You can convert that always.

30:48.580 --> 30:52.140
 It's like survival of the fittest.

30:52.140 --> 30:53.860
 It's unclear what the fittest is.

30:53.860 --> 30:56.820
 In machine learning, the starting point,

30:56.820 --> 31:00.460
 and this is like what human ingenuity provides,

31:00.460 --> 31:04.420
 is that fitness function of what's good and what's bad,

31:04.420 --> 31:08.340
 which it lets you know which of the systems is going to win.

31:08.340 --> 31:10.940
 So you need to have a equation like that.

31:10.940 --> 31:12.860
 One of the fascinating things about humans

31:12.860 --> 31:17.140
 is we figure out objective functions for ourselves.

31:17.140 --> 31:20.500
 Like we're, it's the meaning of life.

31:20.500 --> 31:22.980
 Like why the hell are we here?

31:22.980 --> 31:26.620
 And a machine currently has to have

31:26.620 --> 31:29.220
 a hard-coded statement about why.

31:29.220 --> 31:30.900
 It has to have a meaning of

31:30.900 --> 31:33.220
 artificial intelligence-based life.

31:33.220 --> 31:34.520
 Right, it can't.

31:34.520 --> 31:37.580
 So like there's a lot of interesting explorations

31:37.580 --> 31:42.420
 about that function being more about curiosity,

31:42.420 --> 31:45.220
 about learning new things and all that kind of stuff,

31:45.220 --> 31:46.680
 but it's still hard-coded.

31:46.680 --> 31:49.580
 If you want a machine to be able to be good at stuff,

31:49.580 --> 31:53.420
 it has to be given very clear statements

31:53.420 --> 31:56.060
 of what good at stuff means.

31:56.060 --> 31:58.360
 That's one of the challenges of artificial intelligence

31:58.360 --> 32:01.580
 is you have to formalize the,

32:01.580 --> 32:04.180
 in order to solve a problem, you have to formalize it

32:04.180 --> 32:06.060
 and you have to provide

32:06.060 --> 32:08.260
 both like the full sensory information.

32:08.260 --> 32:10.020
 You have to be very clear about

32:10.020 --> 32:12.600
 what is the data that's being collected.

32:12.600 --> 32:15.860
 And you have to also be clear about the objective function.

32:15.860 --> 32:18.840
 What is the goal that you're trying to reach?

32:18.840 --> 32:20.720
 And that's a very difficult thing

32:20.720 --> 32:22.100
 for artificial intelligence.

32:22.100 --> 32:23.940
 I love that you mentioned curiosity.

32:23.940 --> 32:26.940
 I'm sure this definition falls short in many ways,

32:26.940 --> 32:31.180
 but I define curiosity as a strong interest

32:31.180 --> 32:33.140
 in knowing something,

32:33.140 --> 32:35.660
 but without an attachment to the outcome.

32:35.660 --> 32:37.660
 You know, it's sort of a,

32:37.660 --> 32:39.320
 it's not, it could be a random search,

32:39.320 --> 32:42.060
 but there's not really an emotional attachment.

32:42.060 --> 32:44.020
 It's really just a desire to discover

32:44.020 --> 32:45.540
 and unveil what's there

32:45.540 --> 32:48.860
 without hoping it's a gold coin under a rock.

32:48.860 --> 32:50.620
 You're just looking under rocks.

32:50.620 --> 32:53.060
 Is that more or less how the machine,

32:53.060 --> 32:54.000
 within machine learning,

32:54.000 --> 32:57.260
 it sounds like there are elements of reward prediction

32:57.260 --> 32:59.340
 and rewards the machine has to know

32:59.340 --> 33:01.460
 when it's done the right thing.

33:01.460 --> 33:05.280
 So can you make machines that are curious

33:05.280 --> 33:06.940
 or are the sorts of machines

33:06.940 --> 33:09.360
 that you are describing curious by design?

33:10.220 --> 33:14.300
 Yeah, curiosity is a kind of a symptom,

33:14.300 --> 33:16.260
 not the goal.

33:16.260 --> 33:21.220
 So what happens is one of the big trade-offs

33:21.220 --> 33:22.280
 in reinforcement learning

33:22.280 --> 33:25.540
 is this exploration versus exploitation.

33:25.540 --> 33:27.420
 So when you know very little,

33:27.420 --> 33:29.600
 it pays off to explore a lot,

33:29.600 --> 33:31.340
 even suboptimal,

33:31.340 --> 33:32.940
 like even trajectories that seem like

33:32.940 --> 33:34.620
 they're not going to lead anywhere.

33:34.620 --> 33:36.180
 That's called exploration.

33:36.180 --> 33:38.600
 The smarter and smarter and smarter you get,

33:38.600 --> 33:41.820
 the more emphasis you put on exploitation,

33:41.820 --> 33:44.180
 meaning you take the best solution,

33:44.180 --> 33:45.780
 you take the best path.

33:45.780 --> 33:47.220
 Now through that process,

33:47.220 --> 33:52.220
 the exploration can look like curiosity by us humans,

33:52.700 --> 33:55.580
 but it's really just trying to get out of the local optimal,

33:55.580 --> 33:57.300
 the thing that's already discovered.

33:57.300 --> 34:00.180
 It's from an AI perspective,

34:00.180 --> 34:04.380
 it's always looking to optimize the objective function.

34:04.380 --> 34:08.160
 It derives, and we can talk about this a lot more,

34:08.160 --> 34:11.500
 but in terms of the tools of machine learning today,

34:11.500 --> 34:16.500
 it derives no pleasure from just the curiosity of like,

34:17.220 --> 34:19.300
 I don't know, discovery.

34:19.300 --> 34:20.140
 That moment.

34:20.140 --> 34:20.960
 So there's no dopamine for a machine.

34:20.960 --> 34:21.820
 There's no dopamine.

34:21.820 --> 34:23.980
 There's no reward system chemical

34:23.980 --> 34:26.880
 or I guess electronic reward system.

34:26.880 --> 34:30.380
 That said, if you look at machine learning literature

34:30.380 --> 34:32.060
 and reinforcement learning literature,

34:32.060 --> 34:34.020
 they will use like deep mind,

34:34.020 --> 34:35.740
 we use terms like dopamine.

34:35.740 --> 34:38.820
 We're constantly trying to use the human brain

34:38.820 --> 34:41.820
 to inspire totally new solutions to these problems.

34:41.820 --> 34:42.720
 So they'll think like,

34:42.720 --> 34:44.940
 how does dopamine function in the human brain?

34:44.940 --> 34:49.120
 And how can that lead to more interesting ways

34:49.120 --> 34:51.460
 to discover optimal solutions?

34:51.460 --> 34:53.200
 But ultimately, currently,

34:54.580 --> 34:57.460
 there has to be a formal objective function.

34:57.460 --> 34:58.660
 Now you could argue that humans

34:58.660 --> 35:00.460
 also has a set of objective functions

35:00.460 --> 35:01.860
 we're trying to optimize.

35:01.860 --> 35:04.500
 We're just not able to introspect them.

35:04.500 --> 35:07.800
 Yeah, we don't actually know what we're looking for

35:07.800 --> 35:09.320
 and seeking and doing.

35:09.320 --> 35:10.700
 Well, like Lisa Feldman Barrett,

35:10.700 --> 35:13.420
 she's spoken with at least on Instagram.

35:13.420 --> 35:14.780
 I hope you get her through you.

35:14.780 --> 35:17.620
 Yeah, I hope you actually have her on this podcast.

35:17.620 --> 35:18.800
 That'd be terrific.

35:18.800 --> 35:20.660
 So she has a very,

35:22.500 --> 35:25.920
 it has to do with homeostasis like that.

35:26.780 --> 35:28.980
 Basically there's a very dumb objective function

35:28.980 --> 35:30.860
 that the brain is trying to optimize,

35:30.860 --> 35:32.940
 like to keep like body temperature the same.

35:32.940 --> 35:34.300
 Like there's a very dumb

35:34.300 --> 35:36.460
 kind of optimization function happening.

35:36.460 --> 35:39.540
 And then what we humans do with our fancy consciousness

35:39.540 --> 35:42.320
 and cognitive abilities is we tell stories to ourselves

35:42.320 --> 35:44.060
 so we can have nice podcasts,

35:44.060 --> 35:46.740
 but really it's the brain trying to maintain

35:48.080 --> 35:50.720
 just like healthy state, I guess.

35:50.720 --> 35:51.860
 That's fascinating.

35:51.860 --> 35:55.520
 I also see the human brain

35:55.520 --> 35:58.940
 and I hope artificial intelligence systems

35:58.940 --> 36:03.940
 as not just systems that solve problems or optimize a goal,

36:04.180 --> 36:06.260
 but also storytellers.

36:06.260 --> 36:08.820
 I think there's a power to telling stories.

36:08.820 --> 36:10.060
 We tell stories to each other.

36:10.060 --> 36:11.660
 That's what communication is.

36:11.660 --> 36:16.660
 Like when you're alone, that's when you solve problems.

36:16.680 --> 36:19.040
 That's when it makes sense to talk about solving problems.

36:19.040 --> 36:20.820
 But when you're a community,

36:20.820 --> 36:24.020
 the capability to communicate, tell stories,

36:25.420 --> 36:28.220
 share ideas in such a way that those ideas are stable

36:28.220 --> 36:29.980
 over a long period of time,

36:29.980 --> 36:33.260
 that's like, that's being a charismatic storyteller.

36:33.260 --> 36:35.860
 And I think both humans are very good at this.

36:35.860 --> 36:40.220
 Arguably, I would argue that's why we are who we are

36:40.220 --> 36:42.260
 is we're great storytellers.

36:42.260 --> 36:44.780
 And then AI, I hope will also become that.

36:44.780 --> 36:47.460
 So it's not just about being able to solve problems

36:47.460 --> 36:49.020
 with a clear objective function.

36:49.020 --> 36:51.900
 It's afterwards be able to tell like a way better,

36:51.900 --> 36:53.340
 like make up a way better story

36:53.340 --> 36:55.740
 about why you did something or why you failed.

36:55.740 --> 36:59.840
 So you think that robots and or machines of some sort

36:59.840 --> 37:02.340
 are going to start telling humans stories?

37:02.340 --> 37:03.440
 Well, definitely.

37:03.440 --> 37:07.340
 So the technical field for that is called explainable AI,

37:07.340 --> 37:09.300
 explainable artificial intelligence

37:09.300 --> 37:14.160
 is trying to figure out how you get the AI system

37:14.160 --> 37:17.520
 to explain to us humans why the hell it failed

37:17.520 --> 37:18.880
 or why it succeeded.

37:19.740 --> 37:22.340
 Or there's a lot of different sort of versions of this

37:22.340 --> 37:26.300
 or to visualize how it understands the world.

37:26.300 --> 37:28.100
 That's a really difficult problem,

37:28.100 --> 37:33.100
 especially with neural networks that are famously opaque,

37:33.160 --> 37:36.320
 that they, we don't understand in many cases

37:36.320 --> 37:40.480
 why a particular neural network does what it does so well.

37:40.480 --> 37:43.700
 And to try to figure out where it's going to fail,

37:43.700 --> 37:46.220
 that requires the AI to explain itself.

37:46.220 --> 37:48.340
 There's a huge amount of money,

37:48.340 --> 37:52.340
 like there's a huge amount of money in this,

37:52.340 --> 37:54.540
 especially from government funding and so on.

37:54.540 --> 37:59.460
 Because if you want to deploy AI systems in the real world,

37:59.460 --> 38:02.640
 we humans at least want to ask it a question,

38:02.640 --> 38:04.260
 like why the hell did you do that?

38:04.260 --> 38:08.820
 Like in a dark way, why did you just kill that person?

38:08.820 --> 38:10.620
 Right, like if a car ran over a person,

38:10.620 --> 38:12.980
 we wouldn't understand why that happened.

38:12.980 --> 38:17.880
 And now again, we're sometimes very unfair to AI systems

38:17.880 --> 38:21.900
 because we humans can often not explain why very well.

38:21.900 --> 38:25.560
 But that's the field of explainable AI.

38:25.560 --> 38:28.180
 That's very, people are very interested in

38:28.180 --> 38:31.500
 because the more and more we rely on AI systems,

38:31.500 --> 38:35.660
 like the Twitter recommender system, that AI algorithm,

38:35.660 --> 38:39.140
 that's I would say impacting elections,

38:39.140 --> 38:41.980
 perhaps starting wars or at least military conflict.

38:41.980 --> 38:43.660
 That's that algorithm.

38:43.660 --> 38:46.580
 We want to ask that algorithm, first of all,

38:46.580 --> 38:48.500
 do you know what the hell you're doing?

38:48.500 --> 38:50.300
 Do you know, do you understand

38:50.300 --> 38:52.900
 the society level effects you're having?

38:52.900 --> 38:55.820
 And can you explain the possible other trajectories?

38:55.820 --> 38:58.300
 Like we would have that kind of conversation with a human.

38:58.300 --> 39:00.020
 We want to be able to do that with an AI.

39:00.020 --> 39:02.020
 And on my own personal level,

39:02.020 --> 39:05.420
 I think it would be nice to talk to AI systems

39:05.420 --> 39:10.100
 for stupid stuff, like robots, when they fail to-

39:11.620 --> 39:12.900
 Why'd you fall down the stairs?

39:12.900 --> 39:15.860
 Yeah, but not an engineering question,

39:15.860 --> 39:18.740
 but almost like a endearing question.

39:18.740 --> 39:22.580
 Like I'm looking for, if I fell

39:22.580 --> 39:25.020
 and you and I were hanging out,

39:25.020 --> 39:28.020
 I don't think you need an explanation

39:28.020 --> 39:29.860
 exactly what were the dynamic,

39:29.860 --> 39:32.740
 like what was the under actuated system problem here?

39:32.740 --> 39:36.180
 Like what was the texture of the floor or so on?

39:36.180 --> 39:37.580
 Or like what was the-

39:37.580 --> 39:39.020
 I want to know what you're thinking.

39:39.020 --> 39:41.200
 That, or you might joke about like,

39:41.200 --> 39:43.100
 you're drunk again, go home or something.

39:43.100 --> 39:44.860
 Like there could be humor in it.

39:44.860 --> 39:46.940
 That's an opportunity,

39:46.940 --> 39:51.040
 like storytelling isn't just explanation of what happened.

39:51.040 --> 39:54.380
 It's something that makes people laugh,

39:54.380 --> 39:56.100
 makes people fall in love,

39:56.100 --> 39:58.980
 makes people dream and understand things

39:58.980 --> 40:01.980
 in a way that poetry makes people understand things

40:01.980 --> 40:06.980
 as opposed to a rigorous log of where every sensor was,

40:07.220 --> 40:09.560
 where every actuator was.

40:09.560 --> 40:12.460
 I mean, I find this incredible because,

40:12.460 --> 40:16.220
 one of the hallmarks of severe autism spectrum disorders

40:16.220 --> 40:21.220
 is a report of experience from the autistic person

40:21.860 --> 40:25.340
 that is very much a catalog of action steps.

40:25.340 --> 40:26.380
 It's like, how do you feel today?

40:26.380 --> 40:27.960
 And they'll say, well, I got up and I did this

40:27.960 --> 40:29.080
 and then I did this and I did this.

40:29.080 --> 40:32.180
 And it's not at all the way that a person with,

40:32.180 --> 40:35.700
 who doesn't have autism spectrum disorder would respond.

40:35.700 --> 40:38.740
 And the way you describe these machines

40:38.740 --> 40:43.740
 has so much humanism or so much of a human

40:44.020 --> 40:45.420
 and biological element.

40:45.420 --> 40:48.020
 But I realized that we were talking about machines.

40:48.020 --> 40:51.660
 I want to make sure that I understand

40:51.660 --> 40:56.660
 if there's a distinction between a machine that learns,

40:57.860 --> 41:01.180
 a machine with artificial intelligence and a robot.

41:01.180 --> 41:03.840
 At what point does a machine become a robot?

41:03.840 --> 41:06.460
 So if I have a ballpoint pen,

41:06.460 --> 41:08.660
 I'm assuming I wouldn't call that a robot,

41:08.660 --> 41:12.420
 but if my ballpoint pen can come to me

41:12.420 --> 41:15.340
 when I moved to the opposite side of the table,

41:15.340 --> 41:17.940
 if it moves by whatever mechanism,

41:17.940 --> 41:20.660
 at that point, does it become a robot?

41:20.660 --> 41:23.420
 Okay, there's a million ways to explore this question.

41:23.420 --> 41:25.060
 It's a fascinating one.

41:25.060 --> 41:28.100
 So first of all, there's a question of what is life?

41:29.220 --> 41:32.540
 Like how do you know something is a living form and not?

41:32.540 --> 41:35.460
 And it's similar to the question of when does sort of a,

41:35.460 --> 41:40.140
 maybe a cold computational system becomes a,

41:40.140 --> 41:41.860
 well, we're already loading these words

41:41.860 --> 41:44.140
 with a lot of meaning, robot and machine.

41:44.140 --> 41:49.140
 But so one, I think movement is important,

41:50.300 --> 41:52.340
 but that's kind of a boring idea

41:52.340 --> 41:54.580
 that a robot is just a machine

41:54.580 --> 41:56.600
 that's able to act in the world.

41:56.600 --> 42:00.040
 So one, artificial intelligence could be

42:00.040 --> 42:01.800
 both just the thinking thing,

42:01.800 --> 42:04.040
 which I think is what machine learning is,

42:04.040 --> 42:05.720
 and also the acting thing,

42:05.720 --> 42:07.900
 which is what we usually think about robots.

42:07.900 --> 42:10.180
 So robots are the things that have a perception system

42:10.180 --> 42:11.580
 that's able to take in the world,

42:11.580 --> 42:13.140
 however you define the world,

42:13.140 --> 42:14.600
 is able to think and learn

42:14.600 --> 42:16.700
 and do whatever the hell it does inside

42:16.700 --> 42:18.700
 and then act on the world.

42:18.700 --> 42:21.580
 So that's the difference between maybe an AI system

42:21.580 --> 42:23.220
 or a machine and a robot.

42:23.220 --> 42:24.280
 It's something that's able,

42:24.280 --> 42:27.220
 a robot is something that's able to perceive the world

42:27.220 --> 42:28.180
 and act in the world.

42:28.180 --> 42:31.080
 So it could be through language or sound,

42:31.080 --> 42:32.660
 or it could be through movement or both.

42:32.660 --> 42:36.100
 Yeah, and I think it could also be in the digital space,

42:36.100 --> 42:39.020
 as long as there's a aspect of entity

42:39.020 --> 42:41.260
 that's inside the machine

42:41.260 --> 42:44.220
 and a world that's outside the machine,

42:44.220 --> 42:46.460
 and there's a sense in which the machine

42:46.460 --> 42:49.340
 is sensing that world and acting in it.

42:49.340 --> 42:50.900
 So we could, for instance,

42:50.900 --> 42:52.600
 there could be a version of a robot,

42:52.600 --> 42:55.400
 according to the definition that I think you're providing,

42:55.400 --> 42:58.260
 where the robot, where I go to sleep at night

42:58.260 --> 43:01.420
 and this robot goes and forges for information

43:01.420 --> 43:03.680
 that it thinks I want to see

43:03.680 --> 43:05.280
 loaded onto my desktop in the morning.

43:05.280 --> 43:07.000
 There was no movement of that machine.

43:07.000 --> 43:07.840
 There was no language,

43:07.840 --> 43:11.140
 but it essentially has movement in cyberspace.

43:11.140 --> 43:16.140
 Yeah, there's a distinction that I think is important

43:18.400 --> 43:23.400
 in that there's an element of it being an entity,

43:24.180 --> 43:26.620
 whether it's in the digital or the physical space.

43:26.620 --> 43:31.620
 So when you have something like Alexa in your home,

43:32.280 --> 43:35.140
 most of the speech recognition,

43:35.140 --> 43:36.720
 most of what Alexa is doing

43:36.720 --> 43:39.220
 is constantly being sent back to the mothership.

43:42.060 --> 43:44.940
 When Alexa is there on its own,

43:44.940 --> 43:46.940
 that's, to me, a robot,

43:46.940 --> 43:49.460
 when it's there interacting with the world.

43:49.460 --> 43:54.460
 When it's simply a finger of the main mothership,

43:54.460 --> 43:56.660
 then Alexa is not a robot.

43:56.660 --> 43:58.660
 Then it's just an interaction device.

43:58.660 --> 44:02.380
 Then maybe the main Amazon Alexa AI,

44:02.380 --> 44:04.800
 big, big system is the robot.

44:04.800 --> 44:08.840
 So that's important because there's some element

44:08.840 --> 44:10.600
 to us humans, I think,

44:10.600 --> 44:12.560
 where we want there to be an entity,

44:12.560 --> 44:14.740
 whether in the digital or the physical space.

44:14.740 --> 44:16.740
 That's where ideas of consciousness come in

44:16.740 --> 44:18.560
 and all those kinds of things

44:18.560 --> 44:21.400
 that we project our understanding

44:21.400 --> 44:23.180
 what it means to be a being.

44:23.180 --> 44:27.020
 And so to take that further,

44:27.020 --> 44:28.980
 when does a machine become a robot?

44:31.360 --> 44:35.240
 I think there's a special moment.

44:35.240 --> 44:37.840
 There's a special moment in a person's life,

44:37.840 --> 44:40.780
 in a robot's life where it surprises you.

44:41.640 --> 44:44.280
 I think surprise is a really powerful thing

44:44.280 --> 44:46.840
 where you know how the thing works

44:46.840 --> 44:48.440
 and yet it surprises you.

44:49.460 --> 44:51.960
 That's a magical moment for us humans.

44:51.960 --> 44:54.640
 So whether it's a chess playing program

44:54.640 --> 44:57.600
 that does something that you haven't seen before

44:57.600 --> 44:59.000
 that makes people smile,

44:59.000 --> 45:03.080
 like, huh, those moments happen with alpha zero

45:03.080 --> 45:05.560
 for the first time in chess playing

45:05.560 --> 45:08.760
 or grandmasters were really surprised by a move.

45:08.760 --> 45:10.240
 They didn't understand the move

45:10.240 --> 45:11.600
 and then they studied and study

45:11.600 --> 45:13.380
 and then they understood it.

45:13.380 --> 45:15.280
 But that moment of surprise,

45:15.280 --> 45:17.340
 that's for grandmasters in chess.

45:17.340 --> 45:20.400
 I find that moment of surprise really powerful,

45:20.400 --> 45:23.320
 really magical in just everyday life.

45:23.320 --> 45:26.460
 Because it supersedes the human brain in that moment?

45:27.320 --> 45:31.000
 Not supersedes like outperforms,

45:31.000 --> 45:35.400
 but surprises you in a positive sense.

45:35.400 --> 45:37.980
 Like I didn't think you could do that.

45:37.980 --> 45:40.720
 I didn't think that you had that in you.

45:40.720 --> 45:45.160
 And I think that moment is a big transition for a robot

45:45.160 --> 45:48.120
 from a moment of being a servant

45:48.120 --> 45:51.160
 that accomplishes a particular task

45:51.160 --> 45:52.840
 with some level of accuracy,

45:52.840 --> 45:57.840
 with some rate of failure to an entity,

45:57.920 --> 46:01.920
 a being that's struggling just like you are in this world.

46:01.920 --> 46:04.440
 And that's a really important moment

46:04.440 --> 46:07.560
 that I think you're not gonna find many people

46:07.560 --> 46:09.920
 in the AI community that talk like I just did.

46:11.400 --> 46:14.360
 I'm not speaking like some philosopher or some hippie.

46:14.360 --> 46:16.800
 I'm speaking from purely engineering perspective.

46:16.800 --> 46:20.520
 I think it's really important for robots to become entities

46:20.520 --> 46:23.360
 and explore that as a real engineering problem,

46:23.360 --> 46:25.880
 as opposed to everybody treats robots

46:25.880 --> 46:27.680
 in the robotics community.

46:27.680 --> 46:29.560
 They don't even call them a he or she.

46:29.560 --> 46:32.240
 They don't give them, try to avoid giving them names.

46:32.240 --> 46:36.720
 They really want to see it like a system, like a servant.

46:36.720 --> 46:40.280
 They see it as a servant that's trying to accomplish a task.

46:40.280 --> 46:44.660
 To me, I don't think I'm just romanticizing the notion.

46:44.660 --> 46:46.200
 I think it's a being.

46:46.200 --> 46:48.880
 It's currently perhaps a dumb being,

46:48.880 --> 46:53.080
 but in the long arc of history,

46:53.080 --> 46:55.200
 humans are pretty dumb beings too, so.

46:55.200 --> 46:57.120
 I would agree with that statement.

46:57.120 --> 46:59.560
 So I tend to really want to explore

46:59.560 --> 47:04.460
 this treating robots really as entities.

47:05.720 --> 47:08.400
 So like anthropomorphization,

47:08.400 --> 47:12.480
 which is the sort of the act of looking at a inanimate object

47:12.480 --> 47:15.460
 and projecting onto it lifelike features,

47:15.460 --> 47:20.460
 I think robotics generally sees that as a negative.

47:21.240 --> 47:23.860
 I see it as a superpower.

47:23.860 --> 47:26.740
 Like that, we need to use that.

47:26.740 --> 47:29.560
 Well, I'm struck by how that really grabs on

47:29.560 --> 47:32.800
 to the relationship between human and machine

47:32.800 --> 47:34.060
 or human and robot.

47:34.060 --> 47:37.400
 So it's the simple question is,

47:37.400 --> 47:38.880
 and I think you've already told us the answer,

47:38.880 --> 47:43.040
 but does interacting with a robot change you?

47:43.040 --> 47:46.580
 Does it, in other words, do we develop relationships

47:46.580 --> 47:48.040
 to robots?

47:48.040 --> 47:50.240
 Yeah, I definitely think so.

47:50.240 --> 47:55.240
 I think the moment you see a robot or AI systems

47:55.720 --> 47:57.680
 as more than just servants,

47:57.680 --> 48:01.240
 but entities, they begin to change,

48:01.240 --> 48:04.880
 just like good friends do, just like relationships,

48:04.880 --> 48:06.760
 just like other humans.

48:06.760 --> 48:09.900
 I think for that, you have to have certain aspects

48:09.900 --> 48:11.440
 of that interaction,

48:11.440 --> 48:15.820
 like the robot's ability to say no,

48:15.820 --> 48:19.080
 to have its own sense of identity,

48:19.080 --> 48:21.720
 to have its own set of goals

48:21.720 --> 48:23.120
 that's not constantly serving you,

48:23.120 --> 48:24.920
 but instead trying to understand the world

48:24.920 --> 48:27.200
 and do that dance of understanding

48:27.200 --> 48:28.920
 through communication with you.

48:28.920 --> 48:30.720
 So I definitely think there's a,

48:31.800 --> 48:35.200
 I mean, I have a lot of thoughts about this, as you may know,

48:35.200 --> 48:38.720
 and that's at the core of my lifelong dream, actually,

48:38.720 --> 48:39.840
 of what I want to do,

48:39.840 --> 48:44.840
 which is I believe that most people have

48:46.160 --> 48:50.280
 a notion of loneliness in them that we haven't discovered,

48:50.280 --> 48:53.120
 that we haven't explored, I should say.

48:53.120 --> 48:57.860
 And I see AI systems as helping us explore that

48:57.860 --> 49:00.240
 so that we can become better humans,

49:00.240 --> 49:02.280
 better people towards each other.

49:02.280 --> 49:06.800
 So I think that connection between human and AI,

49:06.800 --> 49:11.280
 human and robot is not only possible,

49:11.280 --> 49:14.520
 but will help us understand ourselves

49:14.520 --> 49:17.500
 in ways that are like several orders of magnitude,

49:18.760 --> 49:21.320
 deeper than we ever could have imagined.

49:21.320 --> 49:22.600
 I tend to believe that,

49:24.360 --> 49:29.360
 well, I have very wild levels of belief

49:32.440 --> 49:34.440
 in terms of how impactful that would be.

49:34.440 --> 49:38.280
 All right, so when I think about human relationships,

49:38.280 --> 49:41.140
 I don't always break them down into variables,

49:41.140 --> 49:43.400
 but we could explore a few of those variables

49:43.400 --> 49:47.340
 and see how they map to human-robot relationships.

49:47.340 --> 49:49.160
 One is just time, right?

49:49.160 --> 49:52.720
 If you spend zero time with another person at all

49:52.720 --> 49:55.380
 in cyberspace or on the phone or in person,

49:55.380 --> 49:58.120
 you essentially have no relationship to them.

49:58.120 --> 49:59.600
 If you spend a lot of time, you have a relationship.

49:59.600 --> 50:01.800
 This is obvious, but I guess one variable would be time,

50:01.800 --> 50:05.180
 how much time you spend with the other entity,

50:05.180 --> 50:06.520
 robot or human.

50:06.520 --> 50:10.000
 The other would be wins and successes.

50:10.000 --> 50:12.020
 You know, you enjoy successes together.

50:13.840 --> 50:16.680
 I'll give an absolutely trivial example of this in a moment,

50:16.680 --> 50:19.160
 but the other would be failures.

50:19.160 --> 50:21.520
 When you struggle with somebody,

50:21.520 --> 50:23.560
 whether or not you struggle between one another,

50:23.560 --> 50:25.320
 you disagree, like I was really struck

50:25.320 --> 50:27.040
 by the fact that you said that robot's saying no.

50:27.040 --> 50:30.080
 I've never thought about a robot saying no to me,

50:30.080 --> 50:31.160
 but there it is.

50:31.160 --> 50:34.040
 I look forward to you being one of the first people

50:34.040 --> 50:35.160
 to send this robot to.

50:35.160 --> 50:36.320
 So do I.

50:36.320 --> 50:37.640
 So there's struggle.

50:37.640 --> 50:40.200
 You grow, you know, when you struggle with somebody,

50:40.200 --> 50:41.120
 you grow closer.

50:41.120 --> 50:43.600
 Sometimes the struggles are imposed

50:43.600 --> 50:45.920
 between those two people, so-called trauma bonding.

50:45.920 --> 50:48.320
 They call it in the whole psychology literature

50:48.320 --> 50:50.120
 and pop psychology literature.

50:50.120 --> 50:52.200
 But in any case, I could imagine,

50:52.200 --> 50:57.200
 so time, successes together, struggle together,

50:57.600 --> 51:00.960
 and then just peaceful time, hanging out at home,

51:00.960 --> 51:04.740
 watching movies, waking up near one another.

51:06.160 --> 51:08.160
 Here, we're breaking down the kind of elements

51:08.160 --> 51:09.840
 of relationships of any kind.

51:10.680 --> 51:13.880
 So do you think that these elements apply

51:13.880 --> 51:16.400
 to robot-human relationships?

51:16.400 --> 51:21.400
 And if so, then I could see how if the robot

51:22.640 --> 51:25.880
 is its own entity and has some autonomy

51:25.880 --> 51:27.160
 in terms of how it reacts to you,

51:27.160 --> 51:28.960
 it's not just there just to serve you.

51:28.960 --> 51:30.200
 It's not just a servant.

51:30.200 --> 51:32.800
 It actually has opinions and can tell you

51:32.800 --> 51:34.440
 when maybe your thinking is flawed

51:34.440 --> 51:35.760
 or your actions are flawed.

51:35.760 --> 51:37.160
 It can also leave.

51:37.160 --> 51:39.360
 It could also leave.

51:39.360 --> 51:40.920
 So I've never conceptualized

51:40.920 --> 51:42.660
 robot-human interactions this way.

51:43.920 --> 51:46.500
 So tell me more about how this might look.

51:46.500 --> 51:50.220
 Are we thinking about a human-appearing robot?

51:51.360 --> 51:53.600
 I know you and I have both had intense relationships

51:53.600 --> 51:57.240
 to our, we have separate dogs, obviously, but to animals.

51:57.240 --> 51:59.120
 This sounds a lot like human-animal interaction.

51:59.120 --> 52:04.120
 So what is the ideal human-robot relationship?

52:04.480 --> 52:06.360
 So there's a lot to be said here,

52:06.360 --> 52:09.520
 but you actually pinpointed one of the big,

52:09.520 --> 52:13.760
 big first steps, which is this idea of time.

52:13.760 --> 52:15.500
 And it's a huge limitation

52:15.500 --> 52:18.560
 in machine learning community currently.

52:18.560 --> 52:21.460
 Now we're back to like the actual details.

52:21.460 --> 52:26.340
 Lifelong learning is a problem space

52:26.340 --> 52:29.600
 that focuses on how AI systems can learn

52:29.600 --> 52:30.960
 over a long period of time.

52:31.940 --> 52:35.400
 What's currently most machine learning systems

52:35.400 --> 52:37.880
 are not able to do is to,

52:37.880 --> 52:39.780
 all of the things you've listed under time,

52:39.780 --> 52:41.820
 the successes, the failures,

52:41.820 --> 52:44.600
 or just chilling together watching movies,

52:44.600 --> 52:47.960
 AI systems are not able to do that,

52:47.960 --> 52:51.040
 which is all the beautiful, magical moments

52:51.040 --> 52:53.880
 that I believe are the days filled with.

52:53.880 --> 52:57.440
 They're not able to keep track of those together with you.

52:57.440 --> 52:59.200
 Because they can't move with you and be with you.

52:59.200 --> 53:02.080
 No, no, like literally we don't have the techniques

53:02.080 --> 53:03.440
 to do the learning,

53:03.440 --> 53:07.200
 the actual learning of containing those moments.

53:07.200 --> 53:09.960
 Current machine learning systems are really focused

53:09.960 --> 53:12.480
 on understanding the world in the following way.

53:12.480 --> 53:14.340
 It's more like the perception system,

53:14.340 --> 53:19.340
 like looking around, understand like what's in the scene,

53:20.000 --> 53:22.240
 that there's a bunch of people sitting down,

53:22.240 --> 53:24.880
 that there is cameras and microphones,

53:24.880 --> 53:27.520
 that there's a table, understand that.

53:27.520 --> 53:30.920
 But the fact that we shared this moment of talking today

53:30.920 --> 53:34.160
 and still remember that for next time you're,

53:34.160 --> 53:36.660
 for like next time you're doing something,

53:36.660 --> 53:38.360
 remember that this moment happened.

53:38.360 --> 53:40.400
 We don't know how to do that technique wise.

53:40.400 --> 53:44.160
 This is what I'm hoping to innovate on

53:44.160 --> 53:47.080
 as I think it's a very, very important component

53:47.080 --> 53:49.940
 of what it means to create a deep relationship,

53:49.940 --> 53:52.060
 that sharing of moments together.

53:52.060 --> 53:54.080
 Could you post a photo of you and the robot,

53:54.080 --> 53:55.800
 like selfie with robot,

53:55.800 --> 53:58.320
 and then the robot sees that image

53:58.320 --> 54:00.960
 and recognizes that was time spent,

54:00.960 --> 54:03.600
 there were smiles or there were tears,

54:03.600 --> 54:08.600
 and create some sort of metric of emotional depth

54:09.040 --> 54:11.640
 in the relationship and update its behavior?

54:11.640 --> 54:15.160
 So could it text you in the middle of the night and say,

54:15.160 --> 54:16.800
 why haven't you texted me back?

54:16.800 --> 54:18.640
 Well, yes, all of those things,

54:18.640 --> 54:21.420
 but we can dig into that.

54:21.420 --> 54:24.820
 But I think that time element, forget everything else,

54:24.820 --> 54:29.200
 just sharing moments together, that changes everything.

54:29.200 --> 54:30.940
 I believe that changes everything.

54:30.940 --> 54:33.560
 There's specific things that are more in terms of systems

54:33.560 --> 54:35.360
 that I can explain you.

54:37.000 --> 54:39.360
 It's more technical and probably a little bit off line

54:39.360 --> 54:41.320
 because I have kind of wild ideas

54:41.320 --> 54:44.760
 how that can revolutionize social networks

54:44.760 --> 54:47.660
 and operating systems.

54:47.660 --> 54:50.640
 But the point is that element alone,

54:50.640 --> 54:53.160
 forget all the other things we're talking about,

54:53.160 --> 54:56.080
 like emotions, saying no, all that,

54:56.080 --> 54:58.600
 just remember sharing moments together

54:58.600 --> 55:00.260
 would change everything.

55:00.260 --> 55:01.680
 We don't currently have systems

55:01.680 --> 55:05.600
 that share moments together.

55:05.600 --> 55:08.080
 Like even just you and your fridge,

55:08.080 --> 55:11.080
 just all those times you went late at night

55:11.080 --> 55:13.280
 and ate the thing you shouldn't have eaten,

55:13.280 --> 55:16.680
 that was a secret moment you had with your refrigerator.

55:16.680 --> 55:17.960
 You shared that moment,

55:17.960 --> 55:20.360
 that darkness or that beautiful moment

55:20.360 --> 55:24.060
 where you just like heartbroken for some reason,

55:24.060 --> 55:26.320
 you're eating that ice cream or whatever,

55:26.320 --> 55:27.600
 that's a special moment.

55:27.600 --> 55:29.640
 And that refrigerator was there for you.

55:29.640 --> 55:31.840
 And the fact that it missed the opportunity

55:31.840 --> 55:36.000
 to remember that is tragic.

55:36.000 --> 55:37.820
 And once it does remember that,

55:38.760 --> 55:42.440
 I think you're gonna be very attached to the refrigerator.

55:42.440 --> 55:45.720
 You're gonna go through some hell with that refrigerator.

55:45.720 --> 55:49.640
 Most of us have like in the developed world

55:49.640 --> 55:51.520
 have weird relationships with food, right?

55:51.520 --> 55:54.880
 So you can go through some deep moments

55:54.880 --> 55:57.280
 of trauma and triumph with food.

55:57.280 --> 55:59.200
 And at the core of that is the refrigerator.

55:59.200 --> 56:04.200
 So a smart refrigerator, I believe would change society,

56:04.960 --> 56:06.200
 not just the refrigerator,

56:06.200 --> 56:10.200
 but these ideas in the systems all around us.

56:10.200 --> 56:12.680
 So I just wanna comment on how powerful

56:12.680 --> 56:14.240
 the idea of time is.

56:14.240 --> 56:17.860
 And then there's a bunch of elements of actual interaction

56:17.860 --> 56:22.860
 of allowing you as a human to feel like you're being heard,

56:26.380 --> 56:30.080
 truly heard, truly understood that we human,

56:30.080 --> 56:33.040
 like deep friendship is like that, I think,

56:33.040 --> 56:36.940
 but we're still, there's still an element of selfishness.

56:36.940 --> 56:39.520
 There's still an element of not really being able

56:39.520 --> 56:40.720
 to understand another human.

56:40.720 --> 56:43.960
 And a lot of the times when you're going through

56:43.960 --> 56:46.680
 trauma together through difficult times and through

56:46.680 --> 56:49.560
 successes, you're actually starting to get that inkling

56:49.560 --> 56:51.200
 of understanding of each other.

56:51.200 --> 56:54.040
 But I think that can be done more aggressively,

56:55.380 --> 56:57.400
 more efficiently.

56:57.400 --> 56:59.280
 Like if you think of a great therapist,

56:59.280 --> 57:01.800
 I think I've never actually been to a therapist,

57:01.800 --> 57:03.200
 but I'm a believer.

57:03.200 --> 57:04.960
 I used to want to be a psychiatrist.

57:04.960 --> 57:06.320
 Do Russians go to therapists?

57:06.320 --> 57:08.000
 No, they don't, they don't.

57:08.000 --> 57:12.040
 And if they do the therapist don't live to tell the story.

57:12.040 --> 57:16.000
 No, I do believe in talk there,

57:16.000 --> 57:20.200
 which friendship is to me is talk therapy or like it's like,

57:20.200 --> 57:23.680
 it's you don't necessarily need to talk.

57:23.680 --> 57:27.120
 It's like just connecting through in the space of ideas

57:27.120 --> 57:28.880
 and the space of experiences.

57:28.880 --> 57:31.800
 And I think there's a lot of ideas of how to make AI

57:31.800 --> 57:35.000
 systems to be able to ask the right questions

57:35.000 --> 57:37.320
 and truly hear another human.

57:37.320 --> 57:40.320
 This is what we try to do with podcasting, right?

57:40.320 --> 57:42.620
 I think there's ways to do that with AI,

57:42.620 --> 57:47.620
 but above all else, just remembering the collection

57:47.820 --> 57:52.000
 of moments that make up the day, the week, the months.

57:52.000 --> 57:55.680
 I think you maybe have some of this as well.

57:55.680 --> 57:57.160
 Some of my closest friends still

57:57.160 --> 57:58.760
 are the friends from high school.

57:59.720 --> 58:02.960
 That's time we've been through a bunch of shit together.

58:02.960 --> 58:06.200
 And that like we're very different people,

58:06.200 --> 58:07.960
 but just the fact that we've been through that

58:07.960 --> 58:11.040
 and we remember those moments and those moments somehow

58:11.040 --> 58:14.200
 create a depth of connection like nothing else,

58:14.200 --> 58:15.840
 like you and your refrigerator.

58:17.080 --> 58:20.640
 I love that because my graduate advisor,

58:20.640 --> 58:22.600
 unfortunately she passed away, but when she passed away,

58:22.600 --> 58:25.780
 somebody said at her memorial,

58:27.400 --> 58:29.400
 all these amazing things she had done, et cetera.

58:29.400 --> 58:32.880
 And then her kids got up there and she had young children

58:32.880 --> 58:35.480
 that I knew as they were when she was pregnant with them.

58:35.480 --> 58:38.440
 And so it was really, even now I can feel like

58:38.440 --> 58:39.880
 your heart gets heavy thinking about this.

58:39.880 --> 58:41.820
 They're going to grow up without their mother.

58:41.820 --> 58:42.660
 And it was really amazing.

58:42.660 --> 58:47.200
 Very, very strong young girls and now young women.

58:47.200 --> 58:49.300
 And what they said was incredible.

58:49.300 --> 58:51.720
 They said what they really appreciated most

58:51.720 --> 58:54.960
 about their mother, who was an amazing person,

58:55.800 --> 58:59.480
 is all the unstructured time they spent together.

58:59.480 --> 59:00.960
 So it wasn't the trips to the zoo.

59:00.960 --> 59:03.600
 It wasn't, oh, she woke up at five in the morning

59:03.600 --> 59:04.440
 and drove us to school.

59:04.440 --> 59:05.520
 She did all those things too.

59:05.520 --> 59:07.360
 She had two hour commute in each direction.

59:07.360 --> 59:09.360
 It was incredible, ran a lab, et cetera.

59:09.360 --> 59:11.440
 But it was the unstructured time.

59:11.440 --> 59:13.280
 So on the passing of their mother,

59:13.280 --> 59:16.520
 that's what they remembered was the biggest give

59:16.520 --> 59:18.380
 and what bonded them to her was all the time

59:18.380 --> 59:20.400
 where they just kind of hung out.

59:20.400 --> 59:22.000
 And the way you described the relationship

59:22.000 --> 59:27.000
 to a refrigerator is so, I want to say human-like,

59:27.200 --> 59:28.720
 but I'm almost reluctant to say that

59:28.720 --> 59:31.420
 because what I'm realizing as we're talking

59:31.420 --> 59:34.400
 is that what we think of as human-like

59:34.400 --> 59:39.000
 might actually be a lower form of relationship.

59:39.000 --> 59:42.360
 There may be relationships that are far better

59:42.360 --> 59:43.720
 than the sorts of relationships

59:43.720 --> 59:46.840
 that we can conceive in our minds right now

59:46.840 --> 59:50.060
 based on what these machine relationship interactions

59:50.060 --> 59:51.080
 could teach us.

59:51.080 --> 59:52.840
 Do I have that right?

59:52.840 --> 59:54.080
 Yeah, I think so.

59:54.080 --> 59:55.720
 I think there's no reason to see machines

59:55.720 --> 59:59.880
 as somehow incapable of teaching us something

59:59.880 --> 1:00:01.600
 that's deeply human.

1:00:01.600 --> 1:00:04.980
 I don't think humans have a monopoly on that.

1:00:04.980 --> 1:00:06.920
 I think we understand ourselves very poorly

1:00:06.920 --> 1:00:11.920
 and we need to have the kind of prompting from a machine.

1:00:13.960 --> 1:00:16.780
 And definitely part of that is just remembering the moments.

1:00:16.780 --> 1:00:17.880
 Remembering the moments.

1:00:18.920 --> 1:00:22.180
 I think the unstructured time together,

1:00:24.240 --> 1:00:27.520
 I wonder if it's quite so unstructured.

1:00:27.520 --> 1:00:30.520
 That's like calling this podcast unstructured time.

1:00:30.520 --> 1:00:34.000
 Maybe what they meant was it wasn't a big outing.

1:00:34.000 --> 1:00:36.160
 There was no specific goal,

1:00:36.160 --> 1:00:39.660
 but a goal was created through the lack of a goal.

1:00:39.660 --> 1:00:40.560
 Like where you just hang out

1:00:40.560 --> 1:00:42.540
 and then you start playing thumb war

1:00:42.540 --> 1:00:45.100
 and you end up playing thumb war for an hour.

1:00:45.100 --> 1:00:48.900
 So the structure emerges from lack of structure.

1:00:48.900 --> 1:00:51.420
 No, but the thing is the moments,

1:00:52.300 --> 1:00:54.320
 there's something about those times

1:00:54.320 --> 1:00:56.440
 that create special moments.

1:00:56.440 --> 1:01:00.280
 And I think that those could be optimized for.

1:01:00.280 --> 1:01:01.900
 I think we think of like a big outing

1:01:01.900 --> 1:01:03.820
 as I don't know, going to Six Flags or something,

1:01:03.820 --> 1:01:08.180
 or some big, the Grand Canyon or go into some,

1:01:08.180 --> 1:01:11.660
 I don't know, I think we would need to,

1:01:11.660 --> 1:01:13.840
 we don't quite yet understand as humans

1:01:13.840 --> 1:01:15.700
 what creates magical moments.

1:01:15.700 --> 1:01:18.180
 I think there's possible to optimize a lot of those things.

1:01:18.180 --> 1:01:21.420
 And perhaps like podcasting is helping people discover that

1:01:21.420 --> 1:01:24.220
 like maybe the thing we want to optimize for

1:01:24.220 --> 1:01:29.220
 isn't necessarily like some sexy, like quick clips.

1:01:29.220 --> 1:01:34.220
 Maybe what we want is long form authenticity, depth, depth.

1:01:36.240 --> 1:01:38.740
 So we were trying to figure that out,

1:01:38.740 --> 1:01:42.260
 certainly from a deep connection between humans

1:01:42.260 --> 1:01:45.920
 and humans and AI systems, I think long conversations

1:01:45.920 --> 1:01:50.920
 or long periods of communication over a series of moments

1:01:52.280 --> 1:01:56.160
 like my new, perhaps seemingly insignificant

1:01:56.160 --> 1:01:58.080
 to the big ones, the big successes,

1:01:58.080 --> 1:02:01.580
 the big failures, those are all,

1:02:01.580 --> 1:02:04.420
 just stitching those together and talking throughout.

1:02:05.280 --> 1:02:06.600
 I think that's the formula

1:02:06.600 --> 1:02:08.240
 for a really, really deep connection

1:02:08.240 --> 1:02:11.940
 that from like a very specific engineering perspective

1:02:11.940 --> 1:02:15.560
 is I think a fascinating open problem

1:02:15.560 --> 1:02:18.400
 that hasn't been really worked on very much.

1:02:18.400 --> 1:02:21.840
 And for me, from a, if I have the guts

1:02:21.840 --> 1:02:24.800
 and I mean, there's a lot of things to say,

1:02:24.800 --> 1:02:29.360
 but one of it is guts as I'll build a startup around it.

1:02:29.360 --> 1:02:32.360
 Yeah, so let's talk about this startup

1:02:32.360 --> 1:02:34.800
 and let's talk about the dream.

1:02:34.800 --> 1:02:36.040
 You've mentioned this dream before

1:02:36.040 --> 1:02:37.280
 in our previous conversations,

1:02:37.280 --> 1:02:40.120
 always as little hints dropped here and there,

1:02:40.120 --> 1:02:41.360
 just for anyone listening,

1:02:41.360 --> 1:02:43.720
 there's never been an offline conversation about this dream.

1:02:43.720 --> 1:02:48.160
 I'm not privy to anything except what Lex says now.

1:02:48.160 --> 1:02:49.760
 And I realized that there's no way

1:02:49.760 --> 1:02:52.840
 to capture the full essence of a dream

1:02:52.840 --> 1:02:57.180
 in any kind of verbal statement in a way

1:02:57.180 --> 1:02:58.200
 that captures all of it.

1:02:58.200 --> 1:03:02.020
 But what is this dream that you've referred to now

1:03:02.020 --> 1:03:05.000
 several times when we've sat down together

1:03:05.000 --> 1:03:06.920
 and talked on the phone?

1:03:06.920 --> 1:03:09.100
 Maybe it's this company, maybe it's something distinct.

1:03:09.100 --> 1:03:11.660
 If you feel comfortable, it'd be great

1:03:11.660 --> 1:03:13.320
 if you could share a little bit about what that is.

1:03:13.320 --> 1:03:18.320
 Sure, so the way people express long-term vision,

1:03:19.060 --> 1:03:20.880
 I've noticed is quite different.

1:03:20.880 --> 1:03:24.720
 Like Elon is an example of somebody who can very crisply

1:03:24.720 --> 1:03:27.520
 say exactly what the goal is.

1:03:27.520 --> 1:03:29.800
 Also has to do with the fact the problems he's solving

1:03:29.800 --> 1:03:31.300
 have nothing to do with humans.

1:03:32.400 --> 1:03:36.400
 So my long-term vision is a little bit more difficult

1:03:36.400 --> 1:03:40.840
 to express in words, I've noticed, as I've tried.

1:03:40.840 --> 1:03:42.400
 It could be my brain's failure.

1:03:43.260 --> 1:03:45.400
 But there's a ways to sneak up to it.

1:03:45.400 --> 1:03:47.180
 So let me just say a few things.

1:03:47.180 --> 1:03:52.180
 Early on in life, and also in the recent years,

1:03:53.000 --> 1:03:55.140
 I've interacted with a few robots

1:03:55.140 --> 1:03:57.220
 where I understood there's magic there.

1:03:58.200 --> 1:04:01.260
 And that magic could be shared by millions

1:04:02.140 --> 1:04:04.640
 if it's brought to light.

1:04:04.640 --> 1:04:07.620
 When I first met Spot from Boston Dynamics,

1:04:07.620 --> 1:04:10.400
 I realized there's magic there that nobody else is seeing.

1:04:10.400 --> 1:04:11.240
 Is the dog.

1:04:11.240 --> 1:04:12.280
 Is the dog, sorry.

1:04:12.280 --> 1:04:17.280
 The Spot is the four-legged robot from Boston Dynamics.

1:04:17.280 --> 1:04:20.380
 Some people might have seen it, it's this yellow dog.

1:04:20.380 --> 1:04:25.380
 And sometimes in life, you just notice something

1:04:26.620 --> 1:04:28.460
 that just grabs you.

1:04:28.460 --> 1:04:31.420
 And I believe that this is something that,

1:04:32.380 --> 1:04:34.820
 this magic is something that could be

1:04:34.820 --> 1:04:37.060
 every single device in the world.

1:04:38.260 --> 1:04:41.380
 The way that I think maybe Steve Jobs

1:04:41.380 --> 1:04:43.180
 thought about the personal computer.

1:04:44.420 --> 1:04:46.700
 Woz didn't think about the personal computer this way,

1:04:46.700 --> 1:04:48.060
 but Steve did.

1:04:48.060 --> 1:04:50.340
 Which is like, he thought that the personal computer

1:04:50.340 --> 1:04:52.100
 should be as thin as a sheet of paper

1:04:52.100 --> 1:04:53.640
 and everybody should have one.

1:04:53.640 --> 1:04:58.640
 I mean, this idea, I think it is heartbreaking

1:04:58.820 --> 1:05:02.500
 that we're getting, the world is being filled up

1:05:02.500 --> 1:05:05.000
 with machines that are soulless.

1:05:06.060 --> 1:05:10.260
 And I think every one of them can have that same magic.

1:05:10.260 --> 1:05:14.780
 One of the things that also inspired me

1:05:14.780 --> 1:05:17.660
 in terms of a startup is that magic can be engineered

1:05:17.660 --> 1:05:19.560
 much easier than I thought.

1:05:19.560 --> 1:05:22.620
 That's my intuition with everything I've ever built

1:05:22.620 --> 1:05:23.960
 and worked on.

1:05:23.960 --> 1:05:28.860
 So the dream is to add a bit of that magic

1:05:28.860 --> 1:05:32.360
 in every single computing system in the world.

1:05:32.360 --> 1:05:36.500
 So the way that Windows operating system for a long time

1:05:36.500 --> 1:05:39.460
 was the primary operating system everybody interacted with.

1:05:39.460 --> 1:05:41.300
 They built apps on top of it.

1:05:41.300 --> 1:05:45.780
 I think this is something that should be as a layer.

1:05:45.780 --> 1:05:47.740
 It's almost as an operating system

1:05:47.740 --> 1:05:51.000
 in every device that humans interacted with in the world.

1:05:51.000 --> 1:05:53.060
 Now what that actually looks like,

1:05:53.060 --> 1:05:56.500
 the actual dream when I was especially a kid,

1:05:57.940 --> 1:06:01.140
 it didn't have this concrete form of a business.

1:06:01.140 --> 1:06:06.140
 It had more of a dream of exploring your own loneliness

1:06:06.140 --> 1:06:11.140
 by interacting with machines, robots.

1:06:13.340 --> 1:06:15.720
 This deep connection between humans and robots

1:06:15.720 --> 1:06:17.180
 was always a dream.

1:06:17.180 --> 1:06:20.140
 And so for me, I'd love to see a world

1:06:20.140 --> 1:06:22.460
 where there's every home has a robot

1:06:22.460 --> 1:06:27.460
 and not a robot that washes the dishes or a sex robot,

1:06:27.860 --> 1:06:31.440
 or I don't know, I think of any kind of activity

1:06:31.440 --> 1:06:33.840
 the robot can do, but more like a companion.

1:06:33.840 --> 1:06:36.620
 A family member, the way a dog is,

1:06:37.660 --> 1:06:41.980
 but a dog that's able to speak your language too.

1:06:41.980 --> 1:06:45.160
 So not just connect the way a dog does

1:06:45.160 --> 1:06:46.800
 by looking at you and looking away

1:06:46.800 --> 1:06:51.100
 and almost like smiling with its soul in that kind of way,

1:06:51.100 --> 1:06:54.580
 but also to actually understand what the hell,

1:06:54.580 --> 1:06:56.740
 like why are you so excited about the successes?

1:06:56.740 --> 1:06:59.900
 Like understand the details, understand the traumas.

1:06:59.900 --> 1:07:04.420
 And I just think that has always filled me

1:07:04.420 --> 1:07:07.060
 with the excitement that I could,

1:07:07.060 --> 1:07:12.060
 with artificial intelligence, bring joy to a lot of people.

1:07:12.300 --> 1:07:15.580
 More recently, I've been more and more

1:07:17.380 --> 1:07:21.700
 heartbroken to see the kind of division, derision,

1:07:23.320 --> 1:07:27.100
 even hate that's boiling up on the internet

1:07:27.100 --> 1:07:28.620
 through social networks.

1:07:28.620 --> 1:07:32.780
 And I thought this kind of mechanism is exactly applicable

1:07:32.780 --> 1:07:34.780
 in the context of social networks as well.

1:07:34.780 --> 1:07:38.420
 So it's an operating system that serves

1:07:38.420 --> 1:07:43.420
 as your guide on the internet.

1:07:43.500 --> 1:07:45.740
 One of the biggest problems with YouTube

1:07:45.740 --> 1:07:47.960
 and social networks currently

1:07:47.960 --> 1:07:49.880
 is they're optimizing for engagement.

1:07:50.760 --> 1:07:52.540
 I think if you create AI systems

1:07:52.540 --> 1:07:54.920
 that know each individual person,

1:07:54.920 --> 1:07:59.080
 you're able to optimize for long-term growth,

1:07:59.080 --> 1:08:00.800
 for long-term happiness.

1:08:00.800 --> 1:08:01.640
 Of the individual?

1:08:01.640 --> 1:08:03.720
 Of the individual, of the individual.

1:08:03.720 --> 1:08:08.480
 And there's a lot of other things to say, which is the,

1:08:09.440 --> 1:08:14.440
 in order for AI systems to learn everything about you,

1:08:15.760 --> 1:08:17.880
 they need to collect, they need to,

1:08:17.880 --> 1:08:19.960
 just like you and I, when we talk offline,

1:08:19.960 --> 1:08:21.520
 we're collecting data about each other,

1:08:21.520 --> 1:08:23.520
 secrets about each other.

1:08:23.520 --> 1:08:26.600
 The same way AI has to do that.

1:08:26.600 --> 1:08:28.720
 And that allows you to,

1:08:28.720 --> 1:08:33.720
 and that requires you to rethink ideas of ownership of data.

1:08:35.460 --> 1:08:39.920
 I think each individual should own all of their data

1:08:39.920 --> 1:08:41.920
 and very easily be able to leave.

1:08:41.920 --> 1:08:43.600
 Just like AI systems can leave,

1:08:43.600 --> 1:08:48.200
 humans can disappear and delete all of their data

1:08:48.200 --> 1:08:49.400
 in a moment's notice,

1:08:49.400 --> 1:08:54.220
 which is actually better than we humans can do.

1:08:54.220 --> 1:08:56.880
 Once we load the data into each other, it's there.

1:08:56.880 --> 1:09:00.420
 I think it's very important to be both,

1:09:00.420 --> 1:09:02.900
 give people complete control over their data

1:09:03.920 --> 1:09:06.180
 in order to establish trust that they can trust you.

1:09:06.180 --> 1:09:08.480
 And the second part of trust is transparency.

1:09:09.460 --> 1:09:11.900
 Whenever the data is used to make it very clear

1:09:11.900 --> 1:09:13.060
 what it's being used for.

1:09:13.060 --> 1:09:16.120
 And not clear in a lawyerly legal sense,

1:09:16.120 --> 1:09:18.780
 but clear in a way that people really understand

1:09:18.780 --> 1:09:19.620
 what it's used for.

1:09:19.620 --> 1:09:21.500
 I believe when people have the ability

1:09:21.500 --> 1:09:24.160
 to delete all their data and walk away

1:09:24.160 --> 1:09:27.900
 and know how the data is being used, I think they'll stay.

1:09:29.860 --> 1:09:31.860
 The possibility of a clean breakup

1:09:31.860 --> 1:09:33.380
 is actually what will keep people together.

1:09:33.380 --> 1:09:34.420
 Yeah, I think so.

1:09:34.420 --> 1:09:36.380
 I think, yeah, exactly.

1:09:36.380 --> 1:09:39.700
 I think a happy marriage requires the ability

1:09:39.700 --> 1:09:44.700
 to divorce easily without the divorce industrial complex

1:09:46.060 --> 1:09:48.040
 or whatever is currently going on.

1:09:48.040 --> 1:09:50.740
 There's so much money to be made from lawyers and divorce.

1:09:50.740 --> 1:09:53.980
 But yeah, the ability to leave is what enables love,

1:09:53.980 --> 1:09:55.220
 I think.

1:09:55.220 --> 1:09:57.340
 It's interesting, I've heard the phrase

1:09:57.340 --> 1:09:58.860
 from a semi-cynical friend

1:09:58.860 --> 1:10:01.580
 that marriage is the leading cause of divorce.

1:10:01.580 --> 1:10:03.560
 But now we've heard that divorce

1:10:03.560 --> 1:10:04.980
 or the possibility of divorce

1:10:04.980 --> 1:10:06.620
 could be the leading cause of marriage.

1:10:06.620 --> 1:10:08.100
 Of a happy marriage.

1:10:08.100 --> 1:10:08.940
 Good point.

1:10:08.940 --> 1:10:09.920
 Of a happy marriage.

1:10:09.920 --> 1:10:12.740
 So yeah, but there's a lot of details there.

1:10:12.740 --> 1:10:14.660
 But the big dream is that connection

1:10:14.660 --> 1:10:17.480
 between AI system and a human.

1:10:17.480 --> 1:10:18.320
 And I haven't,

1:10:20.240 --> 1:10:21.080
 there's so much fear

1:10:21.080 --> 1:10:23.860
 about artificial intelligence systems and about robots

1:10:23.860 --> 1:10:26.420
 that I haven't quite found the right words

1:10:26.420 --> 1:10:27.580
 to express that vision

1:10:27.580 --> 1:10:31.320
 because the vision I have is one,

1:10:31.320 --> 1:10:33.500
 it's not like some naive delusional vision

1:10:33.500 --> 1:10:36.820
 of technology is gonna save everybody.

1:10:36.820 --> 1:10:40.300
 I really do just have a positive view

1:10:40.300 --> 1:10:44.700
 of ways AI systems can help humans explore themselves.

1:10:44.700 --> 1:10:47.540
 I love that positivity and I agree

1:10:47.540 --> 1:10:52.540
 that the stance everything is doomed is equally bad

1:10:54.400 --> 1:10:56.560
 to say that everything's gonna turn out all right.

1:10:56.560 --> 1:10:58.300
 There has to be a dedicated effort.

1:10:58.300 --> 1:11:00.380
 And clearly you're thinking

1:11:00.380 --> 1:11:02.500
 about what that dedicated effort would look like.

1:11:02.500 --> 1:11:06.580
 You mentioned two aspects to this dream.

1:11:06.580 --> 1:11:07.820
 And I wanna make sure that I understand

1:11:07.820 --> 1:11:10.020
 where they connect if they do

1:11:10.020 --> 1:11:12.400
 or if these are independent streams.

1:11:12.400 --> 1:11:17.260
 One was this hypothetical robot family member

1:11:17.260 --> 1:11:19.340
 or some other form of robot

1:11:19.340 --> 1:11:20.980
 that would allow people to experience

1:11:20.980 --> 1:11:25.980
 the kind of delight that you experienced many times

1:11:27.020 --> 1:11:30.280
 and that you would like the world to be able to have.

1:11:30.280 --> 1:11:33.500
 And it's such a beautiful idea of this give.

1:11:33.500 --> 1:11:36.180
 And the other is social media

1:11:36.180 --> 1:11:38.160
 or social network platforms

1:11:38.160 --> 1:11:40.820
 that really serve individuals

1:11:40.820 --> 1:11:44.160
 and their best selves and their happiness and their growth.

1:11:44.160 --> 1:11:45.600
 Is there crossover between those

1:11:45.600 --> 1:11:47.060
 or are these two parallel dreams?

1:11:47.060 --> 1:11:48.500
 It's 100% the same thing.

1:11:48.500 --> 1:11:50.900
 It's difficult to kind of explain

1:11:50.900 --> 1:11:52.100
 without going through details,

1:11:52.100 --> 1:11:54.620
 but maybe one easy way to explain

1:11:54.620 --> 1:11:56.700
 the way I think about social networks

1:11:56.700 --> 1:11:59.660
 is to create an AI system that's yours.

1:11:59.660 --> 1:12:00.660
 That's yours.

1:12:00.660 --> 1:12:03.220
 It's not like Amazon Alexa that's centralized.

1:12:03.220 --> 1:12:04.540
 You own the data.

1:12:04.540 --> 1:12:07.460
 It's like your little friend

1:12:07.460 --> 1:12:11.340
 that becomes your representative on Twitter

1:12:11.340 --> 1:12:16.220
 that helps you find things that will make you feel good,

1:12:16.220 --> 1:12:19.900
 that will also challenge your thinking to make you grow,

1:12:19.900 --> 1:12:24.900
 but not let you get lost in the negative spiral of dopamine

1:12:26.860 --> 1:12:29.420
 that gets you to be angry

1:12:29.420 --> 1:12:34.240
 or most just get you to be not open to learning.

1:12:34.240 --> 1:12:36.220
 And so that little representative

1:12:36.220 --> 1:12:38.780
 is optimizing your long-term health.

1:12:40.360 --> 1:12:45.360
 And I believe that that is not only good for human beings,

1:12:45.620 --> 1:12:47.360
 it's also good for business.

1:12:47.360 --> 1:12:50.480
 I think long-term you can make a lot of money

1:12:50.480 --> 1:12:53.060
 by challenging this idea

1:12:53.060 --> 1:12:57.340
 that the only way to make money is maximizing engagement.

1:12:57.340 --> 1:12:59.700
 And one of the things that people disagree with me on

1:12:59.700 --> 1:13:02.420
 is they think Twitter's always going to win.

1:13:02.420 --> 1:13:04.880
 Like maximizing engagement is always going to win.

1:13:04.880 --> 1:13:06.300
 I don't think so.

1:13:06.300 --> 1:13:09.500
 I think people have woken up now to understanding

1:13:09.500 --> 1:13:12.740
 that they don't always feel good,

1:13:12.740 --> 1:13:16.220
 the ones who are on Twitter a lot,

1:13:16.220 --> 1:13:19.300
 that they don't always feel good at the end of the week.

1:13:19.300 --> 1:13:24.000
 I would love feedback from whatever this creature,

1:13:24.000 --> 1:13:26.740
 whatever, I can't, I don't know what to call it,

1:13:26.740 --> 1:13:28.720
 as to maybe at the end of the week,

1:13:28.720 --> 1:13:30.520
 it would automatically unfollow

1:13:30.520 --> 1:13:31.940
 some of the people that I follow

1:13:31.940 --> 1:13:35.580
 because it realized through some really smart data

1:13:35.580 --> 1:13:37.120
 about how I was feeling inside

1:13:37.120 --> 1:13:38.540
 or how I was sleeping or something

1:13:38.540 --> 1:13:40.600
 that that just wasn't good for me,

1:13:40.600 --> 1:13:43.600
 but it might also put things and people in front of me

1:13:43.600 --> 1:13:45.500
 that I ought to see.

1:13:45.500 --> 1:13:48.660
 Is that kind of a sliver of what this looks like?

1:13:48.660 --> 1:13:50.540
 The whole point, because of the interaction,

1:13:50.540 --> 1:13:53.580
 because of sharing the moments

1:13:53.580 --> 1:13:55.100
 and learning a lot about you,

1:13:56.600 --> 1:13:59.560
 you're now able to understand

1:13:59.560 --> 1:14:01.960
 what interactions led you to become

1:14:01.960 --> 1:14:04.280
 a better version of yourself,

1:14:04.280 --> 1:14:07.280
 like the person you yourself are happy with.

1:14:07.280 --> 1:14:10.080
 This isn't, if you're into flat earth

1:14:11.300 --> 1:14:12.800
 and you feel very good about it,

1:14:12.800 --> 1:14:14.560
 that you believe the earth is flat,

1:14:15.720 --> 1:14:19.600
 like the idea that you should censor, that is ridiculous.

1:14:19.600 --> 1:14:21.000
 If it makes you feel good

1:14:21.000 --> 1:14:23.440
 and you're becoming the best version of yourself,

1:14:23.440 --> 1:14:24.600
 I think you should be getting

1:14:24.600 --> 1:14:26.420
 as much flat earth as possible.

1:14:26.420 --> 1:14:29.320
 Now, it's also good to challenge your ideas,

1:14:29.320 --> 1:14:34.320
 but not because the centralized committee decided,

1:14:34.560 --> 1:14:37.280
 but because you tell to the system

1:14:37.280 --> 1:14:39.280
 that you like challenging your ideas.

1:14:39.280 --> 1:14:40.800
 I think all of us do.

1:14:40.800 --> 1:14:44.040
 And then, which actually YouTube doesn't do that well,

1:14:44.040 --> 1:14:45.900
 once you go down the flat earth rabbit hole,

1:14:45.900 --> 1:14:47.160
 that's all you're gonna see.

1:14:47.160 --> 1:14:51.640
 It's nice to get some really powerful communicators

1:14:51.640 --> 1:14:53.600
 to argue against flat earth.

1:14:53.600 --> 1:14:57.020
 And it's nice to see that for you

1:14:57.020 --> 1:15:01.100
 and potentially at least long-term to expand your horizons,

1:15:01.100 --> 1:15:03.240
 maybe the earth is not flat.

1:15:03.240 --> 1:15:05.160
 But if you continue to live your whole life

1:15:05.160 --> 1:15:08.100
 thinking the earth is flat, I think,

1:15:08.100 --> 1:15:11.680
 and you're being a good father or son or daughter,

1:15:11.680 --> 1:15:14.360
 and like you're being the best version of yourself

1:15:14.360 --> 1:15:18.940
 and you're happy with yourself, I think the earth is flat.

1:15:18.940 --> 1:15:21.360
 So like, I think this kind of idea,

1:15:21.360 --> 1:15:24.220
 and I'm just using that kind of silly, ridiculous example,

1:15:24.220 --> 1:15:29.220
 because I don't like the idea of centralized forces

1:15:30.440 --> 1:15:33.240
 controlling what you can and can't see.

1:15:33.240 --> 1:15:36.880
 But I also don't like this idea of like,

1:15:36.880 --> 1:15:39.680
 not censoring anything,

1:15:39.680 --> 1:15:42.560
 because that's always the biggest problem with that

1:15:42.560 --> 1:15:45.840
 is there's a central decider.

1:15:45.840 --> 1:15:49.480
 I think you yourself can decide what you wanna see and not.

1:15:49.480 --> 1:15:54.040
 And it's good to have a companion that reminds you

1:15:54.040 --> 1:15:56.500
 that you felt shitty last time you did this,

1:15:56.500 --> 1:15:58.560
 or you felt good last time you did this.

1:15:58.560 --> 1:16:00.120
 I mean, I feel like in every good story,

1:16:00.120 --> 1:16:03.540
 there's a guide or a companion that flies out

1:16:03.540 --> 1:16:06.440
 or forages a little bit further or a little bit differently

1:16:06.440 --> 1:16:08.420
 and brings back information that helps us,

1:16:08.420 --> 1:16:11.360
 or at least tries to steer us in the right direction.

1:16:11.360 --> 1:16:16.360
 So that's exactly what I'm thinking

1:16:16.600 --> 1:16:17.800
 and what I've been working on.

1:16:17.800 --> 1:16:20.820
 I should mention there's a bunch of difficulties here.

1:16:20.820 --> 1:16:24.200
 You see me up and down a little bit recently.

1:16:24.200 --> 1:16:27.600
 So there's technically a lot of challenges here.

1:16:28.480 --> 1:16:30.480
 Like with a lot of technologies,

1:16:30.480 --> 1:16:34.360
 and the reason I'm talking about it on a podcast comfortably

1:16:34.360 --> 1:16:38.500
 as opposed to working in secret is it's really hard.

1:16:38.500 --> 1:16:41.000
 And maybe it's time has not come.

1:16:41.960 --> 1:16:44.140
 And that's something you have to constantly struggle with

1:16:44.140 --> 1:16:46.980
 in terms of like entrepreneurially as a startup.

1:16:48.060 --> 1:16:50.640
 Like I've also mentioned to you maybe offline,

1:16:50.640 --> 1:16:52.540
 I really don't care about money.

1:16:52.540 --> 1:16:55.280
 I don't care about business success,

1:16:55.280 --> 1:16:56.620
 all those kinds of things.

1:16:58.480 --> 1:17:01.120
 So it's a difficult decision to make

1:17:01.120 --> 1:17:05.260
 how much of your time do you want to go all in here

1:17:05.260 --> 1:17:06.760
 and give everything to this?

1:17:07.620 --> 1:17:11.560
 It's a big roll of the dice because I've also realized

1:17:11.560 --> 1:17:14.480
 that working on some of these problems,

1:17:14.480 --> 1:17:18.040
 both with the robotics and the technical side

1:17:18.040 --> 1:17:21.000
 in terms of the machine learning system

1:17:21.000 --> 1:17:26.000
 that I'm describing, it's lonely, it's really lonely

1:17:26.800 --> 1:17:31.420
 because both on a personal level and a technical level.

1:17:31.420 --> 1:17:32.480
 So on the technical level,

1:17:32.480 --> 1:17:37.480
 I'm surrounded by people that kind of doubt me,

1:17:37.900 --> 1:17:40.560
 which I think all entrepreneurs go through.

1:17:40.560 --> 1:17:42.800
 And they doubt you in the following sense.

1:17:42.800 --> 1:17:46.800
 They know how difficult it is.

1:17:46.800 --> 1:17:49.560
 Like the people that, the colleagues of mine,

1:17:49.560 --> 1:17:52.320
 they know how difficult lifelong learning is.

1:17:52.320 --> 1:17:53.920
 They also know how difficult it is

1:17:53.920 --> 1:17:56.320
 to build a system like this,

1:17:56.320 --> 1:17:59.360
 to build a competitive social network.

1:17:59.360 --> 1:18:04.360
 And in general, there's a kind of a loneliness

1:18:05.040 --> 1:18:08.800
 to just working on something on your own

1:18:08.800 --> 1:18:10.280
 for long periods of time.

1:18:10.280 --> 1:18:12.400
 And you start to doubt whether,

1:18:13.320 --> 1:18:16.320
 given that you don't have a track record of success,

1:18:16.320 --> 1:18:17.980
 like that's a big one.

1:18:17.980 --> 1:18:20.520
 When you look in the mirror, especially when you're young,

1:18:20.520 --> 1:18:22.680
 but I still have that on most things,

1:18:22.680 --> 1:18:26.520
 you look in the mirror and you have these big dreams.

1:18:26.520 --> 1:18:30.800
 How do you know you're actually as smart

1:18:30.800 --> 1:18:32.480
 as you think you are?

1:18:32.480 --> 1:18:34.240
 Like, how do you know you're going to be able

1:18:34.240 --> 1:18:35.420
 to accomplish this dream?

1:18:35.420 --> 1:18:36.520
 You have this ambition.

1:18:36.520 --> 1:18:40.920
 You sort of don't, but you're kind of pulling on a string

1:18:40.920 --> 1:18:42.960
 hoping that there's a bigger ball of yarn.

1:18:42.960 --> 1:18:45.000
 Yeah, but you have this kind of intuition.

1:18:45.000 --> 1:18:50.000
 I think I pride myself in knowing what I'm good at

1:18:51.360 --> 1:18:54.440
 because the reason I have that intuition

1:18:54.440 --> 1:18:57.560
 is because I think I'm very good at knowing

1:18:57.560 --> 1:19:01.280
 all the things I suck at, which is basically everything.

1:19:01.280 --> 1:19:04.520
 So like, whenever I notice like, wait a minute,

1:19:04.520 --> 1:19:08.180
 I'm kind of good at this, which is very rare for me.

1:19:08.180 --> 1:19:11.520
 I think like that might be a ball of yarn worth pulling at.

1:19:11.520 --> 1:19:14.760
 And the thing with, in terms of engineering systems

1:19:14.760 --> 1:19:16.560
 that are able to interact with humans,

1:19:16.560 --> 1:19:18.520
 I think I'm very good at that.

1:19:18.520 --> 1:19:22.040
 And it's because we talk about podcasting and so on.

1:19:22.040 --> 1:19:23.600
 I don't know if I'm very good at podcasting.

1:19:23.600 --> 1:19:25.200
 You're very good at podcasting.

1:19:25.200 --> 1:19:27.200
 But I certainly don't.

1:19:27.200 --> 1:19:30.760
 I think maybe it is compelling for people

1:19:30.760 --> 1:19:35.360
 to watch a kindhearted idiot struggle with this form.

1:19:35.360 --> 1:19:37.200
 Maybe that's what's compelling.

1:19:37.200 --> 1:19:40.600
 But in terms of like actual being a good engineer

1:19:40.600 --> 1:19:45.560
 of human-robot interaction systems, I think I'm good.

1:19:45.560 --> 1:19:48.000
 But it's hard to know until you do it

1:19:48.000 --> 1:19:50.840
 and then the world keeps telling you you're not.

1:19:50.840 --> 1:19:53.800
 And it's just, it's full of doubt and it's really hard.

1:19:53.800 --> 1:19:55.320
 And I've been struggling with that recently.

1:19:55.320 --> 1:19:57.180
 It's kind of a fascinating struggle.

1:19:57.180 --> 1:19:59.780
 But then that's where the Goggins thing comes in,

1:19:59.780 --> 1:20:03.900
 is like, aside from the stay hard motherfucker,

1:20:03.900 --> 1:20:07.860
 is the like, whenever you're struggling,

1:20:07.860 --> 1:20:11.160
 that's a good sign that if you keep going,

1:20:11.160 --> 1:20:14.520
 that you're going to be alone in the success, right?

1:20:16.240 --> 1:20:18.660
 Well, in your case, however, I agree.

1:20:18.660 --> 1:20:20.340
 And actually David had a post recently

1:20:20.340 --> 1:20:23.240
 that I thought was among his many brilliant posts

1:20:23.240 --> 1:20:26.280
 was one of the more brilliant about how,

1:20:26.280 --> 1:20:27.860
 he talked about this myth of the light

1:20:27.860 --> 1:20:29.520
 at the end of the tunnel.

1:20:29.520 --> 1:20:33.460
 And instead what he replaced that myth with

1:20:33.460 --> 1:20:38.460
 was a concept that eventually your eyes adapt to the dark.

1:20:38.460 --> 1:20:40.320
 That the tunnel, it's not about a light at the end,

1:20:40.320 --> 1:20:42.800
 that it's really about adapting to the dark of the tunnel.

1:20:42.800 --> 1:20:43.640
 It's very Goggins-

1:20:43.640 --> 1:20:44.800
 I love him so much.

1:20:44.800 --> 1:20:48.880
 Yeah, you guys share a lot in common.

1:20:48.880 --> 1:20:52.520
 Knowing you both a bit, you share a lot in common.

1:20:52.520 --> 1:20:57.520
 But in this loneliness and the pursuit of this dream,

1:20:57.840 --> 1:21:01.320
 it seems to me it has a certain component to it

1:21:01.320 --> 1:21:04.040
 that is extremely valuable,

1:21:04.040 --> 1:21:08.000
 which is that the loneliness itself could serve as a driver

1:21:08.000 --> 1:21:10.480
 to build the companion for the journey.

1:21:10.480 --> 1:21:12.800
 Well, I'm very deeply aware of that.

1:21:13.680 --> 1:21:17.280
 So like some people can make,

1:21:17.280 --> 1:21:18.640
 cause I talk about love a lot.

1:21:18.640 --> 1:21:21.900
 I really love everything in this world,

1:21:21.900 --> 1:21:26.040
 but I also love humans, friendship and romantic,

1:21:26.040 --> 1:21:31.040
 you know, like even the cheesy stuff, just-

1:21:31.040 --> 1:21:32.440
 You like romantic movies.

1:21:32.440 --> 1:21:33.760
 Yeah, not those-

1:21:33.760 --> 1:21:35.160
 I'm just kidding.

1:21:35.160 --> 1:21:38.320
 Well, I got so much shit from Rogan about like,

1:21:38.320 --> 1:21:41.100
 what is it, the tango scene from a Scent of a Woman.

1:21:41.100 --> 1:21:44.000
 But yeah, I find like there's nothing better

1:21:44.000 --> 1:21:45.480
 than a woman in a red dress.

1:21:45.480 --> 1:21:49.420
 Like, you know, just like classy.

1:21:49.420 --> 1:21:51.040
 You should move to Argentina, Mike.

1:21:51.040 --> 1:21:52.240
 You know, my father's Argentine.

1:21:52.240 --> 1:21:54.760
 And you know what he said when I went on your podcast

1:21:54.760 --> 1:21:58.320
 for the first time, he said, he dresses well.

1:21:58.320 --> 1:22:00.240
 Because in Argentina, the men go to a wedding

1:22:00.240 --> 1:22:01.440
 or a party or something.

1:22:01.440 --> 1:22:03.660
 You know, in the U.S. by halfway through the night,

1:22:03.660 --> 1:22:05.800
 10 minutes in the night, all the jackets are off.

1:22:05.800 --> 1:22:07.600
 It looks like everyone's undressing for the party

1:22:07.600 --> 1:22:09.040
 they just got dressed up for.

1:22:09.040 --> 1:22:11.600
 And he said, you know, I like the way he dresses.

1:22:11.600 --> 1:22:13.200
 And then when I started, he was talking about you.

1:22:13.200 --> 1:22:15.840
 And then when I started my podcast, he said,

1:22:15.840 --> 1:22:19.320
 why don't you wear a real suit like your friend Lex?

1:22:19.320 --> 1:22:21.240
 I remember that.

1:22:23.440 --> 1:22:27.840
 But let's talk about this pursuit just a bit more.

1:22:27.840 --> 1:22:29.320
 Because I think what you're talking about

1:22:29.320 --> 1:22:33.920
 is building a, not just a solution for loneliness,

1:22:33.920 --> 1:22:36.080
 but you've alluded to the loneliness

1:22:36.080 --> 1:22:37.800
 as itself an important thing.

1:22:37.800 --> 1:22:38.640
 And I think you're right.

1:22:38.640 --> 1:22:43.640
 I think within people, there's like caverns of thoughts

1:22:44.120 --> 1:22:47.680
 and shame, but also just the desire to be,

1:22:47.680 --> 1:22:51.220
 to have resonance, to be seen and heard.

1:22:51.220 --> 1:22:52.440
 And I don't even know that it's seen

1:22:52.440 --> 1:22:53.760
 and heard through language.

1:22:53.760 --> 1:22:58.760
 But these reservoirs of loneliness, I think they're,

1:23:00.500 --> 1:23:01.340
 well, they're interesting.

1:23:01.340 --> 1:23:02.620
 Maybe you could comment a little bit about it

1:23:02.620 --> 1:23:05.160
 because just as often as you talk about love,

1:23:05.160 --> 1:23:06.080
 I haven't quantified it,

1:23:06.080 --> 1:23:08.440
 but it seems that you talk about this loneliness.

1:23:08.440 --> 1:23:10.160
 And maybe you just would, if you're willing,

1:23:10.160 --> 1:23:12.240
 you could share a little bit more about that

1:23:12.240 --> 1:23:15.300
 and what that feels like now in the pursuit

1:23:15.300 --> 1:23:18.360
 of building this robot-human relationship.

1:23:18.360 --> 1:23:20.460
 And you've been, let me be direct.

1:23:20.460 --> 1:23:23.800
 You've been spending a lot of time on building

1:23:23.800 --> 1:23:25.440
 a robot-human relationship.

1:23:26.340 --> 1:23:27.200
 Where's that at?

1:23:28.360 --> 1:23:33.360
 Oh, well, in terms of business and in terms of systems.

1:23:33.440 --> 1:23:35.540
 No, I'm talking about a specific robot.

1:23:35.540 --> 1:23:39.840
 Oh, so, okay, I should mention a few things.

1:23:39.840 --> 1:23:42.640
 So one is there's a startup where there's an idea

1:23:42.640 --> 1:23:46.000
 where I hope millions of people can use.

1:23:46.000 --> 1:23:49.240
 And then there's my own personal,

1:23:49.240 --> 1:23:52.040
 almost like Frankenstein explorations

1:23:52.040 --> 1:23:55.040
 with particular robots.

1:23:55.040 --> 1:23:59.200
 So I'm very fascinated with the legged robots

1:23:59.200 --> 1:24:03.120
 in my own private sounds like dark,

1:24:03.120 --> 1:24:06.440
 but like N of one experiments

1:24:06.440 --> 1:24:08.600
 to see if I can recreate the magic.

1:24:09.620 --> 1:24:14.160
 And that's been, I have a lot of really good already,

1:24:14.160 --> 1:24:17.080
 perception systems and control systems

1:24:17.080 --> 1:24:19.240
 that are able to communicate affection

1:24:19.240 --> 1:24:20.760
 in a dog-like fashion.

1:24:20.760 --> 1:24:22.520
 So I'm in a really good place there.

1:24:22.520 --> 1:24:23.560
 The stumbling blocks,

1:24:23.560 --> 1:24:27.080
 which also have been part of my sadness recently

1:24:27.080 --> 1:24:30.760
 is that I also have to work with robotics companies

1:24:30.760 --> 1:24:34.640
 that I gave so much of my heart, soul,

1:24:34.640 --> 1:24:37.940
 and love and appreciation towards Boston Dynamics.

1:24:37.940 --> 1:24:42.600
 But Boston Dynamics is also as a company

1:24:42.600 --> 1:24:43.840
 that has to make a lot of money

1:24:43.840 --> 1:24:45.560
 and they have marketing teams.

1:24:45.560 --> 1:24:48.000
 And they're like looking at this silly Russian kid

1:24:48.000 --> 1:24:49.320
 in a suit and tie.

1:24:49.320 --> 1:24:51.400
 It's like, what's he trying to do with all this love

1:24:51.400 --> 1:24:53.480
 and robot interaction and dancing and so on?

1:24:53.480 --> 1:24:58.480
 So there was, I think, let's say for now,

1:24:59.000 --> 1:25:01.200
 it's like when you break up with a girlfriend or something.

1:25:01.200 --> 1:25:04.060
 Right now we decided to part ways on this particular thing.

1:25:04.060 --> 1:25:05.900
 They're huge supporters of mine, they're huge fans.

1:25:05.900 --> 1:25:08.640
 But on this particular thing,

1:25:08.640 --> 1:25:12.640
 Boston Dynamics is not focusing on

1:25:12.640 --> 1:25:15.040
 or interested in human robot interaction.

1:25:15.040 --> 1:25:17.160
 In fact, their whole business currently

1:25:17.160 --> 1:25:20.000
 is keep the robot as far away from humans as possible.

1:25:20.960 --> 1:25:23.800
 Because it's in the industrial setting

1:25:23.800 --> 1:25:27.300
 where it's doing monitoring in dangerous environments.

1:25:27.300 --> 1:25:29.440
 It's almost like a remote security camera

1:25:29.440 --> 1:25:31.000
 essentially is its application.

1:25:32.040 --> 1:25:35.740
 To me, I thought it's still even in those applications

1:25:35.740 --> 1:25:37.840
 exceptionally useful for the robot

1:25:37.840 --> 1:25:41.440
 to be able to perceive humans, like see humans

1:25:41.440 --> 1:25:45.520
 and to be able to, in a big map,

1:25:45.520 --> 1:25:48.400
 localize where those humans are and have human intention.

1:25:48.400 --> 1:25:49.800
 For example, like this,

1:25:49.800 --> 1:25:52.080
 I did this a lot of work with pedestrians

1:25:52.080 --> 1:25:53.860
 for a robot to be able to anticipate

1:25:53.860 --> 1:25:57.560
 what the hell the human is doing, like where it's walking.

1:25:57.560 --> 1:25:59.200
 Humans are not ballistics object.

1:25:59.200 --> 1:26:00.920
 They're not, just because you're walking this way

1:26:00.920 --> 1:26:03.820
 one moment doesn't mean you'll keep walking that direction.

1:26:03.820 --> 1:26:05.400
 You have to infer a lot of signals,

1:26:05.400 --> 1:26:07.720
 especially the head movement and the eye movement.

1:26:07.720 --> 1:26:09.760
 So I thought that's super interesting to explore,

1:26:09.760 --> 1:26:11.680
 but they didn't feel that.

1:26:11.680 --> 1:26:14.480
 So I'll be working with a few other robotics companies

1:26:14.480 --> 1:26:18.360
 that are much more open to that kind of stuff.

1:26:18.360 --> 1:26:20.680
 And they're super excited and fans of mine.

1:26:20.680 --> 1:26:23.440
 And hopefully Boston Dynamics, my first love

1:26:23.440 --> 1:26:26.000
 that getting back with an ex-girlfriend will come around.

1:26:26.000 --> 1:26:29.520
 But so the algorithmically it's,

1:26:29.520 --> 1:26:33.380
 I'm basically done there.

1:26:33.380 --> 1:26:35.840
 The rest is actually getting

1:26:35.840 --> 1:26:37.200
 some of these companies to work with.

1:26:37.200 --> 1:26:41.600
 And then there's, for people who'd work with robots know

1:26:41.600 --> 1:26:44.880
 that one thing is to write software that works.

1:26:44.880 --> 1:26:48.440
 And the other is to have a real machine that actually works.

1:26:48.440 --> 1:26:50.860
 And it breaks down in all kinds of different ways

1:26:50.860 --> 1:26:51.720
 that are fascinating.

1:26:51.720 --> 1:26:53.920
 And so there's a big challenge there.

1:26:53.920 --> 1:26:58.760
 But that's almost, it may sound a little bit confusing

1:26:58.760 --> 1:27:01.280
 in the context of our previous discussion,

1:27:01.280 --> 1:27:03.660
 because the previous discussion was more

1:27:03.660 --> 1:27:07.040
 about the big dream, how I hoped to have millions of people

1:27:07.040 --> 1:27:09.420
 enjoy this moment of magic.

1:27:09.420 --> 1:27:12.160
 This current discussion about a robot

1:27:12.160 --> 1:27:15.080
 is something I personally really enjoy.

1:27:15.080 --> 1:27:16.620
 It just brings me happiness.

1:27:16.620 --> 1:27:20.960
 I really try to do now everything that just brings me joy.

1:27:20.960 --> 1:27:24.180
 I maximize that because robots are awesome.

1:27:24.180 --> 1:27:28.440
 But two, given my like a little bit growing platform,

1:27:28.440 --> 1:27:30.880
 I want to use the opportunity to educate people.

1:27:31.860 --> 1:27:34.080
 It's just like robots are cool.

1:27:34.080 --> 1:27:36.600
 And if I think they're cool, I'll be able to,

1:27:36.600 --> 1:27:39.620
 I hope be able to communicate why they're cool to others.

1:27:39.620 --> 1:27:42.160
 So this little robot experiment

1:27:42.160 --> 1:27:43.980
 is a little bit of a research project too.

1:27:43.980 --> 1:27:47.000
 There's a couple of publications with MIT folks around that.

1:27:47.000 --> 1:27:49.800
 But the other is just to make some cool videos

1:27:49.800 --> 1:27:52.220
 and explain to people how they actually work.

1:27:53.320 --> 1:27:56.640
 And as opposed to people being scared of robots,

1:27:56.640 --> 1:27:59.860
 they can still be scared, but also excited.

1:27:59.860 --> 1:28:03.120
 Like see the dark side, the beautiful side,

1:28:03.120 --> 1:28:07.720
 the magic of what it means to bring, you know,

1:28:07.720 --> 1:28:09.800
 for a machine to become a robot.

1:28:09.800 --> 1:28:13.880
 I want to inspire people with that, but that's less,

1:28:13.880 --> 1:28:18.120
 it's interesting because I think the big impact

1:28:18.120 --> 1:28:22.360
 in terms of the dream does not have to do with embodied AI.

1:28:22.360 --> 1:28:24.040
 So it does not need to have a body.

1:28:24.040 --> 1:28:28.680
 I think the refrigerator is enough that for an AI system,

1:28:28.680 --> 1:28:31.320
 just to have a voice and to hear you,

1:28:31.320 --> 1:28:33.400
 that's enough for loneliness.

1:28:33.400 --> 1:28:36.540
 The embodiment is just-

1:28:36.540 --> 1:28:38.480
 By embodiment, you mean the physical structure.

1:28:38.480 --> 1:28:41.280
 Physical instantiation of intelligence.

1:28:41.280 --> 1:28:44.780
 So it's a legged robot or even just a thing.

1:28:45.980 --> 1:28:48.520
 I have a few other humanoid robot,

1:28:48.520 --> 1:28:51.320
 a little humanoid robot, maybe I'll keep them on the table.

1:28:51.320 --> 1:28:54.720
 Just like walks around or even just like a mobile platform

1:28:54.720 --> 1:28:57.560
 that can just like turn around and look at you.

1:28:57.560 --> 1:28:59.200
 It's like we mentioned with the pen,

1:28:59.200 --> 1:29:02.040
 something that moves and can look at you.

1:29:02.040 --> 1:29:07.040
 It's like that butter robot that asks, what is my purpose?

1:29:11.280 --> 1:29:15.640
 That is really, it's almost like art.

1:29:15.640 --> 1:29:19.680
 There's something about a physical entity that moves around,

1:29:19.680 --> 1:29:22.620
 that's able to look at you and interact with you,

1:29:22.620 --> 1:29:25.920
 that makes you wonder what it means to be human.

1:29:25.920 --> 1:29:27.520
 It like challenges you to think,

1:29:27.520 --> 1:29:31.200
 if that thing looks like it has consciousness,

1:29:32.540 --> 1:29:33.920
 what the hell am I?

1:29:33.920 --> 1:29:35.080
 And I like that feeling.

1:29:35.080 --> 1:29:36.500
 I think that's really useful for us.

1:29:36.500 --> 1:29:41.080
 It's humbling for us humans, but that's less about research.

1:29:41.080 --> 1:29:42.660
 It's certainly less about business

1:29:42.660 --> 1:29:45.960
 and more about exploring our own selves

1:29:45.960 --> 1:29:47.800
 and challenging others to think like,

1:29:49.200 --> 1:29:52.880
 to think about what makes them human.

1:29:52.880 --> 1:29:56.360
 I love this desire to share the delight

1:29:56.360 --> 1:29:58.040
 of an interaction with a robot.

1:29:58.040 --> 1:29:58.880
 And as you describe it,

1:29:58.880 --> 1:30:01.340
 I actually, I find myself starting to crave that

1:30:01.340 --> 1:30:04.920
 because we all have those elements from childhood where,

1:30:04.920 --> 1:30:06.760
 or from adulthood where we experience something

1:30:06.760 --> 1:30:09.800
 we want other people to feel that.

1:30:09.800 --> 1:30:10.920
 And I think that you're right.

1:30:10.920 --> 1:30:12.560
 I think a lot of people are scared of AI.

1:30:12.560 --> 1:30:14.440
 I think a lot of people are scared of robots.

1:30:14.440 --> 1:30:18.840
 My only experience of a robotic like thing

1:30:18.840 --> 1:30:21.840
 is my Roomba vacuum where it goes about,

1:30:21.840 --> 1:30:24.280
 actually was pretty good at picking up Costello's hair

1:30:24.280 --> 1:30:25.480
 when he was shed.

1:30:25.480 --> 1:30:28.260
 And then, and I was grateful for it.

1:30:28.260 --> 1:30:30.440
 But then when I was on a call or something

1:30:30.440 --> 1:30:33.200
 and it would get caught on a wire or something,

1:30:33.200 --> 1:30:35.280
 I would find myself getting upset with the Roomba.

1:30:35.280 --> 1:30:37.520
 In that moment, I'm like, what are you doing?

1:30:37.520 --> 1:30:39.920
 And obviously it's just doing what it does.

1:30:39.920 --> 1:30:42.480
 But that's a kind of mostly positive

1:30:42.480 --> 1:30:44.140
 but slightly negative interaction.

1:30:45.480 --> 1:30:46.640
 But what you're describing,

1:30:46.640 --> 1:30:50.640
 it has so much more richness and layers of detail

1:30:50.640 --> 1:30:53.680
 that I can only imagine what those relationships are like.

1:30:53.680 --> 1:30:55.400
 Well, there's a few, just a quick comment.

1:30:55.400 --> 1:30:57.680
 So I've had, they're currently in Boston.

1:30:57.680 --> 1:31:00.560
 I have a bunch of Roombas for my robot.

1:31:00.560 --> 1:31:01.920
 And I did this experiment.

1:31:01.920 --> 1:31:03.080
 Wait, how many Roombas?

1:31:04.760 --> 1:31:05.880
 Sounds like a fleet of Roombas.

1:31:05.880 --> 1:31:08.640
 Yeah, so probably seven or eight.

1:31:08.640 --> 1:31:09.960
 Well, that's a lot of Roombas.

1:31:09.960 --> 1:31:11.400
 So- This place is very clean.

1:31:12.800 --> 1:31:14.760
 Well, so this, I'm kind of waiting.

1:31:14.760 --> 1:31:18.440
 This is the place we're currently in in Austin

1:31:18.440 --> 1:31:20.280
 is way larger than I need.

1:31:20.280 --> 1:31:25.280
 But I basically got it to make sure I have room for robots.

1:31:27.160 --> 1:31:30.400
 So you have these seven or so Roombas.

1:31:30.400 --> 1:31:32.080
 You deploy all seven at once?

1:31:32.080 --> 1:31:36.040
 Oh no, I do different experiments with them.

1:31:36.040 --> 1:31:38.860
 So one of the things I want to mention is this is,

1:31:38.860 --> 1:31:40.140
 I think there was a YouTube video

1:31:40.140 --> 1:31:41.780
 that inspired me to try this,

1:31:42.800 --> 1:31:47.800
 is I got them to scream in pain and moan in pain

1:31:47.800 --> 1:31:52.420
 whenever they were kicked or contacted.

1:31:53.380 --> 1:31:56.680
 And I did that experiment to see how I would feel.

1:31:56.680 --> 1:31:58.780
 I meant to do like a YouTube video on it,

1:31:58.780 --> 1:32:00.380
 but then it just seemed very cruel.

1:32:00.380 --> 1:32:03.420
 Did any Roomba rights activists come at you?

1:32:03.420 --> 1:32:07.620
 Like, I think if I release that video,

1:32:07.620 --> 1:32:09.680
 I think it's going to make me look insane,

1:32:09.680 --> 1:32:12.740
 which I know people know I'm already insane.

1:32:12.740 --> 1:32:14.540
 Now you have to release the video.

1:32:14.540 --> 1:32:18.300
 I think maybe if I contextualize it

1:32:18.300 --> 1:32:23.200
 by showing other robots like to show why this is fascinating

1:32:23.200 --> 1:32:26.020
 because ultimately I felt like they were human

1:32:26.020 --> 1:32:27.380
 almost immediately.

1:32:27.380 --> 1:32:30.340
 And that display of pain was what did that.

1:32:30.340 --> 1:32:31.500
 Giving them a voice.

1:32:31.500 --> 1:32:36.500
 Giving them a voice, especially a voice of dislike of pain.

1:32:37.180 --> 1:32:39.220
 I have to connect you to my friend, Eddie Chang.

1:32:39.220 --> 1:32:40.460
 He studies speech and language.

1:32:40.460 --> 1:32:42.980
 He's a neurosurgeon and we're lifelong friends.

1:32:42.980 --> 1:32:46.340
 He studies speech and language,

1:32:46.340 --> 1:32:50.160
 but he describes some of these more primitive,

1:32:50.160 --> 1:32:55.160
 visceral vocalizations, cries, groans, moans of delight,

1:32:56.440 --> 1:32:58.940
 other sounds as well, use your imagination,

1:32:58.940 --> 1:33:02.820
 as such powerful rudders for the other,

1:33:02.820 --> 1:33:04.620
 for the emotions of other people.

1:33:04.620 --> 1:33:06.060
 And so I find it fascinating.

1:33:06.060 --> 1:33:07.220
 I can't wait to see this video.

1:33:07.220 --> 1:33:09.480
 Is that, so is the video available online?

1:33:09.480 --> 1:33:13.660
 No, I haven't recorded it, I just had a bunch of Roombas

1:33:13.660 --> 1:33:18.240
 that are able to scream in pain in my Boston place.

1:33:20.580 --> 1:33:22.300
 Like people are ready as well.

1:33:22.300 --> 1:33:26.140
 Next podcast episode with Lex, maybe we'll have that one.

1:33:26.140 --> 1:33:26.980
 Who knows?

1:33:26.980 --> 1:33:28.220
 So the thing is like people,

1:33:28.220 --> 1:33:30.940
 I've noticed because I talk so much about love

1:33:30.940 --> 1:33:32.340
 and it's really who I am.

1:33:32.340 --> 1:33:35.660
 I think they wanna, to a lot of people,

1:33:35.660 --> 1:33:38.460
 it seems like there gotta be a dark person

1:33:38.460 --> 1:33:39.460
 in there somewhere.

1:33:39.460 --> 1:33:41.900
 And I thought if I release videos and Roombas screaming

1:33:41.900 --> 1:33:44.540
 and they're like, yep, yep, that guy's definitely insane.

1:33:44.540 --> 1:33:47.760
 What about like shouts of glee and delight?

1:33:47.760 --> 1:33:49.180
 You could do that too, right?

1:33:49.180 --> 1:33:51.660
 Well, I don't know how to,

1:33:51.660 --> 1:33:54.420
 to me delight is quiet, right?

1:33:54.420 --> 1:33:55.580
 Like you're Russian.

1:33:57.580 --> 1:33:59.860
 Americans are much louder than Russians.

1:33:59.860 --> 1:34:01.020
 Yeah, yeah.

1:34:01.020 --> 1:34:04.740
 But I don't, I mean, unless you're talking about like,

1:34:04.740 --> 1:34:06.460
 I don't know how you would have sexual relations

1:34:06.460 --> 1:34:07.300
 with a Roomba.

1:34:07.300 --> 1:34:10.260
 I wasn't necessarily saying sexual delight, but-

1:34:10.260 --> 1:34:12.660
 Trust me, I tried, I'm just kidding.

1:34:12.660 --> 1:34:14.100
 That's a joke, internet.

1:34:14.100 --> 1:34:16.740
 Okay, but I was fascinated in the psychology

1:34:16.740 --> 1:34:17.700
 of how little it took.

1:34:17.700 --> 1:34:20.560
 Cause you mentioned you had a negative relationship

1:34:20.560 --> 1:34:21.940
 with the Roomba.

1:34:21.940 --> 1:34:25.140
 Well, I'd find that mostly I took it for granted.

1:34:25.140 --> 1:34:27.540
 It just served me, it collected Costello's hair.

1:34:27.540 --> 1:34:29.180
 And then when it would do something I didn't like,

1:34:29.180 --> 1:34:30.220
 I would get upset with it.

1:34:30.220 --> 1:34:31.740
 So that's not a good relationship.

1:34:31.740 --> 1:34:35.020
 It was taken for granted and I would get upset

1:34:35.020 --> 1:34:36.060
 and then I'd park it again.

1:34:36.060 --> 1:34:39.020
 And I just like, you're in the corner.

1:34:39.020 --> 1:34:44.020
 Yeah, but there's a way to frame it being quite dumb

1:34:45.380 --> 1:34:49.620
 as a almost cute, almost connecting with it

1:34:49.620 --> 1:34:51.120
 for its dumbness.

1:34:51.120 --> 1:34:54.700
 And I think that's a artificial intelligence problem.

1:34:54.700 --> 1:34:55.540
 Interesting.

1:34:55.540 --> 1:34:59.280
 I think flaws should be a feature, not a bug.

1:34:59.280 --> 1:35:01.140
 So along the lines of this,

1:35:01.140 --> 1:35:03.220
 the different sorts of relationships that one could have

1:35:03.220 --> 1:35:04.440
 with robots and the fear,

1:35:04.440 --> 1:35:06.900
 but also some of the positive relationships

1:35:06.900 --> 1:35:07.900
 that one could have.

1:35:08.900 --> 1:35:10.300
 There's so much dimensionality.

1:35:10.300 --> 1:35:12.400
 There's so much to explore,

1:35:12.400 --> 1:35:16.300
 but power dynamics in relationships are very interesting

1:35:16.300 --> 1:35:20.640
 because the obvious ones that the unsophisticated view

1:35:20.640 --> 1:35:25.020
 of this is one, there's a master and a servant, right?

1:35:25.020 --> 1:35:27.780
 But there's also manipulation.

1:35:27.780 --> 1:35:29.820
 There's benevolent manipulation.

1:35:30.820 --> 1:35:32.020
 Children do this with parents.

1:35:32.020 --> 1:35:33.120
 Puppies do this.

1:35:33.120 --> 1:35:34.920
 Puppies turn their head and look cute

1:35:34.920 --> 1:35:37.560
 and maybe give out a little noise.

1:35:37.560 --> 1:35:39.020
 Kids, coo.

1:35:39.020 --> 1:35:41.540
 And parents always think that they're doing this

1:35:41.540 --> 1:35:44.980
 because they love the parent.

1:35:44.980 --> 1:35:48.460
 But in many ways, studies show that those coos are ways

1:35:48.460 --> 1:35:50.820
 to extract the sorts of behaviors and expressions

1:35:50.820 --> 1:35:51.820
 from the parent that they want.

1:35:51.820 --> 1:35:53.160
 The child doesn't know it's doing this.

1:35:53.160 --> 1:35:54.260
 It's completely subconscious,

1:35:54.260 --> 1:35:56.460
 but it's benevolent manipulation.

1:35:56.460 --> 1:35:59.720
 So there's one version of fear of robots

1:35:59.720 --> 1:36:02.180
 that I hear a lot about that I think most people

1:36:02.180 --> 1:36:04.540
 can relate to where the robots take over

1:36:04.540 --> 1:36:08.140
 and they become the masters and we become the servants.

1:36:08.140 --> 1:36:10.000
 But there could be another version

1:36:10.000 --> 1:36:15.000
 that in certain communities that I'm certainly not a part of

1:36:15.140 --> 1:36:17.220
 but they call topping from the bottom

1:36:17.220 --> 1:36:20.780
 where the robot is actually manipulating you

1:36:20.780 --> 1:36:24.180
 into doing things, but you are under the belief

1:36:24.180 --> 1:36:29.180
 that you are in charge, but actually they're in charge.

1:36:29.180 --> 1:36:33.740
 And so I think that's one that if we could explore

1:36:33.740 --> 1:36:35.540
 that for a second, you could imagine

1:36:35.540 --> 1:36:37.140
 it wouldn't necessarily be bad,

1:36:37.140 --> 1:36:39.100
 although it could lead to bad things.

1:36:40.120 --> 1:36:41.820
 The reason I want to explore this is I think people

1:36:41.820 --> 1:36:45.580
 always default to the extreme, like the robots take over

1:36:45.580 --> 1:36:48.100
 and we're in little jail cells and they're out having fun

1:36:48.100 --> 1:36:49.560
 and ruling the universe.

1:36:50.580 --> 1:36:54.100
 What sorts of manipulation can a robot

1:36:54.100 --> 1:36:56.500
 potentially carry out, good or bad?

1:36:56.500 --> 1:36:59.740
 Yeah, just so there's a lot of good and bad manipulation

1:36:59.740 --> 1:37:01.900
 between humans, right, just like you said.

1:37:04.400 --> 1:37:08.760
 To me, especially like you said,

1:37:09.900 --> 1:37:12.340
 topping from the bottom, is that the term?

1:37:12.340 --> 1:37:16.460
 I think someone from MIT told me that term, wasn't Lex.

1:37:18.420 --> 1:37:22.060
 So first of all, there's power dynamics in bed

1:37:22.060 --> 1:37:24.220
 and power dynamics in relationships

1:37:24.220 --> 1:37:25.860
 and power dynamics on the street

1:37:25.860 --> 1:37:29.260
 and in the work environment, those are all very different.

1:37:29.260 --> 1:37:34.260
 I think power dynamics can make human relationships,

1:37:34.740 --> 1:37:39.740
 especially romantic relationships, fascinating and rich

1:37:39.820 --> 1:37:42.740
 and fulfilling and exciting and all those kinds of things.

1:37:42.740 --> 1:37:47.740
 So I don't think in themselves they're bad

1:37:49.040 --> 1:37:50.820
 and the same goes with robots.

1:37:50.820 --> 1:37:54.220
 I really love the idea that a robot would be a top

1:37:54.220 --> 1:37:57.520
 or a bottom in terms of like power dynamics

1:37:57.520 --> 1:37:59.900
 and I think everybody should be aware of that

1:37:59.900 --> 1:38:02.460
 and the manipulation is not so much manipulation

1:38:02.460 --> 1:38:06.700
 but a dance of like pulling away, a push and pull

1:38:06.700 --> 1:38:08.340
 and all those kinds of things.

1:38:08.340 --> 1:38:12.860
 In terms of control, I think we're very, very, very far away

1:38:12.860 --> 1:38:16.340
 from AI systems that are able to lock us up.

1:38:16.340 --> 1:38:21.340
 They, to lock us up in, like to have so much control

1:38:21.340 --> 1:38:25.180
 that we basically cannot live our lives

1:38:25.180 --> 1:38:26.960
 in the way that we want.

1:38:26.960 --> 1:38:29.720
 I think there's, in terms of dangers of AI systems,

1:38:29.720 --> 1:38:31.300
 there's much more dangers that have to do

1:38:31.300 --> 1:38:34.280
 with autonomous weapon systems and all those kinds of things.

1:38:34.280 --> 1:38:37.940
 So the power dynamics as exercised in the struggle

1:38:37.940 --> 1:38:40.700
 between nations and war and all those kinds of things.

1:38:40.700 --> 1:38:42.780
 But in terms of personal relationships,

1:38:43.700 --> 1:38:45.940
 I think power dynamics are a beautiful thing.

1:38:45.940 --> 1:38:48.380
 Now there is, of course, going to be all those kinds

1:38:48.380 --> 1:38:52.460
 of discussions about consent and rights

1:38:52.460 --> 1:38:53.300
 and all those kinds of things.

1:38:53.300 --> 1:38:54.860
 Well, here we're talking, I always say,

1:38:54.860 --> 1:38:56.420
 in any discussion around this,

1:38:56.420 --> 1:38:59.180
 if we need to define really the context,

1:38:59.180 --> 1:39:02.420
 it's always, it always should be consensual,

1:39:02.420 --> 1:39:06.060
 age appropriate, context appropriate, species appropriate.

1:39:06.060 --> 1:39:09.200
 But now we're talking about human robot interactions

1:39:09.200 --> 1:39:10.780
 and so I guess that-

1:39:10.780 --> 1:39:13.620
 No, I actually was trying to make a different point

1:39:13.620 --> 1:39:17.020
 which is I do believe that robots will have rights

1:39:17.020 --> 1:39:20.820
 down the line and I think in order for us

1:39:20.820 --> 1:39:23.060
 to have deep meaningful relationship with robots,

1:39:23.060 --> 1:39:26.580
 we would have to consider them as entities in themselves

1:39:26.580 --> 1:39:28.360
 that deserve respect.

1:39:29.560 --> 1:39:31.660
 And that's a really interesting concept

1:39:31.660 --> 1:39:34.360
 that I think people are starting to talk about

1:39:34.360 --> 1:39:36.580
 a little bit more, but it's very difficult for us

1:39:36.580 --> 1:39:39.700
 to understand how entities that are other than human,

1:39:39.700 --> 1:39:42.460
 I mean, the same as with dogs and other animals

1:39:42.460 --> 1:39:44.900
 can have rights on a level as humans.

1:39:44.900 --> 1:39:48.220
 Well, yeah, I mean, we can't and nor should we

1:39:48.220 --> 1:39:49.940
 do whatever we want with animals.

1:39:49.940 --> 1:39:54.700
 We have a USDA, we have departments of agriculture

1:39:54.700 --> 1:39:58.260
 that deal with animal care and use committees

1:39:58.260 --> 1:40:02.060
 for research, for farming and ranching and all that.

1:40:02.060 --> 1:40:05.020
 So while when you first said it, I thought,

1:40:05.020 --> 1:40:07.380
 wait, why would there be a bill of robotic rights?

1:40:07.380 --> 1:40:11.240
 But it absolutely makes sense in the context of everything

1:40:11.240 --> 1:40:13.860
 we've been talking about up until now.

1:40:13.860 --> 1:40:18.700
 Let's, if you're willing, I'd love to talk about dogs

1:40:18.700 --> 1:40:21.260
 because you've mentioned dogs a couple of times,

1:40:21.260 --> 1:40:26.260
 a robot dog, you had a biological dog, yeah.

1:40:26.300 --> 1:40:31.300
 Yeah, I had a Newfoundland named Homer

1:40:32.460 --> 1:40:34.220
 for many years growing up.

1:40:34.220 --> 1:40:35.620
 In Russia or in the US?

1:40:35.620 --> 1:40:37.300
 In the United States.

1:40:37.300 --> 1:40:40.940
 And he was about, he's over 200 pounds, that's a big dog.

1:40:40.940 --> 1:40:41.820
 That's a big dog.

1:40:41.820 --> 1:40:45.260
 If people know Newfoundland, so he's this black dog

1:40:45.260 --> 1:40:50.260
 that's a really long hair and just a kind soul.

1:40:50.460 --> 1:40:53.220
 I think perhaps that's true for a lot of large dogs,

1:40:53.220 --> 1:40:55.180
 but he thought he was a small dog.

1:40:55.180 --> 1:40:56.500
 So he moved like that and-

1:40:56.500 --> 1:40:57.380
 Was he your dog?

1:40:57.380 --> 1:40:58.340
 Yeah, yeah.

1:40:58.340 --> 1:41:00.500
 So you had him since he was fairly young?

1:41:00.500 --> 1:41:02.720
 Oh, since, yeah, since the very, very beginning

1:41:02.720 --> 1:41:03.780
 to the very, very end.

1:41:03.780 --> 1:41:08.620
 And one of the things, I mean, he had this kind of,

1:41:08.620 --> 1:41:13.620
 we mentioned like the Roombas, he had a kindhearted

1:41:13.660 --> 1:41:16.620
 dumbness about him that was just overwhelming.

1:41:16.620 --> 1:41:20.020
 It's part of the reason I named him Homer

1:41:20.020 --> 1:41:22.740
 because it's after Homer Simpson,

1:41:22.740 --> 1:41:25.260
 in case people are wondering which Homer I'm referring to.

1:41:25.260 --> 1:41:26.420
 I'm not, you know.

1:41:27.820 --> 1:41:28.660
 So there's a-

1:41:28.660 --> 1:41:29.500
 Not the Odyssey.

1:41:29.500 --> 1:41:30.880
 Yeah, exactly.

1:41:32.300 --> 1:41:35.120
 There's a clumsiness that was just something

1:41:35.120 --> 1:41:37.960
 that immediately led to a deep love for each other.

1:41:37.960 --> 1:41:42.300
 And one of the, I mean, he was always,

1:41:42.300 --> 1:41:43.300
 it's the shared moments.

1:41:43.300 --> 1:41:46.500
 He was always there for so many nights together.

1:41:46.500 --> 1:41:51.060
 That's a powerful thing about a dog that he was there

1:41:51.060 --> 1:41:53.540
 through all the loneliness, through all the tough times,

1:41:53.540 --> 1:41:55.820
 through the successes and all those kinds of things.

1:41:55.820 --> 1:41:57.740
 And I remember, I mean,

1:41:57.740 --> 1:42:00.100
 that was a really moving moment for me.

1:42:00.100 --> 1:42:02.140
 I still miss him to this day.

1:42:02.140 --> 1:42:03.420
 How long ago did he die?

1:42:05.340 --> 1:42:07.060
 Maybe 15 years ago.

1:42:07.060 --> 1:42:09.140
 So it's been a while,

1:42:09.140 --> 1:42:13.940
 but it was the first time I've really experienced

1:42:13.940 --> 1:42:15.680
 like the feeling of death.

1:42:16.980 --> 1:42:21.980
 So what happened is he got cancer

1:42:22.840 --> 1:42:26.020
 and so he was dying slowly.

1:42:26.020 --> 1:42:29.700
 And then at a certain point he couldn't get up anymore.

1:42:29.700 --> 1:42:31.780
 There's a lot of things I could say here,

1:42:31.780 --> 1:42:33.980
 you know, that I struggle with.

1:42:33.980 --> 1:42:38.980
 That maybe he suffered much longer than he needed to.

1:42:39.080 --> 1:42:42.060
 That's something I really think about a lot.

1:42:42.060 --> 1:42:47.060
 But I remember I had to take him to the hospital

1:42:47.160 --> 1:42:52.160
 and the nurses couldn't carry him, right?

1:42:52.300 --> 1:42:54.420
 So you talk about 200 pound dog.

1:42:54.420 --> 1:42:56.700
 I was really into powerlifting at the time.

1:42:56.700 --> 1:42:59.340
 I remember like they tried to figure out

1:42:59.340 --> 1:43:01.740
 all these kinds of ways to,

1:43:01.740 --> 1:43:03.580
 so in order to put them to sleep,

1:43:03.580 --> 1:43:07.220
 they had to take them into a room.

1:43:07.220 --> 1:43:09.500
 And so I had to carry him everywhere.

1:43:09.500 --> 1:43:13.540
 And here's this dying friend of mine

1:43:13.540 --> 1:43:15.220
 that I just had to,

1:43:15.220 --> 1:43:16.980
 first of all, it was really difficult to carry

1:43:16.980 --> 1:43:20.100
 somebody that heavy when they're not helping you out.

1:43:20.100 --> 1:43:25.100
 And yeah, so I remember it was the first time

1:43:25.900 --> 1:43:28.460
 seeing a friend laying there

1:43:28.460 --> 1:43:33.460
 and seeing wife drained from his body and that realization

1:43:36.340 --> 1:43:40.580
 that we're here for a short time was made so real

1:43:40.580 --> 1:43:42.740
 that here's a friend that was there for me

1:43:42.740 --> 1:43:46.220
 the week before, the day before, and now he's gone.

1:43:46.220 --> 1:43:49.180
 And that was, I don't know,

1:43:49.180 --> 1:43:52.380
 that spoke to the fact that he could be deeply connected

1:43:52.380 --> 1:43:56.700
 with a dog, also spoke to the fact

1:43:56.700 --> 1:44:00.820
 that the shared moments together

1:44:00.820 --> 1:44:02.260
 that led to that deep friendship

1:44:05.260 --> 1:44:07.580
 will make life so amazing,

1:44:08.540 --> 1:44:11.700
 but also spoke to the fact that death is a motherfucker.

1:44:13.420 --> 1:44:16.100
 So I know you've lost Costello recently

1:44:16.100 --> 1:44:16.940
 and you've been going.

1:44:16.940 --> 1:44:17.760
 And as you're saying this,

1:44:17.760 --> 1:44:20.180
 I'm definitely fighting back the tears.

1:44:22.100 --> 1:44:23.340
 Thank you for sharing that,

1:44:23.340 --> 1:44:27.640
 that I guess we're about to both cry over our dead dogs,

1:44:28.660 --> 1:44:30.300
 that it was bound to happen

1:44:30.300 --> 1:44:32.600
 just given when this is happening.

1:44:33.480 --> 1:44:35.060
 Yeah, it's-

1:44:35.060 --> 1:44:38.480
 How long did you know that Costello was not doing well?

1:44:39.660 --> 1:44:44.300
 Well, let's see, a year ago, during the start of,

1:44:44.300 --> 1:44:46.420
 about six months into the pandemic,

1:44:46.420 --> 1:44:49.060
 he started getting abscesses and he was not,

1:44:49.060 --> 1:44:51.580
 his behavior changed and something really changed.

1:44:51.580 --> 1:44:55.780
 And then I put him on testosterone

1:44:55.780 --> 1:44:58.100
 because, which helped a lot of things,

1:44:58.100 --> 1:44:59.300
 it certainly didn't cure everything,

1:44:59.300 --> 1:45:01.180
 but it helped a lot of things he was dealing with,

1:45:01.180 --> 1:45:03.860
 joint pain, sleep issues.

1:45:03.860 --> 1:45:07.740
 And then it just became a very slow decline

1:45:08.860 --> 1:45:11.360
 to the point where two, three weeks ago,

1:45:11.360 --> 1:45:15.300
 he had a closet full of medication.

1:45:15.300 --> 1:45:17.340
 I mean, this dog was, it was like a pharmacy.

1:45:17.340 --> 1:45:19.860
 It's amazing to me when I looked at it the other day,

1:45:19.860 --> 1:45:22.080
 I still haven't cleaned up and removed all his things

1:45:22.080 --> 1:45:23.840
 because I can't quite bring myself to do it.

1:45:23.840 --> 1:45:24.680
 But-

1:45:25.860 --> 1:45:27.460
 Do you think he was suffering?

1:45:27.460 --> 1:45:30.300
 Well, so what happened was about a week ago,

1:45:30.300 --> 1:45:32.500
 it was really just about a week ago, it's amazing.

1:45:32.500 --> 1:45:35.020
 He was going up the stairs, I saw him slip.

1:45:35.020 --> 1:45:35.860
 And he was a big dog.

1:45:35.860 --> 1:45:37.740
 He wasn't 200 pounds, but he was about 90 pounds.

1:45:37.740 --> 1:45:39.100
 But he's a bulldog, that's pretty big.

1:45:39.100 --> 1:45:40.100
 And he was fit.

1:45:41.200 --> 1:45:43.920
 And then I noticed that he wasn't carrying a foot

1:45:43.920 --> 1:45:45.140
 in the back like it was injured.

1:45:45.140 --> 1:45:46.400
 It had no feeling at all.

1:45:46.400 --> 1:45:48.220
 He never liked me to touch his hind paws.

1:45:48.220 --> 1:45:50.340
 And I could do, that thing was just flopping there.

1:45:50.340 --> 1:45:53.840
 And then the vet found some spinal degeneration

1:45:53.840 --> 1:45:55.720
 and I was told that the next one would go.

1:45:55.720 --> 1:45:57.300
 Did he suffer?

1:45:57.300 --> 1:45:58.620
 Sure hope not.

1:45:58.620 --> 1:46:01.420
 But something changed in his eyes.

1:46:01.420 --> 1:46:02.260
 Yeah.

1:46:02.260 --> 1:46:03.080
 It's the eyes again.

1:46:03.080 --> 1:46:05.120
 I know you and I spend long hours on the phone

1:46:05.120 --> 1:46:06.620
 and talking about like the eyes and how,

1:46:06.620 --> 1:46:09.180
 what they convey and what they mean about internal states

1:46:09.180 --> 1:46:12.140
 and forsaken robots and biology of other kinds.

1:46:12.140 --> 1:46:12.980
 But-

1:46:12.980 --> 1:46:17.880
 Do you think something about him was gone in his eyes?

1:46:17.880 --> 1:46:20.620
 I think he was real.

1:46:20.620 --> 1:46:22.860
 Here I am anthropomorphizing.

1:46:22.860 --> 1:46:26.840
 I think he was realizing that one of his great joys in life,

1:46:26.840 --> 1:46:31.840
 which was to walk and sniff and pee on things.

1:46:33.140 --> 1:46:36.420
 This dog loved to pee on things.

1:46:36.420 --> 1:46:37.260
 It was amazing.

1:46:37.260 --> 1:46:38.900
 I've wondered where he put it.

1:46:38.900 --> 1:46:41.060
 He was like a reservoir of urine.

1:46:41.060 --> 1:46:42.300
 It was incredible.

1:46:42.300 --> 1:46:43.140
 I think, oh, that's it.

1:46:43.140 --> 1:46:46.560
 He's just, he'd put like one drop on the 50 millionth plant.

1:46:46.560 --> 1:46:49.140
 And then we get to the 50 millionth in one plant

1:46:49.140 --> 1:46:51.220
 and he just have, you know, leave a puddle.

1:46:51.220 --> 1:46:53.340
 And here I am talking about Costello peeing.

1:46:54.500 --> 1:46:57.020
 He was losing that ability to stand up and do that.

1:46:57.020 --> 1:46:58.880
 He was falling down while he was doing that.

1:46:58.880 --> 1:47:01.560
 And I do think he started to realize,

1:47:01.560 --> 1:47:04.560
 and the passage was easy and peaceful,

1:47:04.560 --> 1:47:08.120
 but you know, I'll say this.

1:47:08.120 --> 1:47:09.140
 I'm not ashamed to say it.

1:47:09.140 --> 1:47:11.480
 I mean, I wake up every morning since then just,

1:47:11.480 --> 1:47:13.240
 I don't even make the conscious decision

1:47:13.240 --> 1:47:14.460
 to allow myself to cry.

1:47:14.460 --> 1:47:16.060
 I wake up crying.

1:47:16.060 --> 1:47:18.160
 And I'm fortunately able to make it through the day,

1:47:18.160 --> 1:47:19.980
 thanks to the great support of my friends

1:47:19.980 --> 1:47:21.420
 and you and my family.

1:47:21.420 --> 1:47:24.080
 But I miss him, man.

1:47:24.080 --> 1:47:24.920
 You miss him?

1:47:24.920 --> 1:47:25.740
 Yeah, I miss him.

1:47:25.740 --> 1:47:29.180
 And I feel like he, you know, Homer, Costello,

1:47:29.180 --> 1:47:32.980
 you know, the relationship to one's dog is so specific, but.

1:47:34.220 --> 1:47:36.060
 So that part of you is gone.

1:47:37.940 --> 1:47:39.500
 That's the hard thing.

1:47:39.500 --> 1:47:40.340
 You know,

1:47:40.340 --> 1:47:45.340
 what I think is different is that I made the mistake,

1:47:49.620 --> 1:47:50.620
 I think.

1:47:50.620 --> 1:47:51.800
 I hope it was a good decision,

1:47:51.800 --> 1:47:53.260
 but sometimes I think I made the mistake

1:47:53.260 --> 1:47:56.480
 of I brought Costello a little bit to the world

1:47:56.480 --> 1:47:58.420
 through the podcast, through posting about him.

1:47:58.420 --> 1:48:01.540
 I gave, I anthropomorphized about him in public.

1:48:01.540 --> 1:48:02.380
 Let's be honest.

1:48:02.380 --> 1:48:03.700
 I have no idea what his mental life was

1:48:03.700 --> 1:48:04.840
 or his relationship to me.

1:48:04.840 --> 1:48:06.780
 And I'm just exploring all this for the first time

1:48:06.780 --> 1:48:07.760
 because he was my first dog,

1:48:07.760 --> 1:48:09.660
 but I raised him since he was seven weeks.

1:48:09.660 --> 1:48:11.020
 Yeah, you got to hold it together.

1:48:11.020 --> 1:48:14.940
 I noticed the episode you released on Monday,

1:48:14.940 --> 1:48:16.560
 you mentioned Costello.

1:48:16.560 --> 1:48:18.900
 Like you brought him back to life for me

1:48:18.900 --> 1:48:20.340
 for that brief moment.

1:48:20.340 --> 1:48:22.100
 Yeah, but he's gone.

1:48:22.100 --> 1:48:23.440
 Well, that's the,

1:48:24.620 --> 1:48:26.920
 he's going to be gone for a lot of people too.

1:48:28.500 --> 1:48:29.820
 Well, this is what I'm struggling with.

1:48:29.820 --> 1:48:34.460
 I think that maybe you're pretty good at this, Lila.

1:48:34.460 --> 1:48:36.100
 Wait, have you done this before?

1:48:36.100 --> 1:48:40.560
 This is the challenge is I actually, part of me,

1:48:40.560 --> 1:48:43.080
 I know how to take care of myself pretty well.

1:48:43.080 --> 1:48:44.720
 Not perfectly, but pretty well.

1:48:44.720 --> 1:48:46.000
 And I have good support.

1:48:46.000 --> 1:48:48.780
 I do worry a little bit about how it's going to land

1:48:48.780 --> 1:48:50.200
 and how people will feel.

1:48:50.200 --> 1:48:54.040
 I'm concerned about their internalization.

1:48:54.040 --> 1:48:56.240
 So that's something I'm still iterating on.

1:48:56.240 --> 1:48:58.360
 And you have to, they have to watch you struggle,

1:48:58.360 --> 1:48:59.360
 which is fascinating.

1:48:59.360 --> 1:49:01.620
 Right, and I've mostly been shielding them from this,

1:49:01.620 --> 1:49:03.960
 but what would make me happiest

1:49:03.960 --> 1:49:08.200
 if people would internalize some of Costello's best traits

1:49:08.200 --> 1:49:13.200
 and his best traits were that he was incredibly tough.

1:49:14.220 --> 1:49:17.680
 I mean, he was a 22 inch neck, bulldog, the whole thing.

1:49:17.680 --> 1:49:18.960
 He was just born that way.

1:49:18.960 --> 1:49:22.340
 But what was so beautiful is that his toughness

1:49:22.340 --> 1:49:24.040
 is never what he rolled forward.

1:49:24.040 --> 1:49:27.100
 It was just how sweet and kind he was.

1:49:27.100 --> 1:49:29.140
 And so if people can take that,

1:49:29.140 --> 1:49:32.400
 then there's a win in there someplace.

1:49:32.400 --> 1:49:34.880
 So I think there's some ways in which

1:49:34.880 --> 1:49:37.800
 he should probably live on in your podcast too.

1:49:37.800 --> 1:49:40.540
 You should, I mean, it's such a,

1:49:41.780 --> 1:49:45.800
 one of the things I loved about his role in your podcast

1:49:45.800 --> 1:49:48.740
 is that he brought so much joy to you.

1:49:48.740 --> 1:49:51.920
 I mentioned the robots, right?

1:49:51.920 --> 1:49:55.960
 I think that's such a powerful thing to bring that joy

1:49:55.960 --> 1:49:59.400
 into like allowing yourself to experience that joy,

1:49:59.400 --> 1:50:02.520
 to bring that joy to others, to share it with others.

1:50:02.520 --> 1:50:03.820
 That's really powerful.

1:50:03.820 --> 1:50:07.520
 And I mean, not to, this is like the Russian thing is,

1:50:10.000 --> 1:50:14.040
 it touched me when Louis CK had that moment

1:50:14.040 --> 1:50:17.960
 that I keep thinking about in his show, Louis,

1:50:17.960 --> 1:50:20.240
 where like an old man was criticizing Louis

1:50:20.240 --> 1:50:22.960
 for whining about breaking up with his girlfriend.

1:50:22.960 --> 1:50:27.080
 And he was saying like the most beautiful thing

1:50:27.080 --> 1:50:32.080
 about love, they made a song that's catchy now

1:50:32.480 --> 1:50:35.660
 that's not making me feel horrible saying it,

1:50:35.660 --> 1:50:37.700
 but like is the loss.

1:50:37.700 --> 1:50:41.220
 The loss really also is making you realize

1:50:41.220 --> 1:50:46.220
 how much that person, that dog meant to you.

1:50:46.360 --> 1:50:48.680
 And like allowing yourself to feel that loss

1:50:48.680 --> 1:50:51.420
 and not run away from that loss is really powerful.

1:50:51.420 --> 1:50:56.160
 And in some ways that's also sweet, just like the love was,

1:50:56.160 --> 1:50:59.440
 the loss is also sweet because you know

1:50:59.440 --> 1:51:03.760
 that you felt a lot for that, for your friend.

1:51:03.760 --> 1:51:07.120
 So I, you know, and then continue bringing that joy.

1:51:07.120 --> 1:51:09.420
 I think it would be amazing to the podcast.

1:51:10.280 --> 1:51:13.680
 I hope to do the same with robots

1:51:13.680 --> 1:51:16.080
 or whatever else is the source of joy, right?

1:51:17.720 --> 1:51:22.440
 And maybe, do you think about one day getting another dog?

1:51:22.440 --> 1:51:27.440
 Yeah, in time, you're hitting on all the key buttons here.

1:51:28.560 --> 1:51:32.720
 I want that to, we're thinking about, you know,

1:51:32.720 --> 1:51:35.620
 ways to kind of immortalize Costello in a way that's real,

1:51:35.620 --> 1:51:38.760
 not just, you know, creating some little logo

1:51:38.760 --> 1:51:39.760
 or something silly.

1:51:39.760 --> 1:51:43.840
 You know, Costello, much like David Goggins is a person,

1:51:43.840 --> 1:51:47.120
 but Goggins also has grown into a kind of a verb.

1:51:47.120 --> 1:51:48.820
 You're going to Goggins this or you're going to,

1:51:48.820 --> 1:51:52.400
 and there's an adjective, like that's extreme, like it.

1:51:52.400 --> 1:51:54.440
 I think that for me, Costello was all those things.

1:51:54.440 --> 1:51:56.800
 He was a being, he was his own being.

1:51:56.800 --> 1:52:00.360
 He was a noun, a verb, and an adjective.

1:52:00.360 --> 1:52:02.420
 So, and he had this amazing superpower

1:52:02.420 --> 1:52:04.240
 that I wish I could get, which is this ability

1:52:04.240 --> 1:52:06.020
 to get everyone else to do things for you

1:52:06.020 --> 1:52:07.800
 without doing a damn thing.

1:52:08.680 --> 1:52:10.280
 The Costello effect, as I call it.

1:52:10.280 --> 1:52:12.240
 So it's an idea, I hope he lives on.

1:52:12.240 --> 1:52:14.440
 Yes, thank you for that.

1:52:14.440 --> 1:52:16.840
 This actually has been very therapeutic for me.

1:52:16.840 --> 1:52:21.840
 Which actually brings me to a question, we're friends.

1:52:23.220 --> 1:52:26.700
 We're not just co-scientists, colleagues,

1:52:26.700 --> 1:52:28.160
 working on a project together,

1:52:28.160 --> 1:52:33.160
 and in the world that's somewhat similar.

1:52:34.760 --> 1:52:39.100
 Just two dogs, just two dogs, basically.

1:52:40.540 --> 1:52:42.760
 But let's talk about friendship.

1:52:42.760 --> 1:52:47.760
 Because I think that, I certainly know as a scientist

1:52:49.140 --> 1:52:51.100
 that there are elements that are very lonely

1:52:51.100 --> 1:52:52.720
 of the scientific pursuit.

1:52:52.720 --> 1:52:57.000
 There are elements of many pursuits that are lonely.

1:52:57.000 --> 1:53:00.080
 Music, math, always seemed to me

1:53:00.080 --> 1:53:02.400
 like they're like the loneliest people.

1:53:02.400 --> 1:53:04.040
 Who knows if that's true or not.

1:53:04.040 --> 1:53:05.360
 Also people work in teams,

1:53:05.360 --> 1:53:07.000
 and sometimes people are surrounded by people

1:53:07.000 --> 1:53:09.280
 interacting with people and they feel very lonely.

1:53:09.280 --> 1:53:14.280
 But for me, and I think as well for you,

1:53:14.420 --> 1:53:17.940
 friendship is an incredibly strong force

1:53:17.940 --> 1:53:22.940
 in making one feel like certain things are possible

1:53:23.520 --> 1:53:25.800
 or worth reaching for.

1:53:25.800 --> 1:53:28.200
 Maybe even making us compulsively reach for them.

1:53:28.200 --> 1:53:30.680
 So when you were growing up,

1:53:30.680 --> 1:53:32.800
 you grew up in Russia until what age?

1:53:32.800 --> 1:53:33.680
 13.

1:53:33.680 --> 1:53:38.280
 Okay, and then you moved directly to Philadelphia?

1:53:38.280 --> 1:53:39.320
 To Chicago.

1:53:39.320 --> 1:53:40.160
 Chicago.

1:53:40.160 --> 1:53:44.640
 And then Philadelphia and San Francisco and Boston

1:53:44.640 --> 1:53:46.940
 and so on, but really to Chicago.

1:53:46.940 --> 1:53:48.260
 That's why I went to high school.

1:53:48.260 --> 1:53:49.840
 Do you have siblings?

1:53:49.840 --> 1:53:51.080
 Older brother.

1:53:51.080 --> 1:53:52.480
 Most people don't know that.

1:53:53.840 --> 1:53:57.880
 Yeah, he is a very different person,

1:53:57.880 --> 1:53:59.600
 but somebody I definitely look up to.

1:53:59.600 --> 1:54:00.780
 So he's a wild man.

1:54:00.780 --> 1:54:01.800
 He's extrovert.

1:54:01.800 --> 1:54:05.400
 He's, he was into, I mean,

1:54:05.400 --> 1:54:07.540
 so he's also a scientist, a bio engineer,

1:54:07.540 --> 1:54:12.540
 but when we were growing up and he was the person

1:54:12.600 --> 1:54:17.140
 who did drink and did every drug,

1:54:17.140 --> 1:54:19.360
 but also was the life of the party.

1:54:19.360 --> 1:54:21.080
 And I just thought he was the,

1:54:21.080 --> 1:54:23.440
 when you're the older brother, five years older,

1:54:23.440 --> 1:54:28.300
 he was the coolest person that I always wanted to be him.

1:54:28.300 --> 1:54:31.180
 So to that, he definitely had a big influence.

1:54:31.180 --> 1:54:35.620
 But I think for me, in terms of friendship growing up,

1:54:35.620 --> 1:54:40.060
 I had one really close friend.

1:54:40.060 --> 1:54:42.280
 And then when I came here, I had another close friend,

1:54:42.280 --> 1:54:47.280
 but I'm very, I believe, I don't know if I believe,

1:54:47.520 --> 1:54:52.520
 but I draw a lot of strength from deep connections

1:54:53.020 --> 1:54:57.740
 with other people and just a small number of people,

1:54:57.740 --> 1:54:59.080
 just a really small number of people.

1:54:59.080 --> 1:55:00.480
 That's when I moved to this country,

1:55:00.480 --> 1:55:04.160
 I was really surprised how like there would be

1:55:04.160 --> 1:55:08.280
 these large groups of friends, quote unquote,

1:55:08.280 --> 1:55:12.240
 but the depth of connection was not there at all

1:55:12.240 --> 1:55:14.280
 from my sort of perspective.

1:55:14.280 --> 1:55:17.700
 Now I moved to the suburb of Chicago was Naperville.

1:55:17.700 --> 1:55:20.960
 It's more like a middle-class, maybe upper middle-class.

1:55:20.960 --> 1:55:23.520
 So it's like people that cared more

1:55:23.520 --> 1:55:26.480
 about material possessions than deep human connection.

1:55:26.480 --> 1:55:28.400
 So that added to the thing.

1:55:28.400 --> 1:55:33.400
 But I drew more meaning than almost anything else

1:55:33.400 --> 1:55:35.640
 was from friendship early on.

1:55:35.640 --> 1:55:40.180
 I had a best friend, his name was, his name is Yura.

1:55:41.540 --> 1:55:43.480
 I don't know how to say it in English.

1:55:43.480 --> 1:55:44.680
 How do you say it in Russian?

1:55:44.680 --> 1:55:45.520
 Yura.

1:55:45.520 --> 1:55:46.440
 What's his last name?

1:55:46.440 --> 1:55:47.280
 Do you remember?

1:55:48.640 --> 1:55:51.840
 Mirkulov, Yura Mirkulov.

1:55:53.720 --> 1:55:56.320
 So we just spent all our time together.

1:55:56.320 --> 1:55:58.480
 There's also a group of friends.

1:55:58.480 --> 1:56:01.680
 Like, I don't know, it's like eight guys.

1:56:01.680 --> 1:56:06.680
 In Russia, growing up, it's like parents didn't care

1:56:07.480 --> 1:56:09.680
 if you're coming back at a certain hour.

1:56:09.680 --> 1:56:12.280
 So we would spend all day, all night,

1:56:12.280 --> 1:56:14.800
 just playing soccer, usually called football

1:56:15.840 --> 1:56:18.900
 and just talking about life and all those kinds of things.

1:56:18.900 --> 1:56:22.680
 Even at that young age, I think people in Russia

1:56:22.680 --> 1:56:25.120
 and the Soviet Union grow up much quicker.

1:56:26.980 --> 1:56:30.920
 I think the education system at the university level

1:56:30.920 --> 1:56:33.720
 is world-class in the United States

1:56:33.720 --> 1:56:38.720
 in terms of really creating really big, powerful minds,

1:56:38.960 --> 1:56:42.040
 at least it used to be, but I think that they aspire to that.

1:56:42.040 --> 1:56:46.380
 But the education system for younger kids

1:56:46.380 --> 1:56:49.280
 in the Soviet Union was incredible.

1:56:49.280 --> 1:56:51.240
 They did not treat us as kids.

1:56:51.240 --> 1:56:54.560
 The level of literature, Tolstoy, Dostoyevsky.

1:56:54.560 --> 1:56:55.960
 When you were just a small child?

1:56:55.960 --> 1:56:56.800
 Yeah.

1:56:56.800 --> 1:56:58.560
 Amazing, amazing.

1:56:58.560 --> 1:57:02.080
 The level of mathematics and you're made to feel like shit

1:57:02.080 --> 1:57:03.780
 if you're not good at mathematics.

1:57:03.780 --> 1:57:07.160
 Like we, I think in this country, there's more,

1:57:07.160 --> 1:57:09.700
 like especially young kids, cause they're so cute.

1:57:09.700 --> 1:57:11.480
 Like they're being babied.

1:57:12.320 --> 1:57:15.240
 We only start to really push adults later in life.

1:57:15.240 --> 1:57:17.860
 Like, so if you want to be the best in the world at this,

1:57:17.860 --> 1:57:19.480
 then you get to be pushed.

1:57:19.480 --> 1:57:22.880
 But we were pushed at a young age, everybody was pushed.

1:57:22.880 --> 1:57:25.020
 And that brought out the best in people.

1:57:25.020 --> 1:57:29.600
 I think they really forced people to discover like,

1:57:29.600 --> 1:57:31.840
 discover themselves in the Goggin style,

1:57:31.840 --> 1:57:35.160
 but also discover what they're actually passionate about,

1:57:35.160 --> 1:57:36.000
 what they're not.

1:57:36.000 --> 1:57:37.440
 Was it true for boys and girls

1:57:37.440 --> 1:57:38.960
 where they pushed equally there?

1:57:38.960 --> 1:57:39.960
 Yeah, they were pushed.

1:57:39.960 --> 1:57:41.960
 Yeah, they were pushed equally, I would say.

1:57:41.960 --> 1:57:45.560
 There was, obviously there was more, not obviously,

1:57:45.560 --> 1:57:50.360
 but there, at least from my memories, more of a,

1:57:50.360 --> 1:57:52.040
 what's the right way to put it,

1:57:52.040 --> 1:57:54.000
 but there was like gender roles,

1:57:54.000 --> 1:57:56.280
 but not in a negative connotation.

1:57:56.280 --> 1:57:59.480
 It was the red dress versus the suit and tie

1:57:59.480 --> 1:58:01.560
 kind of connotation, which is like,

1:58:01.560 --> 1:58:06.560
 there's like guys like lifting heavy things

1:58:08.040 --> 1:58:11.520
 and girls like creating beautiful art.

1:58:11.520 --> 1:58:14.200
 And like there's-

1:58:14.200 --> 1:58:18.200
 A more traditional view of gender, more 1950s, 60s.

1:58:18.200 --> 1:58:20.400
 But we didn't think in terms of, at least at that age,

1:58:20.400 --> 1:58:24.120
 in terms of like roles and then like a homemaker or something

1:58:24.120 --> 1:58:28.220
 like that, or no, it was more about what people care about.

1:58:28.220 --> 1:58:31.360
 Like girls cared about this set of things

1:58:31.360 --> 1:58:33.240
 and guys cared about this set of things.

1:58:33.240 --> 1:58:36.520
 I think mathematics and engineering was something

1:58:36.520 --> 1:58:38.560
 that guys cared about and sort of,

1:58:38.560 --> 1:58:40.480
 at least my perception of that time.

1:58:40.480 --> 1:58:43.980
 And then girls cared about beauty.

1:58:43.980 --> 1:58:46.060
 So like guys want to create machines,

1:58:46.060 --> 1:58:47.760
 girls want to create beautiful stuff.

1:58:47.760 --> 1:58:52.760
 And now, of course, that I don't take that forward

1:58:52.760 --> 1:58:54.820
 in some kind of philosophy of life,

1:58:54.820 --> 1:58:57.520
 but it's just the way I grew up and the way I remember it.

1:58:57.520 --> 1:59:01.600
 But all, everyone worked hard.

1:59:01.600 --> 1:59:06.520
 The value of hard work was instilled in everybody.

1:59:06.520 --> 1:59:11.520
 And through that, I think it's a little bit of hardship.

1:59:12.440 --> 1:59:14.960
 Of course, also economically, everybody was poor,

1:59:14.960 --> 1:59:17.100
 especially with the collapse of the Soviet Union.

1:59:17.100 --> 1:59:18.620
 There's poverty everywhere.

1:59:18.620 --> 1:59:19.800
 You didn't notice it as much,

1:59:19.800 --> 1:59:23.120
 but there was a, because there's not much material

1:59:23.120 --> 1:59:26.220
 possessions, there was a huge value placed

1:59:26.220 --> 1:59:28.040
 on human connection.

1:59:28.040 --> 1:59:31.200
 Just meeting with neighbors, everybody knew each other.

1:59:31.200 --> 1:59:33.600
 We lived in an apartment building,

1:59:33.600 --> 1:59:35.600
 very different than you have in the United States

1:59:35.600 --> 1:59:37.780
 these days, everybody knew each other.

1:59:39.000 --> 1:59:41.640
 You would get together, drink vodka, smoke cigarettes

1:59:41.640 --> 1:59:46.640
 and play guitar and sing sad songs about life.

1:59:47.280 --> 1:59:50.600
 What's with the sad songs and the Russian thing?

1:59:50.600 --> 1:59:55.020
 I mean, Russians do express joy from time to time.

1:59:55.020 --> 1:59:55.920
 They do.

1:59:55.920 --> 1:59:57.560
 Certainly you do.

1:59:57.560 --> 2:00:00.240
 But what do you think that's about?

2:00:00.240 --> 2:00:01.320
 Is it because it's cold there?

2:00:01.320 --> 2:00:02.840
 But it's cold other places too.

2:00:04.460 --> 2:00:08.340
 I think, so first of all, the Soviet Union,

2:00:08.340 --> 2:00:13.040
 the echoes of World War II and the millions

2:00:13.040 --> 2:00:14.660
 and millions and millions of people there,

2:00:14.660 --> 2:00:17.020
 civilians that were slaughtered

2:00:17.020 --> 2:00:20.180
 and also starvation is there, right?

2:00:20.180 --> 2:00:24.040
 So like the echoes of that, of the ideas,

2:00:24.040 --> 2:00:25.900
 the literature, the art is there.

2:00:25.900 --> 2:00:29.340
 Like that's grandparents, that's parents, that's all there.

2:00:29.340 --> 2:00:33.780
 So that contributes to it, that life can be absurdly,

2:00:33.780 --> 2:00:35.580
 unexplainably cruel.

2:00:35.580 --> 2:00:37.380
 At any moment, everything can change.

2:00:37.380 --> 2:00:38.600
 So that's in there.

2:00:38.600 --> 2:00:40.920
 Then I think there's an empowering aspect

2:00:40.920 --> 2:00:43.220
 to finding beauty and suffering,

2:00:43.220 --> 2:00:45.480
 but then everything else is beautiful too.

2:00:45.480 --> 2:00:47.660
 Like if you just linger or it's like

2:00:47.660 --> 2:00:49.260
 why you meditate on death.

2:00:49.260 --> 2:00:52.060
 It's like if you just think about the worst possible case

2:00:52.060 --> 2:00:54.680
 and find beauty in that, then everything else beautiful too.

2:00:54.680 --> 2:00:57.400
 And so you write songs about the dark stuff

2:00:57.400 --> 2:01:02.400
 and that somehow helps you deal with whatever comes.

2:01:02.660 --> 2:01:07.220
 There's a hopelessness to the Soviet Union

2:01:07.220 --> 2:01:10.280
 that like inflation, all those kinds of things

2:01:10.280 --> 2:01:15.280
 where people were sold dreams and never delivered.

2:01:15.420 --> 2:01:20.420
 And so like, if you don't sing songs about sad things,

2:01:21.380 --> 2:01:24.300
 you're going to become cynical about this world.

2:01:24.300 --> 2:01:25.140
 Interesting.

2:01:25.140 --> 2:01:27.340
 So they don't want to give into cynicism.

2:01:27.340 --> 2:01:30.360
 Now, a lot of people did, one of the,

2:01:31.500 --> 2:01:34.240
 but it's the battle against cynicism.

2:01:34.240 --> 2:01:38.440
 One of the things that may be common in Russia

2:01:38.440 --> 2:01:41.020
 is the kind of cynicism about,

2:01:41.020 --> 2:01:43.180
 like if I told you the thing I said earlier

2:01:43.180 --> 2:01:45.020
 about dreaming about robots,

2:01:45.020 --> 2:01:48.800
 it's very common for people to dismiss that dream

2:01:48.800 --> 2:01:52.180
 of saying, nah, that's not, that's too wild.

2:01:52.180 --> 2:01:54.320
 Like who else do you know that did that?

2:01:54.320 --> 2:01:57.020
 Or you want to start a podcast, like who else?

2:01:57.020 --> 2:01:58.780
 Like nobody's making money on podcasts.

2:01:58.780 --> 2:02:00.020
 Like why do you want to start a podcast?

2:02:00.020 --> 2:02:03.360
 That kind of mindset I think is quite common,

2:02:03.360 --> 2:02:07.520
 which is why I would say entrepreneurship in Russia

2:02:07.520 --> 2:02:11.020
 is still not very good, which to be a business,

2:02:11.020 --> 2:02:13.420
 like to be an entrepreneur, you have to dream big

2:02:13.420 --> 2:02:14.980
 and you have to have others around you,

2:02:14.980 --> 2:02:19.080
 like friends and support group that makes you dream big.

2:02:19.080 --> 2:02:21.380
 But if you don't give into cynicism

2:02:22.340 --> 2:02:27.340
 and appreciate the beauty in the unfairness of life,

2:02:27.780 --> 2:02:29.940
 the absurd unfairness of life,

2:02:29.940 --> 2:02:34.500
 then I think it just makes you appreciative of everything.

2:02:34.500 --> 2:02:38.020
 It's like a, it's a prerequisite for gratitude.

2:02:38.020 --> 2:02:42.060
 And so, yeah, I think that instilled in me

2:02:42.060 --> 2:02:43.980
 ability to appreciate everything,

2:02:43.980 --> 2:02:46.740
 just like everything, everything's amazing.

2:02:46.740 --> 2:02:48.900
 And then also there's a culture

2:02:51.500 --> 2:02:56.100
 of like romanticizing everything.

2:02:56.100 --> 2:03:01.100
 Like it's almost like romantic relationships

2:03:01.340 --> 2:03:04.280
 were very like soap opera like,

2:03:04.280 --> 2:03:07.700
 is very like over the top dramatic.

2:03:07.700 --> 2:03:10.900
 And I think that was instilled in me too.

2:03:10.900 --> 2:03:13.620
 Not only do I appreciate everything about life,

2:03:13.620 --> 2:03:15.940
 but I get like emotional about it.

2:03:15.940 --> 2:03:20.600
 In a sense, like I get like a visceral feeling of joy

2:03:20.600 --> 2:03:24.500
 for everything and the same with friends

2:03:24.500 --> 2:03:26.100
 or people of the opposite sex.

2:03:26.100 --> 2:03:30.700
 Like there's a deep, like emotional connection there

2:03:30.700 --> 2:03:35.300
 that like, that's like way too dramatic to like,

2:03:35.300 --> 2:03:38.760
 I guess relative to what the actual moment is.

2:03:38.760 --> 2:03:43.760
 But I derive so much deep, like dramatic joy

2:03:45.240 --> 2:03:46.620
 from so many things in life.

2:03:46.620 --> 2:03:50.340
 And I think I would attribute that to bringing in Russia.

2:03:50.340 --> 2:03:54.000
 But the thing that sticks most of all is the friendship

2:03:54.000 --> 2:03:59.000
 and have now since then had one other friend like that

2:03:59.300 --> 2:04:02.620
 in the United States, he lives in Chicago.

2:04:02.620 --> 2:04:04.300
 His name is Matt.

2:04:04.300 --> 2:04:08.280
 And slowly here and there accumulating

2:04:08.280 --> 2:04:09.840
 really fascinating people,

2:04:09.840 --> 2:04:11.540
 but I'm very selective with that.

2:04:11.540 --> 2:04:14.340
 Funny enough, the few times,

2:04:16.140 --> 2:04:17.940
 it's not few, it's a lot of times now

2:04:17.940 --> 2:04:19.900
 interacting with Joe Rogan.

2:04:19.900 --> 2:04:21.840
 It sounds surreal to say,

2:04:21.840 --> 2:04:24.160
 but there was a kindred spirit there.

2:04:24.160 --> 2:04:25.660
 Like I've connected with him.

2:04:26.840 --> 2:04:27.980
 And there's been people like that

2:04:27.980 --> 2:04:29.860
 also in the grappling sports

2:04:29.860 --> 2:04:31.600
 that are really connected with.

2:04:31.600 --> 2:04:33.380
 I've actually struggled,

2:04:33.380 --> 2:04:36.900
 which is why I'm so glad to be your friend

2:04:36.900 --> 2:04:40.260
 is I've struggled to connect with scientists.

2:04:40.260 --> 2:04:41.100
 Like-

2:04:41.100 --> 2:04:42.580
 They can be a little bit wooden sometimes.

2:04:42.580 --> 2:04:43.420
 Yeah.

2:04:43.420 --> 2:04:44.240
 Even the biologists.

2:04:44.240 --> 2:04:45.460
 I mean, one thing that I-

2:04:45.460 --> 2:04:47.000
 Even the biologists.

2:04:47.000 --> 2:04:50.100
 Well, I'm so struck by the fact that you work with robots,

2:04:50.100 --> 2:04:51.740
 you're an engineer, AI.

2:04:51.740 --> 2:04:52.940
 Science, technology.

2:04:52.940 --> 2:04:55.320
 And that all sounds like hardware, right?

2:04:55.320 --> 2:04:56.460
 But what you're describing,

2:04:56.460 --> 2:04:58.260
 and I know is true about you,

2:04:58.260 --> 2:05:01.660
 is this deep emotional life and this resonance.

2:05:01.660 --> 2:05:02.700
 And it's really wonderful.

2:05:02.700 --> 2:05:05.200
 I actually think it's one of the reasons why

2:05:05.200 --> 2:05:07.700
 so many people, scientists and otherwise,

2:05:07.700 --> 2:05:09.900
 have gravitated towards you and your podcast

2:05:09.900 --> 2:05:12.860
 is because you hold both elements.

2:05:12.860 --> 2:05:14.240
 In Hermann Hesse's book,

2:05:14.240 --> 2:05:16.100
 I don't know if you were at Narcissus and Goldman, right?

2:05:16.100 --> 2:05:19.400
 It's about these elements of the logical, rational mind

2:05:19.400 --> 2:05:21.460
 and the emotional mind

2:05:21.460 --> 2:05:22.940
 and how those are woven together.

2:05:22.940 --> 2:05:24.860
 And if people haven't read it, they should.

2:05:24.860 --> 2:05:27.380
 And you embody the full picture.

2:05:27.380 --> 2:05:30.020
 And I think that's so much of what draws people to you.

2:05:30.020 --> 2:05:31.940
 I've read every Hermann Hesse book, by the way.

2:05:31.940 --> 2:05:36.940
 As usual, I've done about 9% of what Lexi said.

2:05:36.980 --> 2:05:38.560
 No, it's true.

2:05:38.560 --> 2:05:42.340
 You mentioned Joe, who is a phenomenal human being,

2:05:42.340 --> 2:05:44.260
 not just for his amazing accomplishments,

2:05:44.260 --> 2:05:48.500
 but for how he shows up to the world one-on-one.

2:05:48.500 --> 2:05:51.620
 I think I heard him say the other day on an interview,

2:05:51.620 --> 2:05:55.660
 he said, there is no public or private version of him.

2:05:55.660 --> 2:05:56.980
 He's like, this is me.

2:05:56.980 --> 2:05:58.340
 He said the word, it was beautiful.

2:05:58.340 --> 2:06:00.940
 He said, I'm like the fish that got through the net.

2:06:00.940 --> 2:06:03.580
 There is no on-stage, off-stage version.

2:06:03.580 --> 2:06:04.580
 And you're absolutely right.

2:06:04.580 --> 2:06:09.580
 And I, so, well, you guys, I have a question actually about-

2:06:09.820 --> 2:06:10.860
 But that's a really good point

2:06:10.860 --> 2:06:12.620
 about public and private life.

2:06:12.620 --> 2:06:15.620
 He was a huge, if I could just comment real quick.

2:06:15.620 --> 2:06:18.560
 Like that, he was, I've been a fan of Joe for a long time,

2:06:18.560 --> 2:06:20.620
 but he's been an inspiration

2:06:20.620 --> 2:06:25.060
 to not have any difference between public and private life.

2:06:25.060 --> 2:06:28.180
 I actually had a conversation with Naval about this.

2:06:28.180 --> 2:06:33.180
 And he said that you can't have a rich life,

2:06:34.260 --> 2:06:36.620
 like an exciting life

2:06:36.620 --> 2:06:39.340
 if you're the same person publicly and privately.

2:06:39.340 --> 2:06:42.420
 And I think I understand that idea,

2:06:42.420 --> 2:06:45.380
 but I don't agree with it.

2:06:45.380 --> 2:06:49.120
 I think it's really fulfilling and exciting

2:06:49.120 --> 2:06:51.340
 to be the same person privately and publicly,

2:06:51.340 --> 2:06:52.500
 with very few exceptions.

2:06:52.500 --> 2:06:57.500
 Now, that said, I don't have any really strange sex kinks.

2:06:58.040 --> 2:07:00.420
 So like, I feel like I can be open with basically everything.

2:07:00.420 --> 2:07:02.340
 I don't have anything I'm ashamed of.

2:07:03.540 --> 2:07:05.660
 There's some things that could be perceived poorly,

2:07:05.660 --> 2:07:09.040
 like the screaming Arumbas, but I'm not ashamed of them.

2:07:09.040 --> 2:07:11.420
 I just have to present them in the right context.

2:07:11.420 --> 2:07:15.600
 But there's freedom to being the same person in private

2:07:15.600 --> 2:07:16.620
 as in public.

2:07:16.620 --> 2:07:21.620
 And that Joe made me realize that you can be that

2:07:22.460 --> 2:07:25.220
 and also to be kind to others.

2:07:25.220 --> 2:07:28.620
 It sounds kind of absurd,

2:07:28.620 --> 2:07:33.620
 but I really always enjoyed like being good to others.

2:07:38.420 --> 2:07:41.100
 Like just being kind towards others.

2:07:41.100 --> 2:07:45.380
 But I always felt like the world didn't want me to be.

2:07:45.380 --> 2:07:48.500
 Like there's so much negativity when I was growing up,

2:07:48.500 --> 2:07:49.700
 like just around people.

2:07:49.700 --> 2:07:52.520
 If you actually just notice how people talk,

2:07:54.980 --> 2:07:57.460
 from like complaining about the weather,

2:07:57.460 --> 2:07:59.620
 this could be just like the big cities that I visited in,

2:07:59.620 --> 2:08:02.020
 but there's a general negativity

2:08:02.020 --> 2:08:05.860
 and positivity is kind of suppressed.

2:08:05.860 --> 2:08:08.620
 One, you're not seen as very intelligent.

2:08:08.620 --> 2:08:13.220
 And two, you're seen as like a little bit of a weirdo.

2:08:13.220 --> 2:08:15.740
 And so I always felt like I had to hide that.

2:08:15.740 --> 2:08:17.100
 And what Joe made me realize,

2:08:17.100 --> 2:08:21.220
 one, I could be fully just the same person,

2:08:21.220 --> 2:08:22.220
 private and public.

2:08:22.220 --> 2:08:25.220
 And two, I can embrace being kind

2:08:25.220 --> 2:08:28.220
 and just in the way that I like,

2:08:28.220 --> 2:08:31.220
 in the way I know how to do.

2:08:31.220 --> 2:08:33.860
 And sort of for me on like on Twitter

2:08:35.140 --> 2:08:37.500
 or like publicly, whenever I say stuff,

2:08:37.500 --> 2:08:39.620
 that means saying stuff simply,

2:08:39.620 --> 2:08:41.220
 almost to the point of cliche.

2:08:41.220 --> 2:08:44.540
 And like, I have the strength now to say it,

2:08:44.540 --> 2:08:47.140
 even if I'm being mocked, you know what I mean?

2:08:47.140 --> 2:08:48.680
 Like just, it's okay.

2:08:48.680 --> 2:08:50.380
 If everything's going to be okay.

2:08:50.380 --> 2:08:52.520
 Okay, some people will think you're dumb.

2:08:52.520 --> 2:08:53.520
 They're probably right.

2:08:53.520 --> 2:08:56.260
 The point is like, it's just enjoy being yourself.

2:08:56.260 --> 2:08:58.300
 And Joe more than almost anybody else,

2:08:58.300 --> 2:09:03.060
 because he's so successful at it, inspired me to do that.

2:09:03.060 --> 2:09:06.200
 Be kind and be the same person, private and public.

2:09:06.200 --> 2:09:08.660
 I love it, and I love the idea that authenticity

2:09:08.660 --> 2:09:11.520
 doesn't have to be oversharing, right?

2:09:11.520 --> 2:09:14.460
 That it doesn't mean you reveal every detail of your life,

2:09:14.460 --> 2:09:17.220
 what, you know, it's a way of being true

2:09:17.220 --> 2:09:19.000
 to an essence of oneself.

2:09:19.000 --> 2:09:21.220
 Right, there's never a feeling

2:09:22.300 --> 2:09:24.420
 when you deeply think and introspect

2:09:24.420 --> 2:09:26.100
 that you're hiding something from the world

2:09:26.100 --> 2:09:28.740
 or you're being dishonest in some fundamental way.

2:09:28.740 --> 2:09:33.480
 So yeah, that's truly liberating.

2:09:33.480 --> 2:09:36.060
 It allows you to think, it allows you to,

2:09:36.060 --> 2:09:38.680
 like think freely, to speak freely,

2:09:38.680 --> 2:09:42.100
 to just to be freely.

2:09:42.100 --> 2:09:45.840
 That said, it's not like, you know,

2:09:47.060 --> 2:09:49.500
 it's not like there's not still a responsibility

2:09:49.500 --> 2:09:52.120
 to be the best version of yourself.

2:09:52.120 --> 2:09:57.120
 So, you know, I'm very careful with the way I say something.

2:09:57.180 --> 2:10:00.680
 So the whole point, it's not so simple

2:10:00.680 --> 2:10:04.820
 to express the spirit that's inside you with words.

2:10:04.820 --> 2:10:06.500
 It depends, I mean, some people are much better

2:10:06.500 --> 2:10:07.500
 than others.

2:10:08.740 --> 2:10:12.740
 I struggle, like oftentimes when I say something

2:10:12.740 --> 2:10:15.240
 and I hear myself say it, it sounds really dumb

2:10:15.240 --> 2:10:16.700
 and not at all what I meant.

2:10:16.700 --> 2:10:18.400
 So that's the responsibility you have.

2:10:18.400 --> 2:10:21.380
 It's not just like being the same person publicly

2:10:21.380 --> 2:10:24.460
 and privately means you can just say whatever the hell.

2:10:24.460 --> 2:10:27.640
 It means there's still a responsibility to try to be,

2:10:27.640 --> 2:10:29.300
 to express who you truly are.

2:10:29.300 --> 2:10:32.740
 And that's hard.

2:10:32.740 --> 2:10:37.640
 It is hard and I think that, you know, we have this pressure,

2:10:37.640 --> 2:10:40.900
 all people, when I say we, I mean all humans,

2:10:40.900 --> 2:10:44.480
 and maybe robots too, feel this pressure

2:10:44.480 --> 2:10:47.580
 to be able to express ourselves in that one moment,

2:10:47.580 --> 2:10:48.900
 in that one form.

2:10:48.900 --> 2:10:51.260
 And it is beautiful when somebody, for instance,

2:10:51.260 --> 2:10:53.740
 can capture some essence of love or sadness

2:10:53.740 --> 2:10:57.160
 or anger or something in a song or in a poem

2:10:57.160 --> 2:10:58.740
 or in a short quote.

2:10:58.740 --> 2:11:02.740
 But perhaps it's also possible to do it in aggregate.

2:11:02.740 --> 2:11:06.080
 You know, all the things, you know, how you show up.

2:11:06.080 --> 2:11:08.680
 For instance, one of the things that initially drew me

2:11:08.680 --> 2:11:10.620
 to want to get to know you as a human being

2:11:10.620 --> 2:11:13.700
 and a scientist and eventually we became friends

2:11:13.700 --> 2:11:16.580
 was the level of respect that you brought

2:11:16.580 --> 2:11:19.620
 to your podcast listeners by wearing a suit.

2:11:19.620 --> 2:11:20.620
 I'm being serious here.

2:11:20.620 --> 2:11:23.460
 You know, I was raised thinking that if you overdress

2:11:23.460 --> 2:11:25.940
 a little bit, overdress by American,

2:11:25.940 --> 2:11:27.260
 certainly by American standards,

2:11:27.260 --> 2:11:28.640
 you're overdressed for a podcast,

2:11:28.640 --> 2:11:30.540
 but this is, but it's genuine.

2:11:30.540 --> 2:11:31.820
 You're not doing it for any reason,

2:11:31.820 --> 2:11:35.060
 except I have to assume, and I assumed at the time,

2:11:35.060 --> 2:11:37.940
 that it was because you have a respect for your audience.

2:11:37.940 --> 2:11:42.400
 You respect them enough to show up a certain way for them.

2:11:42.400 --> 2:11:44.420
 It's for you also, but it's for them.

2:11:44.420 --> 2:11:47.060
 And I think between that and your commitment

2:11:47.060 --> 2:11:49.540
 to your friendships, the way that you talk about friendships

2:11:49.540 --> 2:11:52.820
 and love and the way you hold up these higher ideals,

2:11:52.820 --> 2:11:56.620
 I think at least as a consumer of your content

2:11:56.620 --> 2:12:01.620
 and as your friend, what I find is that in aggregate,

2:12:01.760 --> 2:12:03.360
 you're communicating who you are.

2:12:03.360 --> 2:12:05.840
 It doesn't have to be one quote or something.

2:12:05.840 --> 2:12:08.160
 And I think that we were sort of obsessed

2:12:08.160 --> 2:12:10.120
 by like the one Einstein quote

2:12:10.120 --> 2:12:13.000
 or the one line of poetry or something,

2:12:13.000 --> 2:12:18.000
 but I think you so embody the way that, and Joe as well,

2:12:18.280 --> 2:12:21.080
 it's about how you live your life and how you show up

2:12:21.080 --> 2:12:24.880
 as a collection of things and said and done.

2:12:24.880 --> 2:12:28.000
 Yeah, that's fascinating, so the aggregate is the goal.

2:12:28.000 --> 2:12:32.200
 The tricky thing, and Jordan Peterson talks about this

2:12:32.200 --> 2:12:34.400
 because he's under attack way more than you and I

2:12:34.400 --> 2:12:36.200
 will ever be, but that-

2:12:36.200 --> 2:12:37.200
 For now.

2:12:37.200 --> 2:12:38.500
 For now, right?

2:12:38.500 --> 2:12:40.400
 This is very true for now.

2:12:40.400 --> 2:12:45.400
 That the people who attack on the internet,

2:12:46.940 --> 2:12:49.080
 this is one of the problems with Twitter,

2:12:49.080 --> 2:12:53.160
 is they don't consider the aggregate.

2:12:53.160 --> 2:12:55.320
 They take a single statements.

2:12:55.320 --> 2:12:58.440
 And so one of the defense mechanisms,

2:12:58.440 --> 2:13:01.160
 like again, why Joe has been an inspiration

2:13:01.160 --> 2:13:05.540
 is that when you in aggregate are a good person,

2:13:05.540 --> 2:13:07.040
 a lot of people will know that.

2:13:07.040 --> 2:13:08.940
 And so that makes you much more immune

2:13:08.940 --> 2:13:10.000
 to the attacks of people

2:13:10.000 --> 2:13:11.900
 that bring out an individual statement

2:13:11.900 --> 2:13:14.240
 that might be a misstatement of some kind

2:13:14.240 --> 2:13:16.840
 or doesn't express who you are.

2:13:16.840 --> 2:13:20.940
 And so that, I like that idea is the aggregate

2:13:20.940 --> 2:13:25.720
 and the power of the podcast is you have hundreds

2:13:25.720 --> 2:13:28.500
 of hours out there and being yourself

2:13:28.500 --> 2:13:30.200
 and people get to know who you are.

2:13:30.200 --> 2:13:33.520
 And once they do and you post pictures

2:13:33.520 --> 2:13:36.520
 of screaming Roombas as you kick them,

2:13:36.520 --> 2:13:38.120
 they will understand that you don't mean well.

2:13:38.120 --> 2:13:40.120
 By the way, as a side comment,

2:13:41.280 --> 2:13:42.920
 I don't know if I want to release this

2:13:42.920 --> 2:13:45.520
 because it's not just the Roombas.

2:13:46.760 --> 2:13:48.480
 You have a whole dungeon of robots.

2:13:48.480 --> 2:13:51.840
 Okay, so this is a problem.

2:13:51.840 --> 2:13:54.440
 Boston Dynamics came up against this problem,

2:13:54.440 --> 2:13:57.240
 but let me just, let me work this out,

2:13:57.240 --> 2:13:58.840
 like workshop this out with you.

2:13:59.840 --> 2:14:02.520
 And maybe because we'll post this,

2:14:02.520 --> 2:14:03.720
 people will let me know.

2:14:05.440 --> 2:14:07.580
 So there's legged robots.

2:14:07.580 --> 2:14:08.800
 They look like a dog.

2:14:08.800 --> 2:14:09.640
 They have a very,

2:14:09.640 --> 2:14:13.980
 I'm trying to create a very real human robot connection,

2:14:13.980 --> 2:14:15.400
 but like they're also incredible

2:14:15.400 --> 2:14:19.480
 because you can throw them like off of a building

2:14:19.480 --> 2:14:21.400
 and it'll land fine.

2:14:21.400 --> 2:14:22.380
 And it's beautiful.

2:14:22.380 --> 2:14:23.220
 That's amazing.

2:14:23.220 --> 2:14:25.320
 I've seen the Instagram videos of like cats

2:14:25.320 --> 2:14:27.280
 jumping off of like fifth story buildings

2:14:27.280 --> 2:14:29.080
 and then walking away.

2:14:29.080 --> 2:14:30.960
 No one should throw their cat out of a window.

2:14:30.960 --> 2:14:32.520
 This is the problem I'm experiencing.

2:14:32.520 --> 2:14:34.680
 I'll certainly kicking the robots.

2:14:34.680 --> 2:14:38.000
 It's really fascinating how they recover from those kicks,

2:14:38.000 --> 2:14:40.440
 but like just seeing myself do it

2:14:40.440 --> 2:14:43.360
 and also seeing others do it, it just does not look good.

2:14:43.360 --> 2:14:44.920
 And I don't know what to do with that.

2:14:44.920 --> 2:14:46.280
 Cause I, it's such a-

2:14:46.280 --> 2:14:47.120
 I'll do it.

2:14:49.400 --> 2:14:52.920
 See, but you don't, I, you, cause you-

2:14:52.920 --> 2:14:54.040
 Robot, no, I'm kidding.

2:14:54.040 --> 2:14:55.960
 Now I'm, you know what's interesting?

2:14:55.960 --> 2:14:56.800
 Yeah.

2:14:56.800 --> 2:14:59.980
 Before today's conversation, I probably could do it.

2:14:59.980 --> 2:15:03.040
 And now I think I'm thinking about robots,

2:15:03.040 --> 2:15:04.600
 bills of rights and things I'm actually,

2:15:04.600 --> 2:15:07.400
 and not for any, not to satisfy you

2:15:07.400 --> 2:15:10.000
 or to satisfy anything, except that if I,

2:15:10.000 --> 2:15:14.120
 if they have some sentient aspect to their being,

2:15:14.120 --> 2:15:16.360
 then I would loathe to kick it.

2:15:16.360 --> 2:15:17.660
 I don't think you'd be able to kick it.

2:15:17.660 --> 2:15:20.120
 You might be able to get the first time, but not the second.

2:15:20.120 --> 2:15:21.840
 This is, this is the problem I've experienced.

2:15:21.840 --> 2:15:25.760
 One of the cool things is one of the robots I'm,

2:15:25.760 --> 2:15:26.600
 I'm working with,

2:15:26.600 --> 2:15:29.340
 you can pick it up by one leg and it's dangling.

2:15:29.340 --> 2:15:31.540
 You can throw it in any kind of way

2:15:31.540 --> 2:15:33.680
 and it'll land correctly.

2:15:33.680 --> 2:15:34.500
 So it's really-

2:15:34.500 --> 2:15:35.720
 I had a friend who had a cat like that.

2:15:35.720 --> 2:15:40.560
 Oh man, we look forward to the letters from the cat.

2:15:40.560 --> 2:15:42.080
 Oh no, I'm not suggesting anyone did that,

2:15:42.080 --> 2:15:45.320
 but he had this cat and the cat, he would just, you know,

2:15:45.320 --> 2:15:46.800
 throw it onto the bed from across the room

2:15:46.800 --> 2:15:48.840
 and then it would run back for more.

2:15:48.840 --> 2:15:51.580
 Somehow they had, that was the nature of the relationship.

2:15:51.580 --> 2:15:54.080
 I think most, no one should do that to an animal,

2:15:54.080 --> 2:15:56.560
 but this cat seemed to, you know,

2:15:56.560 --> 2:15:58.000
 return for it for whatever reason.

2:15:58.000 --> 2:16:00.100
 The robot is a robot and it's fascinating to me

2:16:00.100 --> 2:16:02.240
 how hard it is for me to do that.

2:16:02.240 --> 2:16:04.000
 So it's unfortunate,

2:16:04.000 --> 2:16:06.240
 but I don't think I can do that to a robot.

2:16:06.240 --> 2:16:08.240
 Like I struggle with that.

2:16:08.240 --> 2:16:13.240
 So for me to be able to do that with a robot,

2:16:13.520 --> 2:16:15.820
 I have to almost get like into the state

2:16:15.820 --> 2:16:17.720
 that I imagine like doctors get into

2:16:17.720 --> 2:16:19.340
 when they're doing surgery.

2:16:19.340 --> 2:16:20.760
 Like I have to start,

2:16:20.760 --> 2:16:22.960
 I have to do what robotics colleagues of mine do,

2:16:22.960 --> 2:16:25.080
 which is like start seeing it as an object.

2:16:25.080 --> 2:16:25.920
 Like dissociate.

2:16:25.920 --> 2:16:26.880
 Like dissociate.

2:16:26.880 --> 2:16:28.840
 So it was just fascinating that I have to do that

2:16:28.840 --> 2:16:30.700
 in order to do that with a robot.

2:16:30.700 --> 2:16:33.600
 I just wanted to take that a little bit of a tangent.

2:16:33.600 --> 2:16:35.000
 No, I think it's an important thing.

2:16:35.000 --> 2:16:39.600
 I mean, I am not, I'm not shy about the fact

2:16:39.600 --> 2:16:42.400
 that for many years I've worked on experimental animals

2:16:42.400 --> 2:16:44.700
 and that's been a very challenging aspect

2:16:44.700 --> 2:16:46.880
 to being a biologist, mostly mice,

2:16:46.880 --> 2:16:49.200
 but in the past, no longer, thank goodness,

2:16:49.200 --> 2:16:51.200
 cause I just don't like doing it,

2:16:51.200 --> 2:16:52.680
 larger animals as well.

2:16:52.680 --> 2:16:53.720
 And now I work on humans,

2:16:53.720 --> 2:16:56.280
 which I can give consent, verbal consent.

2:16:56.280 --> 2:17:00.800
 So I think that it's extremely important

2:17:00.800 --> 2:17:03.880
 to have an understanding of what the guidelines are

2:17:03.880 --> 2:17:06.200
 and where one's own boundaries are around this.

2:17:06.200 --> 2:17:09.040
 It's not just an important question.

2:17:09.040 --> 2:17:11.040
 It might be the most important question

2:17:11.040 --> 2:17:12.860
 before any work can progress.

2:17:12.860 --> 2:17:14.920
 So you asked me about friendship.

2:17:14.920 --> 2:17:18.180
 I know you have a lot of thoughts about friendship.

2:17:18.180 --> 2:17:20.680
 What do you think is the value of friendship in life?

2:17:22.320 --> 2:17:24.800
 Well, for me personally,

2:17:24.800 --> 2:17:29.800
 just because of my life trajectory and arc of friendship,

2:17:29.800 --> 2:17:34.440
 and I should say, I do have some female friends

2:17:34.440 --> 2:17:35.600
 that are just friends,

2:17:35.600 --> 2:17:37.100
 they're completely platonic relationships,

2:17:37.100 --> 2:17:39.640
 but it's been mostly male friendship to me has been-

2:17:39.640 --> 2:17:41.920
 It's been all male friendships to me, actually.

2:17:41.920 --> 2:17:43.000
 Interesting, yeah.

2:17:43.000 --> 2:17:45.680
 It's been an absolute lifeline.

2:17:45.680 --> 2:17:47.040
 They are my family.

2:17:47.040 --> 2:17:49.360
 I have a biological family and I have great respect

2:17:49.360 --> 2:17:51.260
 and love for them and an appreciation for them,

2:17:51.260 --> 2:17:55.260
 but it's provided me the,

2:17:57.720 --> 2:17:58.880
 I wouldn't even say confidence

2:17:58.880 --> 2:18:02.440
 because there's always an anxiety in taking any good risk

2:18:02.440 --> 2:18:04.200
 or any risk worth taking.

2:18:04.200 --> 2:18:08.860
 It's given me the sense that I should go for certain things

2:18:08.860 --> 2:18:10.720
 and try certain things to take risks,

2:18:10.720 --> 2:18:12.160
 to weather that anxiety.

2:18:12.160 --> 2:18:14.640
 And I don't consider myself

2:18:14.640 --> 2:18:16.760
 a particularly competitive person,

2:18:16.760 --> 2:18:21.760
 but I would sooner die than disappoint

2:18:21.840 --> 2:18:24.640
 or let down one of my friends.

2:18:24.640 --> 2:18:26.920
 I can think of nothing worse, actually,

2:18:26.920 --> 2:18:29.120
 than disappointing one of my friends.

2:18:29.120 --> 2:18:31.260
 Everything else is secondary to me.

2:18:31.260 --> 2:18:33.120
 Well, disappointment-

2:18:33.120 --> 2:18:35.880
 Disappointing meaning not,

2:18:35.880 --> 2:18:39.600
 I mean, certainly I strive always to show up

2:18:39.600 --> 2:18:41.560
 as best I can for the friendship,

2:18:41.560 --> 2:18:43.000
 and that can be in small ways.

2:18:43.000 --> 2:18:45.000
 That can mean making sure the phone is away.

2:18:45.000 --> 2:18:46.080
 Sometimes it's about,

2:18:48.520 --> 2:18:50.840
 I'm terrible with punctuality because I'm an academic

2:18:50.840 --> 2:18:53.380
 and so I just get lost in time and I don't mean anything by,

2:18:53.380 --> 2:18:58.380
 but striving to listen, to enjoy good times and to make time.

2:18:59.080 --> 2:19:01.840
 It kind of goes back to this first variable we talked about,

2:19:01.840 --> 2:19:04.000
 to make sure that I spend time

2:19:04.000 --> 2:19:06.880
 and to get time in person and check in.

2:19:09.360 --> 2:19:10.640
 I think there's so many ways

2:19:10.640 --> 2:19:12.560
 in which friendship is vital to me.

2:19:12.560 --> 2:19:14.820
 It's actually, to me, what makes life worth living.

2:19:14.820 --> 2:19:15.660
 Yeah.

2:19:17.400 --> 2:19:20.060
 I am surprised, like with the high school friends,

2:19:20.060 --> 2:19:22.640
 how we don't actually talk that often these days

2:19:22.640 --> 2:19:24.920
 in terms of time, but every time we see each other,

2:19:24.920 --> 2:19:27.080
 it's immediately right back to where we started.

2:19:27.080 --> 2:19:28.720
 So I struggle with that,

2:19:28.720 --> 2:19:30.360
 how much time you really allocate

2:19:32.120 --> 2:19:34.400
 for the friendship to be deeply meaningful

2:19:34.400 --> 2:19:36.760
 because they're always there with me,

2:19:36.760 --> 2:19:38.300
 even if we don't talk often.

2:19:39.520 --> 2:19:40.980
 So there's a kind of loyalty.

2:19:40.980 --> 2:19:44.000
 I think maybe it's a different style,

2:19:44.000 --> 2:19:47.000
 but I think to me,

2:19:47.000 --> 2:19:52.760
 friendship is being there in the hard times, I think.

2:19:52.760 --> 2:19:56.020
 I'm much more reliable when you're going through shit.

2:19:57.560 --> 2:19:59.560
 You're pretty reliable anyway.

2:19:59.560 --> 2:20:03.120
 No, but if you're like a wedding or something like that,

2:20:03.120 --> 2:20:07.720
 or I don't know, you want an award of some kind,

2:20:08.760 --> 2:20:12.200
 yeah, I'll congratulate the shit out of you,

2:20:12.200 --> 2:20:14.440
 but that's not, and I'll be there,

2:20:14.440 --> 2:20:16.700
 but that's not as important to me as being there

2:20:16.700 --> 2:20:19.560
 when nobody else is,

2:20:19.560 --> 2:20:22.680
 just being there when shit hits the fan

2:20:22.680 --> 2:20:25.680
 or something's tough where the world turns their back

2:20:25.680 --> 2:20:26.880
 on you, all those kinds of things.

2:20:26.880 --> 2:20:29.360
 That to me, that's where friendship is meaningful.

2:20:29.360 --> 2:20:30.920
 Well, I know that to be true about you,

2:20:30.920 --> 2:20:33.420
 and that's a felt thing and a real thing with you.

2:20:33.420 --> 2:20:35.560
 Let me ask one more thing about that actually,

2:20:35.560 --> 2:20:38.440
 because I'm not a practitioner of jujitsu.

2:20:38.440 --> 2:20:39.660
 I know you are, Joe is,

2:20:39.660 --> 2:20:42.400
 but years ago I read a book that I really enjoyed,

2:20:42.400 --> 2:20:44.800
 which is Sam Sheridan's book, A Fighter's Heart.

2:20:44.800 --> 2:20:46.920
 He talks about all these different forms of martial arts,

2:20:46.920 --> 2:20:50.640
 and maybe it was in the book, maybe it was in an interview,

2:20:50.640 --> 2:20:52.480
 but he said that, you know,

2:20:52.480 --> 2:20:56.200
 fighting or being in physical battle with somebody,

2:20:56.200 --> 2:20:58.880
 jujitsu, boxing, or some other form of physical,

2:20:58.880 --> 2:21:01.840
 direct physical contact between two individuals

2:21:01.840 --> 2:21:04.300
 creates this bond unlike any other,

2:21:04.300 --> 2:21:06.580
 because he said, it's like a one night stand.

2:21:06.580 --> 2:21:08.480
 You're sharing bodily fluids with somebody

2:21:08.480 --> 2:21:09.800
 that you barely know.

2:21:09.800 --> 2:21:12.040
 And I, you know, and I chuckled about it

2:21:12.040 --> 2:21:14.800
 because it's kind of funny and a kind of tongue in cheek,

2:21:14.800 --> 2:21:18.960
 but at the same time, I think this is a fundamental way

2:21:18.960 --> 2:21:22.280
 in which members of a species bond

2:21:22.280 --> 2:21:24.300
 is through physical contact.

2:21:24.300 --> 2:21:25.880
 And certainly there are other forms.

2:21:25.880 --> 2:21:27.360
 There's cuddling and there's hand holding

2:21:27.360 --> 2:21:29.680
 and there's sexual intercourse

2:21:29.680 --> 2:21:30.520
 and there's all sorts of things.

2:21:30.520 --> 2:21:31.720
 What's cuddling?

2:21:31.720 --> 2:21:32.720
 I haven't heard of it.

2:21:32.720 --> 2:21:33.700
 I heard this recently.

2:21:33.700 --> 2:21:36.140
 I didn't know this term, but there's a term.

2:21:36.140 --> 2:21:39.120
 They've turned the noun cupcake into a verb.

2:21:39.120 --> 2:21:41.740
 Cupcaking, it turns out, I just learned about this.

2:21:41.740 --> 2:21:45.060
 Cupcaking is when you spend time just cuddling.

2:21:45.060 --> 2:21:46.300
 I didn't know about this.

2:21:46.300 --> 2:21:47.300
 You heard it here first,

2:21:47.300 --> 2:21:48.800
 although I heard it first just the other day,

2:21:48.800 --> 2:21:50.000
 cupcaking is actually a verb.

2:21:50.000 --> 2:21:50.880
 So cuddling is everything.

2:21:50.880 --> 2:21:53.760
 It's not just like, is it in bed or is it on the couch?

2:21:53.760 --> 2:21:55.400
 Like what's cuddling?

2:21:55.400 --> 2:21:56.400
 I need to look up what cuddling is.

2:21:56.400 --> 2:21:57.240
 We need to look at this stuff

2:21:57.240 --> 2:21:59.000
 and we need to define the variables.

2:21:59.000 --> 2:22:02.840
 I think it definitely has to do with physical contact,

2:22:02.840 --> 2:22:07.840
 I am told, but in terms of battle, competition,

2:22:07.840 --> 2:22:12.640
 you know, and the Sheridan quote, I'm just curious.

2:22:12.640 --> 2:22:17.640
 So do you get close or feel a bond with people

2:22:18.120 --> 2:22:20.680
 that, for instance, you rolled jujitsu with,

2:22:20.680 --> 2:22:23.480
 even though you don't know anything else about them?

2:22:23.480 --> 2:22:25.520
 Is he, was he right about this?

2:22:25.520 --> 2:22:27.120
 Yeah, I mean, on many levels.

2:22:27.120 --> 2:22:28.400
 He also has the book, what,

2:22:28.400 --> 2:22:30.880
 A Fighter's Mind and The First Heart.

2:22:30.880 --> 2:22:32.120
 He's actually an excellent writer.

2:22:32.120 --> 2:22:34.520
 What's interesting about him, just briefly about Sheridan,

2:22:34.520 --> 2:22:36.160
 I don't know, but I did a little bit of research.

2:22:36.160 --> 2:22:38.640
 He went to Harvard.

2:22:38.640 --> 2:22:40.080
 He was an art major at Harvard.

2:22:40.080 --> 2:22:43.440
 He claims all he did was smoke cigarettes and do art.

2:22:43.440 --> 2:22:45.120
 I don't know if his art was any good.

2:22:45.120 --> 2:22:48.960
 And I think his father was in the SEAL teams.

2:22:48.960 --> 2:22:51.720
 And then when he got out of Harvard, graduated,

2:22:51.720 --> 2:22:52.880
 he took off around the world,

2:22:52.880 --> 2:22:54.440
 learning all the forms of martial arts

2:22:54.440 --> 2:22:56.640
 and was early to the kind of ultimate fighting

2:22:56.640 --> 2:22:59.020
 to kind of mix martial arts and things.

2:22:59.020 --> 2:22:59.960
 Great, great book.

2:22:59.960 --> 2:23:01.240
 Yeah, it's amazing.

2:23:01.240 --> 2:23:03.800
 I don't actually remember it, but I read it.

2:23:03.800 --> 2:23:06.960
 I remember thinking there was an amazing encapsulation

2:23:06.960 --> 2:23:10.520
 of what makes fighting the art,

2:23:10.520 --> 2:23:12.600
 like what makes it compelling.

2:23:12.600 --> 2:23:17.120
 I would say that there's so many ways that jiu-jitsu,

2:23:17.120 --> 2:23:21.080
 grappling, wrestling, combat sports in general,

2:23:21.080 --> 2:23:24.240
 is like one of the most intimate things you could do.

2:23:24.240 --> 2:23:25.640
 I don't know if I would describe it

2:23:25.640 --> 2:23:27.920
 in terms of bodily liquids and all those kinds of things.

2:23:27.920 --> 2:23:29.920
 I think he was more or less joking, but.

2:23:29.920 --> 2:23:34.920
 I think there's a few ways that it does that.

2:23:35.040 --> 2:23:39.360
 So one, because you're so vulnerable.

2:23:41.000 --> 2:23:44.200
 So that the honesty of stepping on the mat

2:23:45.520 --> 2:23:48.720
 and often all of us have ego thinking

2:23:48.720 --> 2:23:52.360
 we're better than we are at this particular art.

2:23:52.360 --> 2:23:55.880
 And then the honesty of being submitted

2:23:55.880 --> 2:23:58.720
 or being worse than you thought you are

2:23:58.720 --> 2:24:01.000
 and just sitting with that knowledge.

2:24:01.000 --> 2:24:02.000
 That kind of honesty,

2:24:02.000 --> 2:24:06.040
 we don't get to experience it in most of daily life.

2:24:06.040 --> 2:24:08.560
 We can continue living somewhat of an illusion

2:24:08.560 --> 2:24:10.760
 of our conceptions of ourselves

2:24:10.760 --> 2:24:13.760
 because people are not going to hit us with the reality.

2:24:13.760 --> 2:24:15.760
 The mat speaks only the truth,

2:24:15.760 --> 2:24:17.800
 that the reality just hits you.

2:24:17.800 --> 2:24:19.960
 And that vulnerability is the same

2:24:19.960 --> 2:24:22.520
 as like the loss of a loved one.

2:24:22.520 --> 2:24:26.320
 It's the loss of a reality that you knew before.

2:24:26.320 --> 2:24:28.160
 You now have to deal with this new reality.

2:24:28.160 --> 2:24:30.480
 And when you're sitting there in that vulnerability

2:24:30.480 --> 2:24:32.120
 and there's these other people

2:24:32.120 --> 2:24:34.320
 that are also sitting in that vulnerability,

2:24:34.320 --> 2:24:36.920
 you get to really connect like, fuck.

2:24:36.920 --> 2:24:40.200
 Like I'm not as special as I thought I was

2:24:40.200 --> 2:24:42.760
 and life is like not,

2:24:44.160 --> 2:24:46.000
 life is harsher than I thought I was

2:24:46.000 --> 2:24:47.680
 and we're just sitting there with that reality.

2:24:47.680 --> 2:24:50.000
 Some of us can put words to them, some of them can't.

2:24:50.000 --> 2:24:51.640
 So I think that definitely is the thing

2:24:51.640 --> 2:24:52.920
 that leads to intimacy.

2:24:52.920 --> 2:24:57.720
 The other thing is the human contact.

2:24:58.600 --> 2:25:03.600
 There is something about, I mean, like a big hug.

2:25:03.680 --> 2:25:06.800
 Like during COVID, very few people hugged me

2:25:06.800 --> 2:25:10.240
 and I hugged them and I always felt good when they did.

2:25:10.240 --> 2:25:13.880
 Like we're all tested and especially now we're vaccinated,

2:25:13.880 --> 2:25:16.360
 but there's still people, this is true of San Francisco,

2:25:16.360 --> 2:25:17.320
 this is true in Boston.

2:25:17.320 --> 2:25:19.360
 They want to keep not only six feet away,

2:25:19.360 --> 2:25:21.720
 but stay at home and never touch you.

2:25:21.720 --> 2:25:25.280
 That was, that loss of basic humanity

2:25:25.280 --> 2:25:29.400
 is the opposite of what I feel in jiu-jitsu

2:25:29.400 --> 2:25:33.920
 where it was like that contact where you're like,

2:25:33.920 --> 2:25:35.880
 I don't give a shit about whatever rules

2:25:35.880 --> 2:25:38.360
 we're supposed to have in society where you're not,

2:25:38.360 --> 2:25:40.440
 you have to keep a distance and all that kind of stuff.

2:25:40.440 --> 2:25:45.000
 Just the hug, like the intimacy of a hug

2:25:45.000 --> 2:25:46.440
 that's like a good bear hug

2:25:46.440 --> 2:25:49.720
 and you're like just controlling another person.

2:25:49.720 --> 2:25:52.080
 And also there is some kind of love communicating

2:25:52.080 --> 2:25:54.680
 through just trying to break each other's arms.

2:25:54.680 --> 2:25:57.600
 I don't exactly understand why violence

2:25:57.600 --> 2:26:02.600
 is such a close neighbor to love, but it is.

2:26:02.600 --> 2:26:04.760
 Well, in the hypothalamus,

2:26:04.760 --> 2:26:08.200
 the neurons that control sexual behavior,

2:26:08.200 --> 2:26:12.960
 but also non-sexual contact are not just nearby

2:26:12.960 --> 2:26:15.760
 the neurons that control aggression and fighting,

2:26:15.760 --> 2:26:19.960
 they are salt and pepper with those neurons.

2:26:19.960 --> 2:26:23.120
 It's a very interesting and it almost sounds

2:26:23.120 --> 2:26:25.040
 kind of risque and controversial and stuff.

2:26:25.040 --> 2:26:27.840
 I'm not anthropomorphizing about what this means,

2:26:27.840 --> 2:26:32.200
 but in the brain, those structures are interdigitated.

2:26:32.200 --> 2:26:36.000
 You can't separate them except at a very fine level.

2:26:36.000 --> 2:26:38.800
 And here the way you describe it is the same

2:26:38.800 --> 2:26:39.720
 as a real thing.

2:26:39.720 --> 2:26:42.840
 I do want to make an interesting comment.

2:26:42.840 --> 2:26:43.720
 Again, these are the things

2:26:43.720 --> 2:26:45.660
 that could be taken out of context,

2:26:45.660 --> 2:26:50.520
 but one of the amazing things about jiu-jitsu

2:26:50.520 --> 2:26:52.240
 is both guys and girls train it.

2:26:53.100 --> 2:26:54.760
 And I was surprised.

2:26:54.760 --> 2:26:57.280
 So like I'm a big fan of yoga pants,

2:26:59.600 --> 2:27:01.000
 at the gym kind of thing.

2:27:01.000 --> 2:27:04.800
 It reveals the beauty of the female form.

2:27:04.800 --> 2:27:07.760
 But the thing is like girls are dressed

2:27:07.760 --> 2:27:10.180
 in skintight clothes in jiu-jitsu often.

2:27:10.180 --> 2:27:13.880
 And I found myself like not at all thinking like that at all

2:27:13.880 --> 2:27:15.000
 when training with girls.

2:27:15.000 --> 2:27:17.160
 Well, the context is very non-sexual.

2:27:17.160 --> 2:27:20.040
 But I was surprised to learn that.

2:27:20.040 --> 2:27:21.160
 When I first started jiu-jitsu,

2:27:21.160 --> 2:27:22.640
 I thought wouldn't that be kind of weird

2:27:22.640 --> 2:27:26.360
 to train with the opposites in something so intimate?

2:27:26.360 --> 2:27:28.480
 Boys and girls, men and women,

2:27:28.480 --> 2:27:30.960
 they rolled jiu-jitsu together completely.

2:27:30.960 --> 2:27:31.800
 Interesting.

2:27:31.800 --> 2:27:35.320
 And the only times girls kind of try to stay away from guys,

2:27:35.320 --> 2:27:36.400
 I mean, there's two contexts.

2:27:36.400 --> 2:27:38.840
 Of course, there's always going to be creeps in this world.

2:27:38.840 --> 2:27:42.080
 So everyone knows who to stay away from.

2:27:42.080 --> 2:27:44.280
 And the other is like, there's a size disparity.

2:27:44.280 --> 2:27:46.240
 So girls will often try to roll with people

2:27:46.240 --> 2:27:48.280
 a little bit closer weight-wise.

2:27:48.280 --> 2:27:51.100
 But no, that's one of the things

2:27:51.100 --> 2:27:53.100
 that are empowering to women.

2:27:53.100 --> 2:27:54.280
 That's what they fall in love with

2:27:54.280 --> 2:27:56.320
 when they start doing jiu-jitsu is I can,

2:27:56.320 --> 2:27:58.680
 first of all, they gain an awareness

2:27:58.680 --> 2:28:00.960
 and a pride over their body, which is great.

2:28:00.960 --> 2:28:04.360
 And then second, they get, especially later on,

2:28:04.360 --> 2:28:05.960
 start submitting big dudes,

2:28:05.960 --> 2:28:09.240
 like these like bros that come in

2:28:09.240 --> 2:28:11.280
 who are all shredded and like muscular.

2:28:11.280 --> 2:28:15.320
 And they get through technique to exercise dominance

2:28:15.320 --> 2:28:16.160
 over them.

2:28:16.160 --> 2:28:17.720
 And that's a powerful feeling.

2:28:17.720 --> 2:28:21.560
 You've seen women force a larger guy to tap

2:28:21.560 --> 2:28:22.400
 or even choke him out.

2:28:22.400 --> 2:28:25.900
 Well, I was deadlifting for,

2:28:29.520 --> 2:28:31.520
 oh boy, I think it's 495.

2:28:31.520 --> 2:28:35.000
 So I was really into powerlifting when I started jiu-jitsu.

2:28:35.000 --> 2:28:37.400
 And I remember being submitted by,

2:28:37.400 --> 2:28:40.640
 I thought I walked in feeling like I'm going to be,

2:28:40.640 --> 2:28:43.240
 if not the greatest fighter of at least top three.

2:28:43.240 --> 2:28:47.400
 And so as a white belt, you roll in like all happy.

2:28:47.400 --> 2:28:50.740
 And then you realize that as long as you're not applying

2:28:50.740 --> 2:28:52.920
 too much force that you're having,

2:28:52.920 --> 2:28:54.360
 I remember being submitted many times

2:28:54.360 --> 2:28:57.220
 by like 130, 120 pound girls

2:28:57.220 --> 2:28:59.860
 at the Balance Studios in Philadelphia,

2:28:59.860 --> 2:29:02.480
 that a lot of incredible female jiu-jitsu players.

2:29:02.480 --> 2:29:04.360
 And that's really humbling too.

2:29:04.360 --> 2:29:09.360
 The technique can overpower in combat

2:29:09.360 --> 2:29:12.160
 and pure strength.

2:29:12.160 --> 2:29:15.540
 And that's the other thing that there is something

2:29:15.540 --> 2:29:18.140
 about combat that's primal.

2:29:18.140 --> 2:29:23.140
 Like it just feels, it feels like we were born to do this.

2:29:26.480 --> 2:29:27.320
 Like that there's-

2:29:27.320 --> 2:29:30.320
 But we have circuits in our brain that are dedicated

2:29:30.320 --> 2:29:32.380
 to this kind of interaction.

2:29:32.380 --> 2:29:34.080
 There's no question.

2:29:34.080 --> 2:29:35.720
 And like, that's what it felt like.

2:29:35.720 --> 2:29:38.920
 It wasn't that I'm learning a new skill.

2:29:38.920 --> 2:29:42.780
 It was like somehow I am remembering echoes

2:29:42.780 --> 2:29:44.400
 of something I've learned in the past.

2:29:44.400 --> 2:29:45.640
 It's like hitting puberty.

2:29:45.640 --> 2:29:47.760
 A child before puberty has no concept

2:29:47.760 --> 2:29:51.020
 of boys and girls having this attraction,

2:29:51.020 --> 2:29:52.600
 regardless of whether or not they're attracted

2:29:52.600 --> 2:29:53.640
 to boys or girls, doesn't matter.

2:29:53.640 --> 2:29:56.400
 At some point, most people, not all, but certainly,

2:29:56.400 --> 2:29:58.360
 but most people, when they hit puberty,

2:29:58.360 --> 2:30:01.280
 suddenly people appear differently.

2:30:01.280 --> 2:30:05.540
 And certain people take on a romantic or sexual interest

2:30:05.540 --> 2:30:07.560
 for the very first time.

2:30:07.560 --> 2:30:11.400
 And so it's like, it's revealing a circuitry in the brain.

2:30:11.400 --> 2:30:14.360
 It's not like they learned that, it's innate.

2:30:14.360 --> 2:30:18.120
 And I think when I hear the way you describe jiu-jitsu

2:30:18.120 --> 2:30:21.120
 and rolling jiu-jitsu, it reminds me a little bit,

2:30:21.120 --> 2:30:23.340
 Joe was telling me recently about the first time

2:30:23.340 --> 2:30:26.680
 he went hunting and he felt like it revealed a circuit

2:30:26.680 --> 2:30:28.560
 that was in him all along,

2:30:28.560 --> 2:30:30.960
 but he hadn't experienced before.

2:30:30.960 --> 2:30:32.360
 Yeah, that's definitely there.

2:30:32.360 --> 2:30:34.960
 And of course there's the physical activity.

2:30:34.960 --> 2:30:37.280
 One of the interesting things about jiu-jitsu

2:30:37.280 --> 2:30:40.880
 is it's one of the really strenuous exercises

2:30:40.880 --> 2:30:43.880
 that you can do late into your adult life,

2:30:43.880 --> 2:30:48.000
 like into your 50s, 60s, 70s, 80s.

2:30:48.000 --> 2:30:50.480
 When I came up, there's a few people in their 80s

2:30:50.480 --> 2:30:51.720
 that were training.

2:30:51.720 --> 2:30:53.080
 And as long as you're smart,

2:30:53.080 --> 2:30:54.840
 as long as you practice techniques

2:30:54.840 --> 2:30:55.940
 and pick your partners correctly,

2:30:55.940 --> 2:30:57.320
 you can do that kind of art.

2:30:57.320 --> 2:30:59.880
 It's late into life and so you're getting exercise.

2:30:59.880 --> 2:31:01.880
 There's not many activities I find

2:31:01.880 --> 2:31:04.500
 that are amenable to that.

2:31:04.500 --> 2:31:07.760
 So because it's such a thinking game,

2:31:07.760 --> 2:31:10.880
 the jiu-jitsu in particular is an art

2:31:10.880 --> 2:31:13.360
 where technique pays off a lot.

2:31:13.360 --> 2:31:17.240
 So you can still maintain, first of all,

2:31:17.240 --> 2:31:20.840
 remain injury free if you use good technique

2:31:20.840 --> 2:31:22.960
 and also through good technique,

2:31:24.280 --> 2:31:26.920
 be able to go, be active with people

2:31:26.920 --> 2:31:28.480
 that are much, much younger.

2:31:28.480 --> 2:31:30.560
 And so that was to me,

2:31:30.560 --> 2:31:32.480
 that and running are the two activities

2:31:32.480 --> 2:31:33.840
 you can kind of do late in life

2:31:33.840 --> 2:31:38.280
 because to me, a healthy life has exercise

2:31:38.280 --> 2:31:39.360
 as a piece of the puzzle.

2:31:39.360 --> 2:31:40.400
 No, absolutely.

2:31:40.400 --> 2:31:42.720
 And I'm glad that we're on the physical component

2:31:42.720 --> 2:31:47.720
 because I know that there's, for you,

2:31:47.960 --> 2:31:49.680
 you've talked before about the crossover

2:31:49.680 --> 2:31:52.440
 between the physical and the intellectual and the mental.

2:31:54.980 --> 2:31:57.680
 Are you still running at ridiculous hours of the night

2:31:57.680 --> 2:31:59.600
 for ridiculously long?

2:31:59.600 --> 2:32:01.520
 Yeah, so definitely.

2:32:01.520 --> 2:32:03.520
 I've been running late at night here in Austin.

2:32:03.520 --> 2:32:05.800
 People tell me, the area we're in now,

2:32:05.800 --> 2:32:07.240
 people say is a dangerous area,

2:32:07.240 --> 2:32:10.580
 which I find laughable coming from the bigger cities.

2:32:10.580 --> 2:32:12.780
 No, I've run late at night.

2:32:12.780 --> 2:32:14.160
 There's something.

2:32:15.160 --> 2:32:17.580
 If you see a guy running through Austin at 2 a.m.

2:32:17.580 --> 2:32:19.920
 in a suit and tie, it's probably.

2:32:22.200 --> 2:32:24.080
 Well, yeah, I mean, I do think about that

2:32:24.080 --> 2:32:26.640
 because I get recognized more and more in Austin.

2:32:26.640 --> 2:32:29.040
 I worry that, not really,

2:32:29.040 --> 2:32:30.800
 that I get recognized late at night.

2:32:30.800 --> 2:32:34.980
 But there is something about the night

2:32:36.600 --> 2:32:38.960
 that brings out those deep philosophical thoughts

2:32:38.960 --> 2:32:40.720
 and self-reflection that I really enjoy.

2:32:40.720 --> 2:32:44.560
 But recently I started getting back to the grind.

2:32:44.560 --> 2:32:47.680
 So I'm gonna be competing or hoping to be compete

2:32:47.680 --> 2:32:49.840
 in September and October.

2:32:49.840 --> 2:32:50.680
 In jiu-jitsu.

2:32:50.680 --> 2:32:53.120
 In jiu-jitsu, yeah, to get back to competition.

2:32:53.120 --> 2:32:58.120
 And so that requires getting back into great cardio shape.

2:32:58.120 --> 2:33:00.560
 And so I've been getting running

2:33:00.560 --> 2:33:02.280
 as part of my daily routine.

2:33:02.280 --> 2:33:03.640
 Got it.

2:33:03.640 --> 2:33:05.140
 Well, I always know I can reach you

2:33:05.140 --> 2:33:08.000
 regardless of time zone in the middle of the night,

2:33:08.000 --> 2:33:09.280
 wherever that happens.

2:33:09.280 --> 2:33:11.120
 Well, part of that has to be just being single

2:33:11.120 --> 2:33:13.780
 and being a programmer.

2:33:13.780 --> 2:33:16.040
 Those two things just don't work well

2:33:16.040 --> 2:33:18.040
 in terms of a steady sleep schedule.

2:33:18.040 --> 2:33:21.440
 It's not banker's hours kind of work, nine to five.

2:33:21.440 --> 2:33:23.280
 I want to, you mentioned single.

2:33:23.280 --> 2:33:24.840
 I want to ask you a little bit

2:33:24.840 --> 2:33:26.400
 about the other form of relationship,

2:33:26.400 --> 2:33:29.700
 which is a romantic love.

2:33:29.700 --> 2:33:32.400
 So your parents are still married?

2:33:32.400 --> 2:33:34.120
 Still married, still happily married.

2:33:34.120 --> 2:33:34.960
 That's impressive.

2:33:34.960 --> 2:33:35.780
 Yeah.

2:33:35.780 --> 2:33:36.620
 A rare thing nowadays.

2:33:36.620 --> 2:33:37.460
 Yeah.

2:33:37.460 --> 2:33:38.960
 So you grew up with that example.

2:33:38.960 --> 2:33:40.920
 Yeah, I guess that's a powerful thing, right?

2:33:40.920 --> 2:33:43.120
 If there's an example that I think can work.

2:33:44.640 --> 2:33:46.480
 Yeah, I didn't have that in my own family,

2:33:46.480 --> 2:33:51.480
 but when I see it, it's inspiring and it's beautiful.

2:33:52.040 --> 2:33:55.040
 The fact that they have that and that was the norm for you,

2:33:55.040 --> 2:33:57.000
 I think is really wonderful.

2:33:58.000 --> 2:34:00.080
 In the case of my parents, it was interesting to watch

2:34:00.080 --> 2:34:03.240
 because there's obviously tension.

2:34:03.240 --> 2:34:04.920
 Like there'll be times where they fought

2:34:04.920 --> 2:34:06.680
 and all those kinds of things.

2:34:06.680 --> 2:34:09.920
 They obviously get frustrated with each other

2:34:09.920 --> 2:34:13.320
 and they like, but they find mechanisms

2:34:13.320 --> 2:34:15.000
 how to communicate that to each other,

2:34:15.000 --> 2:34:16.520
 like to make fun of each other a little bit,

2:34:16.520 --> 2:34:19.400
 like to tease, to get some of that frustration out

2:34:19.400 --> 2:34:21.040
 and then ultimately to reunite

2:34:21.040 --> 2:34:25.280
 and find their joyful moments and be that the energy.

2:34:25.280 --> 2:34:27.600
 I think it's clear because I got together in there,

2:34:27.600 --> 2:34:30.280
 I think early twenties, like very, very young.

2:34:30.280 --> 2:34:32.440
 I think you grow together as people.

2:34:32.440 --> 2:34:34.320
 Yeah, you're still in the critical period

2:34:34.320 --> 2:34:35.580
 of brain plasticity.

2:34:35.580 --> 2:34:40.200
 And also, I mean, it's just like divorce

2:34:40.200 --> 2:34:43.860
 was so frowned upon that you stick it out.

2:34:43.860 --> 2:34:44.960
 And I think a lot of couples,

2:34:44.960 --> 2:34:46.640
 especially from that time in the Soviet Union,

2:34:46.640 --> 2:34:48.600
 that's probably applies to a lot of cultures.

2:34:48.600 --> 2:34:50.640
 You stick it out and you put in the work.

2:34:50.640 --> 2:34:52.360
 You learn how to put in the work.

2:34:52.360 --> 2:34:54.940
 And once you do, you start to get to some of those

2:34:54.940 --> 2:34:59.080
 rewarding aspects of being like through time,

2:34:59.080 --> 2:35:00.780
 sharing so many moments together.

2:35:03.120 --> 2:35:07.840
 That's definitely something that was an inspiration to me,

2:35:07.840 --> 2:35:09.720
 but maybe that's where I have,

2:35:09.720 --> 2:35:11.340
 so I have a similar kind of longing

2:35:11.340 --> 2:35:13.280
 to have a lifelong partner like that,

2:35:13.280 --> 2:35:16.760
 have that kind of view where same with friendship,

2:35:16.760 --> 2:35:20.600
 lifelong friendship is the most meaningful kind

2:35:20.600 --> 2:35:22.040
 that there is something with that time

2:35:22.040 --> 2:35:24.120
 of sharing all that time together,

2:35:24.120 --> 2:35:26.800
 like till death do us part as a powerful thing,

2:35:26.800 --> 2:35:29.200
 not by force, not because the religion said it

2:35:29.200 --> 2:35:31.580
 or the government said it or your culture said it,

2:35:31.580 --> 2:35:33.240
 but because you want to.

2:35:33.240 --> 2:35:34.520
 Do you want children?

2:35:34.520 --> 2:35:35.920
 Definitely, yeah.

2:35:35.920 --> 2:35:37.340
 Definitely want children.

2:35:38.400 --> 2:35:39.220
 It's common.

2:35:39.220 --> 2:35:41.280
 How many Roombas do you have?

2:35:41.280 --> 2:35:43.240
 Oh, I thought you should know human children.

2:35:43.240 --> 2:35:44.400
 No, human children.

2:35:44.400 --> 2:35:45.560
 Because I already have the children.

2:35:45.560 --> 2:35:46.880
 Exactly, well, I was saying you probably need

2:35:46.880 --> 2:35:49.520
 at least as many human children as you do Roombas,

2:35:49.520 --> 2:35:50.980
 big family, small family.

2:35:53.360 --> 2:35:55.160
 In your mind's eye, is there a big,

2:35:55.160 --> 2:35:59.160
 are there a bunch of freedman's running around?

2:35:59.160 --> 2:36:01.240
 So I'll tell you like realistically,

2:36:01.240 --> 2:36:04.080
 I can explain exactly my thinking.

2:36:04.080 --> 2:36:06.160
 And this is similar to the robotics work

2:36:06.160 --> 2:36:10.080
 is if I'm like purely logical right now,

2:36:10.080 --> 2:36:12.320
 my answer would be I don't want kids

2:36:12.320 --> 2:36:15.720
 because I just don't have enough time.

2:36:15.720 --> 2:36:17.280
 I have so much going on.

2:36:17.280 --> 2:36:19.240
 But when I'm using the same kind of vision

2:36:19.240 --> 2:36:22.620
 I use for the robots is I know my life

2:36:22.620 --> 2:36:25.000
 will be transformed with the first.

2:36:25.000 --> 2:36:27.640
 Like I know I would love being a father.

2:36:27.640 --> 2:36:30.280
 And so the question of how many,

2:36:30.280 --> 2:36:33.140
 that's on the other side of that hill.

2:36:33.140 --> 2:36:35.880
 It could be some ridiculous number.

2:36:35.880 --> 2:36:36.800
 So I just know that-

2:36:36.800 --> 2:36:40.820
 I have a feeling and I don't have a crystal ball,

2:36:40.820 --> 2:36:43.720
 but I don't know, I see an upwards

2:36:43.720 --> 2:36:47.760
 of certainly three or more comes to mind.

2:36:47.760 --> 2:36:49.840
 So so much of that has to do

2:36:49.840 --> 2:36:51.920
 with the partner you're with too.

2:36:51.920 --> 2:36:55.560
 So like that's such an open question,

2:36:55.560 --> 2:36:58.880
 especially in this society of what the right partnership is.

2:36:58.880 --> 2:37:02.800
 Because I'm deeply empathetic.

2:37:02.800 --> 2:37:05.100
 I want to see, like to me,

2:37:05.100 --> 2:37:07.760
 what I look for in a relationship is

2:37:07.760 --> 2:37:11.160
 for me to be really excited about the passions

2:37:11.160 --> 2:37:13.040
 of another person, like whatever they're into.

2:37:13.040 --> 2:37:15.740
 It doesn't have to be a career success,

2:37:15.740 --> 2:37:18.440
 any kind of success, just to be excited for them.

2:37:18.440 --> 2:37:20.520
 And for them to be excited for me

2:37:20.520 --> 2:37:21.840
 and they can share in that excitement

2:37:21.840 --> 2:37:23.720
 and build and build and build.

2:37:23.720 --> 2:37:25.800
 But there was also practical aspects of like,

2:37:25.800 --> 2:37:28.760
 what kind of shit do you enjoy doing together?

2:37:28.760 --> 2:37:32.320
 And I think family is a real serious undertaking.

2:37:32.320 --> 2:37:34.040
 Oh, it certainly is.

2:37:34.040 --> 2:37:37.480
 I mean, I think that I have a friend who said it,

2:37:37.480 --> 2:37:41.240
 I think best, which is that you first half,

2:37:41.240 --> 2:37:44.440
 he's in a very successful relationship and has a family.

2:37:44.440 --> 2:37:47.480
 And he said, you first have to define the role

2:37:47.480 --> 2:37:50.240
 and then you have to cast the right person for the role.

2:37:51.360 --> 2:37:53.880
 Well, yeah, there's some deep aspect to that,

2:37:53.880 --> 2:37:58.280
 but there's also an aspect to which you're not smart enough

2:37:58.280 --> 2:38:03.040
 from this side of it to define the role.

2:38:03.040 --> 2:38:04.840
 There's part of it that has to be a leap

2:38:04.840 --> 2:38:06.520
 that you have to take.

2:38:06.520 --> 2:38:11.520
 And I see having kids that way.

2:38:11.520 --> 2:38:16.120
 You just have to go with it and figure it out also,

2:38:16.120 --> 2:38:17.640
 as long as there's love there.

2:38:17.640 --> 2:38:20.560
 Like what the hell is life for even?

2:38:20.560 --> 2:38:25.360
 So there's so many incredibly successful people that I know,

2:38:26.320 --> 2:38:28.960
 that I've gotten to know, that all have kids.

2:38:28.960 --> 2:38:32.800
 And the presence of kids for the most part

2:38:32.800 --> 2:38:36.600
 has only been something that energized them,

2:38:36.600 --> 2:38:37.900
 something that gave them meaning,

2:38:37.900 --> 2:38:40.200
 something that made them the best version of themselves,

2:38:40.200 --> 2:38:42.360
 like made them more productive, not less,

2:38:42.360 --> 2:38:43.680
 which is fascinating to me.

2:38:43.680 --> 2:38:44.520
 It is fascinating.

2:38:44.520 --> 2:38:47.240
 I mean, you can imagine if the way that you felt about Homer,

2:38:47.240 --> 2:38:49.760
 the way that I feel and felt about Costello

2:38:49.760 --> 2:38:54.600
 is at all a glimpse of what that must be like then.

2:38:54.600 --> 2:38:55.840
 Exactly.

2:38:55.840 --> 2:39:00.000
 The downside, the thing I worry more about

2:39:00.000 --> 2:39:04.560
 is the partner side of that.

2:39:04.560 --> 2:39:07.840
 I've seen the kids are almost universally

2:39:07.840 --> 2:39:10.840
 a source of increased productivity and joy and happiness.

2:39:11.720 --> 2:39:13.280
 Like, yeah, they're a pain in the ass.

2:39:13.280 --> 2:39:14.120
 Yeah, it's complicated.

2:39:14.120 --> 2:39:15.640
 Yeah, so on and so forth.

2:39:15.640 --> 2:39:17.440
 People like to complain about kids.

2:39:17.440 --> 2:39:20.960
 But when you actually look past that little shallow layer

2:39:20.960 --> 2:39:22.940
 of complaint, kids are great.

2:39:22.940 --> 2:39:24.760
 The source of pain for a lot of people

2:39:24.760 --> 2:39:27.560
 is when the relationship doesn't work.

2:39:27.560 --> 2:39:30.940
 And so I'm very kind of concerned about,

2:39:32.600 --> 2:39:36.560
 dating is very difficult and I'm a complicated person.

2:39:36.560 --> 2:39:38.240
 And so it's been very difficult

2:39:38.240 --> 2:39:42.000
 to find the right kind of person.

2:39:42.000 --> 2:39:45.040
 But that statement doesn't even make sense

2:39:45.040 --> 2:39:46.880
 because I'm not on dating apps.

2:39:46.880 --> 2:39:48.260
 I don't see people.

2:39:48.260 --> 2:39:50.360
 You're like the first person I saw in a while.

2:39:50.360 --> 2:39:53.040
 It's like you, Michael Malice and like Joe.

2:39:53.040 --> 2:39:58.040
 So I don't think I've seen like a female, what is it?

2:40:00.840 --> 2:40:03.680
 An element of the female species in quite a while.

2:40:03.680 --> 2:40:06.520
 So I think you have to put yourself out there.

2:40:06.520 --> 2:40:07.440
 What is it?

2:40:07.440 --> 2:40:10.240
 Daniel Johnson says, true love will find you,

2:40:10.240 --> 2:40:11.760
 but only if you're looking.

2:40:11.760 --> 2:40:13.680
 So there's some element of really taking the leap

2:40:13.680 --> 2:40:14.780
 and putting yourself out there

2:40:14.780 --> 2:40:17.000
 in kind of different situations.

2:40:17.000 --> 2:40:18.400
 And I don't know how to do that

2:40:18.400 --> 2:40:20.460
 when you're behind a computer all the time.

2:40:20.460 --> 2:40:25.200
 Well, you're a builder and you're a problem solver

2:40:25.200 --> 2:40:30.200
 and you find solutions and I'm confident this solution is,

2:40:30.200 --> 2:40:33.620
 and the solution is out there.

2:40:33.620 --> 2:40:34.460
 And-

2:40:34.460 --> 2:40:35.700
 I think you're implying that I'm going to build

2:40:35.700 --> 2:40:38.500
 the girlfriend, which I think-

2:40:38.500 --> 2:40:41.120
 Or that you, well, and maybe we shouldn't separate

2:40:41.120 --> 2:40:45.460
 this friendship, the notion of friendship and community.

2:40:45.460 --> 2:40:48.900
 And if we go back to this concept of the aggregate,

2:40:48.900 --> 2:40:52.420
 maybe you'll meet this woman through a friend

2:40:52.420 --> 2:40:53.940
 or maybe or something of that sort.

2:40:53.940 --> 2:40:56.920
 So one of the things, I don't know if you feel the same way.

2:40:56.920 --> 2:41:01.500
 I definitely one of those people that just falls in love

2:41:01.500 --> 2:41:02.480
 and that's it.

2:41:02.480 --> 2:41:03.980
 Yeah, I can't say I'm like that.

2:41:03.980 --> 2:41:06.560
 With Costello, it was instantaneous.

2:41:06.560 --> 2:41:07.400
 Yeah.

2:41:07.400 --> 2:41:08.220
 It really was.

2:41:08.220 --> 2:41:09.700
 I mean, I know it's not romantic love,

2:41:09.700 --> 2:41:10.740
 but it was instantaneous.

2:41:10.740 --> 2:41:12.340
 No, but that's me.

2:41:12.340 --> 2:41:14.660
 And I think that if you know, you know,

2:41:14.660 --> 2:41:18.260
 because that's a good thing that you have that.

2:41:18.260 --> 2:41:20.860
 Well, I'm very careful of that

2:41:21.740 --> 2:41:24.980
 because you don't want to fall in love with the wrong person.

2:41:24.980 --> 2:41:27.340
 So I try to be very kind of careful with,

2:41:27.340 --> 2:41:29.460
 I've noticed this because I fall in love with everything,

2:41:29.460 --> 2:41:31.040
 like this mug, everything.

2:41:31.040 --> 2:41:34.020
 I fall in love with things in this world.

2:41:34.020 --> 2:41:35.500
 So like, you have to be really careful

2:41:35.500 --> 2:41:40.500
 because a girl comes up to you and says,

2:41:41.060 --> 2:41:42.640
 she loves Dostoevsky.

2:41:43.820 --> 2:41:46.700
 That doesn't necessarily mean you need to marry her tonight.

2:41:46.700 --> 2:41:49.060
 Yes, and I liked the way you said that out loud

2:41:49.060 --> 2:41:49.980
 so that you heard it.

2:41:49.980 --> 2:41:52.540
 It doesn't mean you need to marry her tonight.

2:41:52.540 --> 2:41:53.380
 Exactly.

2:41:53.380 --> 2:41:54.220
 Right, exactly.

2:41:54.220 --> 2:41:57.140
 But people are amazing and people are beautiful.

2:41:57.140 --> 2:42:00.540
 And that's, so I'm fully embraced that,

2:42:00.540 --> 2:42:02.940
 but I also, you have to be careful with relationships.

2:42:02.940 --> 2:42:05.820
 And at the same time, like I mentioned to you offline,

2:42:05.820 --> 2:42:10.840
 I don't, there's something about me that appreciates

2:42:10.840 --> 2:42:13.640
 swinging for the fences and not dating,

2:42:13.640 --> 2:42:15.860
 like doing serial dating or dating around.

2:42:15.860 --> 2:42:17.580
 Yeah, you're a one guy, one girl kind of guy.

2:42:17.580 --> 2:42:18.400
 Yeah.

2:42:18.400 --> 2:42:19.240
 You said that.

2:42:19.240 --> 2:42:23.100
 And it's tricky because you want to be careful

2:42:23.100 --> 2:42:24.180
 with that kind of stuff.

2:42:24.180 --> 2:42:26.380
 Especially now there's a growing platform

2:42:26.380 --> 2:42:29.140
 that have a ridiculous amount of female interest

2:42:29.140 --> 2:42:33.580
 of a certain kind, but I'm looking for deep connection.

2:42:33.580 --> 2:42:36.580
 And I'm looking by sitting home alone

2:42:36.580 --> 2:42:40.180
 and every once in a while, talking to Stanford professors.

2:42:41.100 --> 2:42:42.380
 Perfect solution.

2:42:42.380 --> 2:42:43.220
 Perfect solution.

2:42:43.220 --> 2:42:44.040
 It's going to work out great.

2:42:44.040 --> 2:42:45.260
 It's well incorporated.

2:42:45.260 --> 2:42:49.540
 It's part of, that constitutes machine learning of sorts.

2:42:49.540 --> 2:42:51.100
 Yeah, of sorts.

2:42:51.100 --> 2:42:55.860
 I do, you mentioned what has now become a quite extensive

2:42:55.860 --> 2:42:59.260
 and expansive public platform, which is incredible.

2:42:59.260 --> 2:43:01.260
 I mean, the number of people out,

2:43:01.260 --> 2:43:03.540
 first time I saw your podcast, I noticed the suit.

2:43:03.540 --> 2:43:05.460
 I was like, he respects his audience, which was great.

2:43:05.460 --> 2:43:08.480
 But I also thought, this is amazing.

2:43:08.480 --> 2:43:10.740
 People are showing up for science and engineering

2:43:10.740 --> 2:43:12.820
 and technology information and those discussions

2:43:12.820 --> 2:43:14.100
 and other sorts of discussions.

2:43:14.100 --> 2:43:18.100
 Now, I do want to talk for a moment about the podcast.

2:43:18.100 --> 2:43:21.700
 So my two questions about the podcast are,

2:43:21.700 --> 2:43:24.300
 when you started it, did you have a plan?

2:43:24.300 --> 2:43:27.500
 And regardless of what that answer is,

2:43:27.500 --> 2:43:29.820
 do you know where you're taking it?

2:43:29.820 --> 2:43:31.960
 Or would you like to leave us?

2:43:31.960 --> 2:43:35.220
 I do believe in an element of surprise is always fun.

2:43:35.220 --> 2:43:36.400
 But what about the podcast?

2:43:36.400 --> 2:43:37.680
 Do you enjoy the podcast?

2:43:37.680 --> 2:43:40.340
 I mean, your audience certainly includes me,

2:43:40.340 --> 2:43:41.720
 really enjoys the podcast.

2:43:41.720 --> 2:43:42.660
 It's incredible.

2:43:42.660 --> 2:43:46.540
 So I love talking to people

2:43:46.540 --> 2:43:50.200
 and there's something about microphones

2:43:50.200 --> 2:43:52.300
 that really bring out the best in people.

2:43:52.300 --> 2:43:54.500
 Like you don't get a chance to talk like this.

2:43:54.500 --> 2:43:56.100
 If you and I were just hanging out,

2:43:56.100 --> 2:43:58.500
 we would have a very different conversation

2:43:58.500 --> 2:44:02.100
 in the amount of focus we allocate to each other.

2:44:02.100 --> 2:44:04.420
 We would be having fun talking about other stuff

2:44:04.420 --> 2:44:06.000
 and doing other things.

2:44:06.000 --> 2:44:07.480
 There'd be a lot of distraction.

2:44:07.480 --> 2:44:11.020
 There would be some phone use and all that kind of stuff.

2:44:11.020 --> 2:44:13.980
 But here we're 100% focused on each other

2:44:13.980 --> 2:44:16.060
 and focused on the idea.

2:44:16.060 --> 2:44:18.180
 And sometimes playing with ideas

2:44:18.180 --> 2:44:21.060
 that we both don't know the answer to,

2:44:21.060 --> 2:44:23.040
 like a question we don't know the answer to.

2:44:23.040 --> 2:44:25.460
 We're both like fumbling with it, trying to figure out,

2:44:25.460 --> 2:44:27.060
 trying to get some insights

2:44:27.060 --> 2:44:29.480
 at something we haven't really figured out before

2:44:29.480 --> 2:44:31.240
 and together arriving at that.

2:44:31.240 --> 2:44:32.380
 I think that's magical.

2:44:32.380 --> 2:44:34.160
 I don't know why we need microphones for that,

2:44:34.160 --> 2:44:35.220
 but we somehow do.

2:44:35.220 --> 2:44:36.700
 It feels like doing science.

2:44:36.700 --> 2:44:38.700
 It feels like doing science for me, definitely.

2:44:38.700 --> 2:44:39.860
 That's exactly it.

2:44:39.860 --> 2:44:42.180
 Then, and I'm really glad you said that

2:44:42.180 --> 2:44:45.980
 because I don't actually often say this,

2:44:45.980 --> 2:44:48.900
 but that's exactly what I felt like.

2:44:48.900 --> 2:44:52.780
 I wanted to talk to friends and colleagues at MIT

2:44:53.940 --> 2:44:56.420
 to do real science together.

2:44:56.420 --> 2:44:57.780
 That's how I felt about it.

2:44:57.780 --> 2:45:00.180
 Like to really talk to problems

2:45:00.180 --> 2:45:01.780
 that are actually interesting

2:45:03.100 --> 2:45:06.060
 as opposed to like incremental work

2:45:06.060 --> 2:45:10.860
 that we're currently working for a particular conference.

2:45:10.860 --> 2:45:14.140
 So really asking questions like, what are we doing?

2:45:14.140 --> 2:45:15.980
 Like, where's this headed to?

2:45:15.980 --> 2:45:17.220
 Like, what are the big,

2:45:17.220 --> 2:45:19.940
 is this really going to help us solve,

2:45:19.940 --> 2:45:22.800
 in the case of AI, solve intelligence?

2:45:22.800 --> 2:45:24.700
 Like, is this even working on intelligence?

2:45:24.700 --> 2:45:26.380
 There's a certain sense,

2:45:26.380 --> 2:45:29.980
 which is why I initially called it artificial intelligence

2:45:29.980 --> 2:45:32.900
 is like most of us are not working

2:45:32.900 --> 2:45:34.780
 on artificial intelligence.

2:45:34.780 --> 2:45:37.680
 You're working on some very specific problem

2:45:37.680 --> 2:45:39.480
 and a set of techniques.

2:45:39.480 --> 2:45:41.260
 At the time, it's machine learning

2:45:41.260 --> 2:45:42.980
 to solve this particular problem.

2:45:42.980 --> 2:45:45.560
 This is not going to take us to a system

2:45:45.560 --> 2:45:49.220
 that is anywhere close to the generalizability

2:45:49.220 --> 2:45:51.340
 of the human mind.

2:45:51.340 --> 2:45:52.940
 Like the kind of stuff the human mind can do

2:45:52.940 --> 2:45:54.660
 in terms of memory, in terms of cognition,

2:45:54.660 --> 2:45:56.940
 in terms of reasoning, common sense reasoning.

2:45:56.940 --> 2:45:58.700
 This doesn't seem to take us there.

2:45:58.700 --> 2:46:00.540
 So the initial impulse was,

2:46:00.540 --> 2:46:03.140
 can I talk to these folks,

2:46:03.140 --> 2:46:05.620
 do science together through conversation?

2:46:05.620 --> 2:46:08.620
 And I also thought that there was not enough,

2:46:08.620 --> 2:46:13.620
 now, I didn't think there was enough good conversations

2:46:13.860 --> 2:46:17.640
 with world-class minds that I got to meet

2:46:17.640 --> 2:46:21.740
 and not the ones with a book or this was the thing.

2:46:21.740 --> 2:46:24.260
 Oftentimes you go on this tour when you have a book,

2:46:24.260 --> 2:46:26.660
 but there's a lot of minds that don't write books.

2:46:26.660 --> 2:46:27.480
 They don't.

2:46:27.480 --> 2:46:28.860
 And the books constrain the conversation too,

2:46:28.860 --> 2:46:31.720
 because then you're talking about this thing, this book.

2:46:31.720 --> 2:46:34.460
 But there's, I've noticed that with people

2:46:34.460 --> 2:46:37.260
 that haven't written a book who are brilliant,

2:46:37.260 --> 2:46:40.080
 we get to talk about ideas in a new way.

2:46:40.080 --> 2:46:42.620
 We both haven't actually,

2:46:42.620 --> 2:46:43.780
 when we raise a question,

2:46:43.780 --> 2:46:45.500
 we don't know the answer to it

2:46:45.500 --> 2:46:48.720
 when the question is raised and we try to arrive there.

2:46:49.820 --> 2:46:50.660
 Like, I don't know.

2:46:50.660 --> 2:46:52.220
 I remember asking questions

2:46:53.100 --> 2:46:57.140
 of world-class researchers in deep learning

2:46:57.140 --> 2:47:01.580
 of why do neural networks work as well as they do?

2:47:02.600 --> 2:47:06.620
 That question is often loosely asked,

2:47:06.620 --> 2:47:09.220
 but like when you have microphones

2:47:09.220 --> 2:47:11.020
 and you have to think through it

2:47:11.020 --> 2:47:12.460
 and you have 30 minutes to an hour

2:47:12.460 --> 2:47:16.140
 to think through it together, I think that's science.

2:47:16.140 --> 2:47:17.620
 I think that's really powerful.

2:47:17.620 --> 2:47:19.660
 So that was the one goal.

2:47:19.660 --> 2:47:20.560
 The other one is,

2:47:23.460 --> 2:47:25.460
 I again don't usually talk about this,

2:47:25.460 --> 2:47:28.500
 but there's some sense in which I wanted

2:47:28.500 --> 2:47:30.340
 to have dangerous conversations.

2:47:32.460 --> 2:47:36.180
 Part of the reasons I wanted to wear a suit is like,

2:47:36.180 --> 2:47:38.060
 I want it to be fearless.

2:47:38.060 --> 2:47:40.180
 Now, the reason I don't usually talk about it

2:47:40.180 --> 2:47:43.000
 is because I feel like I'm not good at conversation.

2:47:43.000 --> 2:47:48.000
 So it looks like it doesn't match the current skill level,

2:47:48.240 --> 2:47:53.240
 but I wanted to have really dangerous conversations

2:47:53.860 --> 2:47:56.860
 that I uniquely would be able to do.

2:47:58.180 --> 2:48:01.620
 Not completely uniquely, but like I'm a huge fan

2:48:01.620 --> 2:48:04.540
 of Joe Rogan and I had to ask myself,

2:48:04.540 --> 2:48:08.240
 what conversations can I do that Joe Rogan can't?

2:48:08.240 --> 2:48:10.540
 For me, I know I bring this up,

2:48:11.980 --> 2:48:13.760
 but for me, that person I thought about

2:48:13.760 --> 2:48:15.660
 at the time was Putin.

2:48:15.660 --> 2:48:17.580
 Like that's why I bring him up.

2:48:17.580 --> 2:48:22.100
 He's just like with Costello, he's not just a person.

2:48:22.100 --> 2:48:25.580
 He's also an idea to me for what I strive for,

2:48:25.580 --> 2:48:27.860
 just to have those dangerous conversations.

2:48:27.860 --> 2:48:31.440
 And the reason I'm uniquely qualified is both the Russian,

2:48:31.440 --> 2:48:34.060
 but also there's the judo and the martial arts.

2:48:34.060 --> 2:48:37.820
 There's a lot of elements that make me have a conversation

2:48:37.820 --> 2:48:39.460
 he hasn't had before.

2:48:39.460 --> 2:48:44.460
 And there's a few other people that I kept in mind,

2:48:45.060 --> 2:48:49.340
 like Don Knuth, he's a computer scientist from Stanford

2:48:49.340 --> 2:48:54.060
 that I thought is one of the most beautiful minds ever.

2:48:54.060 --> 2:48:59.060
 And nobody really talked to him, like really talked to him.

2:48:59.500 --> 2:49:01.380
 He did a few lectures, which people love,

2:49:01.380 --> 2:49:03.720
 but really just have a conversation with him.

2:49:03.720 --> 2:49:04.820
 There's a few people like that.

2:49:04.820 --> 2:49:07.300
 One of them passed away, John Conway, that I never got.

2:49:07.300 --> 2:49:10.660
 We agreed to talk, but he died before we didn't.

2:49:10.660 --> 2:49:13.460
 There's a few people like that that I thought like,

2:49:13.460 --> 2:49:18.460
 it's such a crime to not hear those folks.

2:49:19.660 --> 2:49:24.660
 And I have the unique ability to know how to purchase

2:49:24.860 --> 2:49:28.000
 a microphone on Amazon and plug it into a device

2:49:28.000 --> 2:49:30.420
 that records audio and then publish it,

2:49:30.420 --> 2:49:32.060
 which seems relatively unique.

2:49:32.060 --> 2:49:34.700
 Like that's not easy in the scientific community.

2:49:34.700 --> 2:49:36.700
 People knowing how to plug in a microphone.

2:49:36.700 --> 2:49:40.500
 No, they can build Faraday cages and two photon microscopes

2:49:40.500 --> 2:49:43.040
 and bio-engineer, all sorts of things.

2:49:43.040 --> 2:49:47.100
 But the idea that you could take ideas and export them

2:49:47.100 --> 2:49:49.260
 into a structure or a pseudo structure

2:49:49.260 --> 2:49:50.660
 that people would benefit from

2:49:50.660 --> 2:49:54.060
 seems like a cosmic achievement to them.

2:49:54.060 --> 2:49:57.460
 I don't know if it's a fear or just basically

2:49:57.460 --> 2:49:58.300
 they haven't tried it,

2:49:58.300 --> 2:50:00.300
 so they haven't learned the skill level.

2:50:00.300 --> 2:50:02.140
 I think they're not trained, I mean,

2:50:02.140 --> 2:50:03.380
 we could riff on this for a while,

2:50:03.380 --> 2:50:08.040
 but I think that, but it's important and maybe we should,

2:50:08.040 --> 2:50:11.060
 which is that it's, they're not trained to do it.

2:50:11.060 --> 2:50:12.820
 They're trained to think in specific aims

2:50:12.820 --> 2:50:14.260
 and specific hypotheses.

2:50:14.260 --> 2:50:17.840
 And many of them don't care to, right?

2:50:17.840 --> 2:50:22.660
 They became scientists because that's where they felt safe.

2:50:22.660 --> 2:50:25.900
 And so why would they leave that haven of safety?

2:50:25.900 --> 2:50:27.860
 Well, they also don't necessarily always see

2:50:27.860 --> 2:50:29.300
 the value in it.

2:50:29.300 --> 2:50:30.580
 We're all together learning,

2:50:30.580 --> 2:50:33.740
 you and I are learning the value of this.

2:50:33.740 --> 2:50:35.380
 I think you're probably,

2:50:35.380 --> 2:50:39.440
 you have an exceptionally successful and amazing podcast

2:50:39.440 --> 2:50:40.900
 that you started just recently.

2:50:40.900 --> 2:50:42.420
 Thanks to your encouragement.

2:50:42.420 --> 2:50:45.820
 Well, but there's a raw skill there

2:50:45.820 --> 2:50:49.180
 that you're definitely an inspiration to me

2:50:49.180 --> 2:50:50.420
 in how you do the podcast

2:50:50.420 --> 2:50:52.860
 in the level of excellence you reach.

2:50:52.860 --> 2:50:54.440
 But I think you've discovered

2:50:54.440 --> 2:50:57.040
 that that's also an impactful way to do science,

2:50:57.040 --> 2:50:58.020
 that podcast.

2:50:58.020 --> 2:51:01.620
 And I think a lot of scientists have not yet discovered

2:51:01.620 --> 2:51:06.620
 that this is, if they apply same kind of rigor

2:51:06.860 --> 2:51:09.340
 as they do to academic publication

2:51:09.340 --> 2:51:11.780
 or to even conference presentations,

2:51:11.780 --> 2:51:16.120
 and they do that rigor and effort to podcast,

2:51:16.120 --> 2:51:18.500
 whatever that is, that could be a five minute podcast,

2:51:18.500 --> 2:51:21.000
 a two hour podcast, it could be conversational,

2:51:21.000 --> 2:51:22.940
 or it can be more like lecture-like.

2:51:22.940 --> 2:51:24.420
 If they apply that effort,

2:51:24.420 --> 2:51:26.840
 you have the potential to reach over time,

2:51:26.840 --> 2:51:28.620
 tens of thousands, hundreds of thousands,

2:51:28.620 --> 2:51:29.800
 millions of people.

2:51:29.800 --> 2:51:32.460
 And that's really, really powerful.

2:51:32.460 --> 2:51:35.220
 But yeah, for me,

2:51:35.220 --> 2:51:38.320
 giving a platform to a few of those folks,

2:51:39.420 --> 2:51:40.940
 especially for me personally,

2:51:40.940 --> 2:51:45.940
 so maybe you can speak to what fields you're drawn to,

2:51:46.260 --> 2:51:48.940
 but I thought computer scientists

2:51:51.020 --> 2:51:52.660
 were especially bad at this.

2:51:53.540 --> 2:51:56.300
 So there's brilliant computer scientists

2:51:56.300 --> 2:52:00.300
 that I thought it would be amazing to explore their mind,

2:52:00.300 --> 2:52:02.060
 explore their thinking.

2:52:02.060 --> 2:52:06.540
 And so I took that almost on as an effort.

2:52:06.540 --> 2:52:11.140
 And at the same time, I had other guests in mind

2:52:11.140 --> 2:52:13.900
 or people that connect to my own interests.

2:52:13.900 --> 2:52:15.500
 So the wrestling,

2:52:16.780 --> 2:52:18.900
 wrestling, music, football,

2:52:18.900 --> 2:52:21.260
 both American football and soccer.

2:52:21.260 --> 2:52:22.620
 I have a few particular people

2:52:22.620 --> 2:52:24.120
 that I'm really interested in.

2:52:24.120 --> 2:52:27.960
 Bovisar Satiev, the Satiev brothers,

2:52:27.960 --> 2:52:29.640
 even Khabib for wrestling,

2:52:29.640 --> 2:52:31.000
 just to talk to them.

2:52:31.000 --> 2:52:33.000
 Because you guys can communicate.

2:52:33.000 --> 2:52:35.360
 In Russian and in wrestling,

2:52:36.380 --> 2:52:38.840
 as wrestlers and as Russians.

2:52:38.840 --> 2:52:41.760
 And so that little,

2:52:41.760 --> 2:52:44.040
 it's like an opportunity to explore a mind

2:52:44.040 --> 2:52:47.680
 that I'm able to bring to the world.

2:52:47.680 --> 2:52:52.680
 And also, I feel like it makes me a better person

2:52:52.680 --> 2:52:55.640
 just that being that vulnerable

2:52:55.640 --> 2:52:57.380
 and exploring ideas together.

2:52:57.380 --> 2:52:59.820
 I don't know, like good conversation.

2:52:59.820 --> 2:53:01.760
 I don't know how often you have a really good conversation

2:53:01.760 --> 2:53:04.460
 with friends, but like podcasts are like that.

2:53:04.460 --> 2:53:07.120
 And it's deeply moving.

2:53:07.120 --> 2:53:07.960
 It's the best.

2:53:07.960 --> 2:53:09.920
 And what you brought through,

2:53:09.920 --> 2:53:12.200
 I mean, when I saw you sit down with Penrose,

2:53:12.200 --> 2:53:15.000
 Nobel Prize winning physicists and these other folks,

2:53:15.000 --> 2:53:16.280
 it's not just because he has a Nobel,

2:53:16.280 --> 2:53:18.040
 it's what comes out of his mouth is incredible.

2:53:18.040 --> 2:53:22.900
 And what you were able to hold in that conversation

2:53:22.900 --> 2:53:24.400
 was so much better.

2:53:24.400 --> 2:53:28.880
 Light years beyond what he had any other interviewer,

2:53:28.880 --> 2:53:30.120
 I don't want to even call you an interviewer

2:53:30.120 --> 2:53:31.620
 because it's really about conversation.

2:53:31.620 --> 2:53:34.460
 Light years beyond what anyone else had been able

2:53:34.460 --> 2:53:39.460
 to engage with him was such a beacon of what's possible.

2:53:39.960 --> 2:53:42.440
 And I know that, I think that's what people are drawn to.

2:53:42.440 --> 2:53:44.420
 And there's a certain intimacy

2:53:44.420 --> 2:53:47.560
 that certainly if two people are friends as we are

2:53:47.560 --> 2:53:49.860
 and they know each other, that there's more of that,

2:53:49.860 --> 2:53:51.940
 but there's an intimacy in those kinds

2:53:51.940 --> 2:53:54.280
 of private conversations that are made public.

2:53:55.880 --> 2:53:57.880
 Well, that's the, with you,

2:53:57.880 --> 2:54:01.200
 you're probably starting to realize, and Costello,

2:54:01.200 --> 2:54:04.560
 it's like part of it, because you're authentic

2:54:04.560 --> 2:54:07.000
 and you're putting yourself out there completely,

2:54:07.000 --> 2:54:10.860
 people are almost not just consuming

2:54:10.860 --> 2:54:15.800
 the words you're saying, they also enjoy watching you,

2:54:15.800 --> 2:54:19.080
 Andrew, struggle with these ideas

2:54:19.080 --> 2:54:20.680
 or try to communicate these ideas.

2:54:20.680 --> 2:54:21.680
 They like the flaws.

2:54:21.680 --> 2:54:24.840
 They like a human being exploring ideas.

2:54:24.840 --> 2:54:26.480
 Well, that's good, because I got plenty of those.

2:54:26.480 --> 2:54:28.640
 Well, they like the self-critical aspects,

2:54:28.640 --> 2:54:30.280
 like where you're very careful,

2:54:30.280 --> 2:54:33.040
 where you're very self-critical about your flaws.

2:54:33.040 --> 2:54:35.440
 I mean, in that same way, it's interesting,

2:54:35.440 --> 2:54:37.920
 I think, for people to watch me talk to Penrose,

2:54:37.920 --> 2:54:42.280
 not just because Penrose is communicating ideas,

2:54:42.280 --> 2:54:46.960
 but here's this like silly kid trying to explore ideas.

2:54:46.960 --> 2:54:48.300
 Like they know this kid,

2:54:48.300 --> 2:54:51.240
 that there's a human connection that is really powerful.

2:54:51.240 --> 2:54:53.560
 Same, I think, with Putin, right?

2:54:53.560 --> 2:54:57.440
 Like it's not just a good interview with Putin.

2:54:57.440 --> 2:55:00.880
 It's also, here's this kid struggling

2:55:00.880 --> 2:55:03.840
 to talk with one of the most powerful,

2:55:04.760 --> 2:55:08.000
 some would argue dangerous people in the world,

2:55:08.000 --> 2:55:11.920
 that they love that, the authenticity that led up to that.

2:55:11.920 --> 2:55:16.080
 Like, and in return, I get to connect everybody I run to

2:55:16.080 --> 2:55:18.280
 in the street and all those kinds of things.

2:55:19.420 --> 2:55:21.040
 There's a depth of connection there,

2:55:21.040 --> 2:55:22.920
 almost within like a minute or two,

2:55:22.920 --> 2:55:24.300
 that's unlike any other.

2:55:24.300 --> 2:55:26.840
 Yeah, there's an intimacy that you've formed with them.

2:55:26.840 --> 2:55:29.760
 Yeah, we've been on this like journey together.

2:55:29.760 --> 2:55:31.280
 I mean, I have the same thing with Joe Rogan

2:55:31.280 --> 2:55:32.740
 before I ever met him, right?

2:55:32.740 --> 2:55:36.640
 Like I was, because I was a fan of Joe for so many years,

2:55:36.640 --> 2:55:40.880
 there's something, there's a kind of friendship

2:55:40.880 --> 2:55:44.440
 as absurd as it might be to say in podcasting

2:55:44.440 --> 2:55:45.960
 and listening to podcasts.

2:55:45.960 --> 2:55:48.880
 Yeah, maybe it fills in a little bit of that,

2:55:48.880 --> 2:55:50.640
 or solves a little bit of that loneliness

2:55:50.640 --> 2:55:51.480
 that you're talking about.

2:55:51.480 --> 2:55:53.000
 Until the robots are here.

2:55:54.520 --> 2:55:56.520
 I have just a couple more questions,

2:55:56.520 --> 2:55:59.560
 but one of them is on behalf of your audience,

2:55:59.560 --> 2:56:02.720
 which is, I'm not going to ask you

2:56:02.720 --> 2:56:04.460
 the meaning of the hedgehog,

2:56:04.460 --> 2:56:08.460
 but I just want to know, does it have a name?

2:56:08.460 --> 2:56:09.920
 And you don't have to tell us the name,

2:56:09.920 --> 2:56:12.400
 but just does it have a name, yes or no?

2:56:12.400 --> 2:56:17.400
 Well, there's a name he likes to be referred to as,

2:56:17.520 --> 2:56:19.280
 and then there's a private name

2:56:19.280 --> 2:56:21.280
 in the privacy of our own company that we call each other.

2:56:21.280 --> 2:56:24.360
 No, I'm not that insane.

2:56:24.360 --> 2:56:25.660
 No, his name is Hedgy.

2:56:27.280 --> 2:56:28.640
 He's a hedgehog.

2:56:28.640 --> 2:56:30.260
 I don't like stuffed animals,

2:56:31.620 --> 2:56:35.600
 but his story is one of minimalism.

2:56:35.600 --> 2:56:40.600
 So I gave away everything I own now three times in my life.

2:56:41.120 --> 2:56:42.960
 By everything, I mean, almost everything,

2:56:42.960 --> 2:56:45.220
 kept jeans and shirt and a laptop.

2:56:46.140 --> 2:56:49.920
 And recently it's also been guitar, things like that.

2:56:51.240 --> 2:56:55.280
 But he survived because he was always in the,

2:56:55.280 --> 2:56:58.560
 at least in the first two times, was in the laptop bag,

2:56:58.560 --> 2:57:00.240
 and he just got lucky.

2:57:00.240 --> 2:57:02.960
 And so I just liked the perseverance of that.

2:57:02.960 --> 2:57:05.960
 And I first saw him in the,

2:57:05.960 --> 2:57:07.360
 the reason I got a stuffed animal

2:57:07.360 --> 2:57:09.600
 and I don't have other stuffed animals

2:57:09.600 --> 2:57:11.560
 is it was in a thrift store

2:57:12.960 --> 2:57:16.000
 in this like giant pile of stuffed animals.

2:57:16.000 --> 2:57:20.000
 And he jumped out at me because unlike all the rest of them,

2:57:20.000 --> 2:57:25.000
 he has this intense mean look about him,

2:57:25.240 --> 2:57:28.640
 that he's just, he's upset at life,

2:57:29.600 --> 2:57:31.040
 at the cruelty of life.

2:57:31.040 --> 2:57:33.040
 And just, especially in the contrast of the other

2:57:33.040 --> 2:57:35.840
 stuffed animals, they have this dumb smile on their face.

2:57:35.840 --> 2:57:37.100
 If you look at most stuffed animals,

2:57:37.100 --> 2:57:38.720
 they have this dumb look on their face

2:57:38.720 --> 2:57:39.640
 and they're just happy.

2:57:39.640 --> 2:57:40.560
 It's like Pleasantville.

2:57:40.560 --> 2:57:41.640
 It's what we say in neuroscience,

2:57:41.640 --> 2:57:44.440
 they have a smooth cortex, not many fold.

2:57:44.440 --> 2:57:45.280
 Exactly.

2:57:45.280 --> 2:57:48.000
 And this, like, Hegyi like saw through all of it.

2:57:48.000 --> 2:57:52.000
 He was like Dostoevsky's man from underground.

2:57:52.000 --> 2:57:54.800
 I mean, there's a sense that he saw the darkness

2:57:54.800 --> 2:57:56.560
 of the world and persevered.

2:57:56.560 --> 2:58:00.720
 So I got, and there's also a famous Russian cartoon,

2:58:00.720 --> 2:58:03.920
 Hedgehog in the Fog, that I grew up with,

2:58:03.920 --> 2:58:04.760
 I connected with.

2:58:04.760 --> 2:58:07.600
 There's people who know of that cartoon.

2:58:07.600 --> 2:58:09.440
 You can see it on YouTube.

2:58:09.440 --> 2:58:10.280
 It's like-

2:58:10.280 --> 2:58:11.100
 Hedgehog in the Fog.

2:58:11.100 --> 2:58:11.940
 Yeah.

2:58:13.800 --> 2:58:15.280
 It's just as you would expect,

2:58:15.280 --> 2:58:17.920
 especially from like early Soviet cartoons.

2:58:17.920 --> 2:58:22.880
 It's a hedgehog, like sad, walking through the fog,

2:58:22.880 --> 2:58:25.320
 exploring like loneliness and sadness.

2:58:25.320 --> 2:58:26.760
 It's like, but it's beautiful.

2:58:26.760 --> 2:58:27.840
 It's like a piece of art.

2:58:27.840 --> 2:58:29.600
 People should, even if you don't speak Russian,

2:58:29.600 --> 2:58:31.560
 you'll see, you'll understand.

2:58:31.560 --> 2:58:33.920
 Oh, it's like the moment you said that I was gonna ask.

2:58:33.920 --> 2:58:35.440
 So it's in Russian, but of course it's in Russian.

2:58:35.440 --> 2:58:37.240
 It's in Russian, but it's more,

2:58:37.240 --> 2:58:39.040
 there's very little speaking in it.

2:58:39.040 --> 2:58:43.040
 It's almost, there's an interesting exploration

2:58:43.040 --> 2:58:47.360
 of how you make sense of the world

2:58:47.360 --> 2:58:52.160
 when you see it only vaguely through the fog.

2:58:52.160 --> 2:58:54.120
 So he's trying to understand the world.

2:58:55.240 --> 2:58:56.660
 Here we have Mickey Mouse.

2:58:56.660 --> 2:58:57.500
 Yeah.

2:58:57.500 --> 2:58:58.320
 We have Bugs Bunny.

2:58:58.320 --> 2:58:59.160
 Yeah.

2:58:59.160 --> 2:59:01.520
 We have all these crazy animals,

2:59:01.520 --> 2:59:03.560
 and you have the hedgehog in the fog.

2:59:03.560 --> 2:59:06.660
 So there's a certain period, and this is, again,

2:59:07.880 --> 2:59:09.260
 I don't know what to attribute it to,

2:59:09.260 --> 2:59:10.800
 but it was really powerful,

2:59:10.800 --> 2:59:12.960
 which there's a period in Soviet history,

2:59:12.960 --> 2:59:16.040
 I think probably 70s and 80s,

2:59:16.040 --> 2:59:21.040
 where like, especially kids were treated very seriously.

2:59:21.280 --> 2:59:24.280
 Like they were treated like they're able to deal

2:59:24.280 --> 2:59:27.800
 with the weightiness of life.

2:59:27.800 --> 2:59:29.960
 And that was reflected in the cartoons.

2:59:31.020 --> 2:59:36.020
 And it was allowed to have really artistic content,

2:59:37.160 --> 2:59:39.040
 not like dumb cartoons that are trying

2:59:39.040 --> 2:59:40.960
 to get you to be like smile and run around,

2:59:40.960 --> 2:59:42.320
 but like create art.

2:59:42.320 --> 2:59:45.300
 Like stuff that, you know how like short cartoons

2:59:45.300 --> 2:59:47.040
 or short films can win Oscars?

2:59:47.040 --> 2:59:48.720
 Like that's what they're swinging for.

2:59:48.720 --> 2:59:51.080
 So what strikes me about this is a little bit

2:59:51.080 --> 2:59:52.760
 how we were talking about the suit earlier.

2:59:52.760 --> 2:59:55.140
 It's almost like they treat kids with respect.

2:59:55.140 --> 2:59:55.980
 Yeah.

2:59:55.980 --> 2:59:58.280
 Like they have an intelligence

2:59:58.280 --> 2:59:59.760
 and they honor that intelligence.

2:59:59.760 --> 3:00:02.360
 Yeah, they're really just adult in a small body.

3:00:03.360 --> 3:00:04.600
 Like you want to protect them

3:00:04.600 --> 3:00:06.400
 from the true cruelty of the world,

3:00:06.400 --> 3:00:08.440
 but in terms of their intellectual capacity

3:00:08.440 --> 3:00:10.240
 or like philosophical capacity,

3:00:10.240 --> 3:00:11.540
 they're right there with you.

3:00:11.540 --> 3:00:14.240
 And so the cartoons reflected that,

3:00:14.240 --> 3:00:17.720
 the art that they consumed, the education reflected that.

3:00:17.720 --> 3:00:19.100
 So he represents that.

3:00:19.100 --> 3:00:24.100
 I mean, there's a sense of because he survived so long

3:00:24.100 --> 3:00:27.320
 and because I don't like stuffed animals,

3:00:27.320 --> 3:00:30.920
 that it's like, we've been through all of this together

3:00:30.920 --> 3:00:32.980
 and it's the same sharing the moments together.

3:00:32.980 --> 3:00:34.340
 It's the friendship.

3:00:34.340 --> 3:00:36.200
 And there's a sense in which, you know,

3:00:36.200 --> 3:00:39.000
 if all the world turns on you and goes to hell,

3:00:39.000 --> 3:00:40.400
 at least we got each other.

3:00:40.400 --> 3:00:44.300
 That, and he doesn't die because he's an inanimate object.

3:00:44.300 --> 3:00:45.480
 So.

3:00:45.480 --> 3:00:47.400
 Until you animate him.

3:00:47.400 --> 3:00:49.060
 Until you animate him.

3:00:49.060 --> 3:00:50.420
 And then I probably wouldn't want to know

3:00:50.420 --> 3:00:53.460
 what he was thinking about this whole time.

3:00:53.460 --> 3:00:55.200
 He's probably really into Taylor Swift

3:00:55.200 --> 3:00:56.040
 or something like that.

3:00:56.040 --> 3:00:57.700
 It's like that I wouldn't even want to know.

3:00:57.700 --> 3:00:58.540
 Anyway.

3:00:58.540 --> 3:01:02.460
 Well, I now feel a connection to Hedgy the Hedgehog

3:01:02.460 --> 3:01:04.100
 that I certainly didn't have before.

3:01:04.100 --> 3:01:07.340
 And I think that encapsulates the kind of possibility

3:01:07.340 --> 3:01:11.120
 of connection that is possible

3:01:11.120 --> 3:01:13.620
 between human and other object

3:01:13.620 --> 3:01:17.740
 and through robotics, certainly.

3:01:17.740 --> 3:01:19.580
 There's a saying that I heard when I was a graduate student

3:01:19.580 --> 3:01:22.520
 that's just been ringing in my mind

3:01:22.520 --> 3:01:25.060
 throughout this conversation in such a,

3:01:25.060 --> 3:01:26.340
 I think, appropriate way,

3:01:26.340 --> 3:01:31.300
 which is that Lex, you are in a minority of one.

3:01:31.300 --> 3:01:35.140
 You are truly extraordinary in your ability

3:01:35.140 --> 3:01:40.140
 to encapsulate so many aspects of science, engineering,

3:01:40.180 --> 3:01:43.740
 public communication about so many topics,

3:01:43.740 --> 3:01:47.100
 martial arts, and the emotional depth that you bring to it,

3:01:47.100 --> 3:01:49.060
 and just the purposefulness.

3:01:49.060 --> 3:01:51.620
 And I think if it's not clear to people,

3:01:51.620 --> 3:01:53.880
 it absolutely should be stated,

3:01:53.880 --> 3:01:55.460
 but I think it's abundantly clear

3:01:55.460 --> 3:01:58.400
 that just the amount of time and thinking

3:01:58.400 --> 3:02:01.340
 that you put into things is,

3:02:01.340 --> 3:02:04.820
 it is the ultimate mark of respect.

3:02:04.820 --> 3:02:08.020
 So I'm just extraordinarily grateful for your friendship

3:02:08.020 --> 3:02:09.140
 and for this conversation.

3:02:09.140 --> 3:02:11.140
 I'm proud to be your friend.

3:02:11.140 --> 3:02:13.420
 And I just wish you showed me the same kind of respect

3:02:13.420 --> 3:02:15.840
 by wearing a suit and make your father proud,

3:02:15.840 --> 3:02:17.500
 maybe next time.

3:02:17.500 --> 3:02:19.060
 Next time, indeed.

3:02:19.060 --> 3:02:20.460
 Thanks so much, my friend.

3:02:20.460 --> 3:02:22.340
 Thank you. Thank you, Andrew.

3:02:22.340 --> 3:02:24.420
 Thank you for joining me for my discussion

3:02:24.420 --> 3:02:26.280
 with Dr. Lex Friedman.

3:02:26.280 --> 3:02:29.140
 If you're enjoying this podcast and learning from it,

3:02:29.140 --> 3:02:31.380
 please consider subscribing on YouTube.

3:02:31.380 --> 3:02:35.500
 As well, you can subscribe to us on Spotify or Apple.

3:02:35.500 --> 3:02:38.220
 Please leave any questions and comments and suggestions

3:02:38.220 --> 3:02:40.840
 that you have for future podcast episodes and guests

3:02:40.840 --> 3:02:43.220
 in the comment section on YouTube.

3:02:43.220 --> 3:02:46.780
 At Apple, you can also leave us up to a five-star review.

3:02:46.780 --> 3:02:49.640
 If you'd like to support this podcast, we have a Patreon.

3:02:49.640 --> 3:02:52.820
 That's patreon.com slash Andrew Huberman.

3:02:52.820 --> 3:02:56.200
 And there, you can support us at any level that you like.

3:02:56.200 --> 3:02:58.540
 Also, please check out our sponsors mentioned

3:02:58.540 --> 3:03:00.840
 at the beginning of the podcast episode.

3:03:00.840 --> 3:03:03.180
 That's the best way to support this podcast.

3:03:03.180 --> 3:03:06.740
 Links to our sponsors can be found in the show notes.

3:03:06.740 --> 3:03:09.740
 And finally, thank you for your interest in science.

3:03:10.960 --> 3:02:50.580
 ["The

3:02:50.580 --> 3:02:55.580
 Letter on the prefers"]

